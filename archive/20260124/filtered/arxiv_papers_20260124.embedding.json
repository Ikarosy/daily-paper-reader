{
  "top_k": 100,
  "generated_at": "2026-01-24T15:25:45.214402+00:00",
  "papers": [
    {
      "id": "2601.16206v1",
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "abstract": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.",
      "authors": [
        "Daixuan Cheng",
        "Shaohan Huang",
        "Yuxian Gu",
        "Huatong Song",
        "Guoxin Chen",
        "Li Dong",
        "Wayne Xin Zhao",
        "Ji-Rong Wen",
        "Furu Wei"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-22 18:57:09+00:00",
      "link": "https://arxiv.org/pdf/2601.16206v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.16194v1",
      "title": "A Rolling-Space Branch-and-Price Algorithm for the Multi-Compartment Vehicle Routing Problem with Multiple Time Windows",
      "abstract": "This paper investigates the multi-compartment vehicle routing problem with multiple time windows (MCVRPMTW), an extension of the classical vehicle routing problem with time windows that considers vehicles equipped with multiple compartments and customers requiring service across several delivery time windows. The problem incorporates three key compartment-related features: (i) compartment flexibility in the number of compartments, (ii) item-to-compartment compatibility, and (iii) item-to-item compatibility. The problem also accommodates practical operational requirements such as driver breaks. To solve the MCVRPMTW, we develop an exact branch-and-price (B&P) algorithm in which the pricing problem is solved using a labeling algorithm. Several acceleration strategies are introduced to limit symmetry during label extensions, improve the stability of dual solutions in column generation, and enhance the branching process. To handle large-scale instances, we propose a rolling-space B&P algorithm that integrates clustering techniques into the solution framework. Extensive computational experiments on instances inspired by a real-world industrial application demonstrate the effectiveness of the proposed approach and provide useful managerial insights for practical implementation.",
      "authors": [
        "El Mehdi Er Raqabi",
        "Kevin Dalmeijer",
        "Pascal Van Hentenryck"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "cs.LG"
      ],
      "published": "2026-01-22 18:46:46+00:00",
      "link": "https://arxiv.org/pdf/2601.16194v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.16187v1",
      "title": "Average Unfairness in Routing Games",
      "abstract": "We propose average unfairness as a new measure of fairness in routing games, defined as the ratio between the average latency and the minimum latency experienced by users. This measure is a natural complement to two existing unfairness notions: loaded unfairness, which compares maximum and minimum latencies of routes with positive flow, and user equilibrium (UE) unfairness, which compares maximum latency with the latency of a Nash equilibrium. We show that the worst-case values of all three unfairness measures coincide and are characterized by a steepness parameter intrinsic to the latency function class. We show that average unfairness is always no greater than loaded unfairness, and the two measures are equal only when the flow is fully fair. Besides that, we offer a complete comparison of the three unfairness measures, which, to the best of our knowledge, is the first theoretical analysis in this direction. Finally, we study the constrained system optimum (CSO) problem, where one seeks to minimize total latency subject to an upper bound on unfairness. We prove that, for the same tolerance level, the optimal flow under an average unfairness constraint achieves lower total latency than any flow satisfying a loaded unfairness constraint. We show that such improvement is always strict in parallel-link networks and establish sufficient conditions for general networks. We further illustrate the latter with numerical examples. Our results provide theoretical guarantees and valuable insights for evaluating fairness-efficiency tradeoffs in network routing.",
      "authors": [
        "Pan-Yang Su",
        "Arwa Alanqary",
        "Bryce L. Ferguson",
        "Manxi Wu",
        "Alexandre M. Bayen",
        "Shankar Sastry"
      ],
      "primary_category": "cs.MA",
      "categories": [
        "cs.MA",
        "cs.GT",
        "eess.SY"
      ],
      "published": "2026-01-22 18:40:57+00:00",
      "link": "https://arxiv.org/pdf/2601.16187v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.16175v1",
      "title": "Learning to Discover at Test Time",
      "abstract": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.",
      "authors": [
        "Mert Yuksekgonul",
        "Daniel Koceja",
        "Xinhao Li",
        "Federico Bianchi",
        "Jed McCaleb",
        "Xiaolong Wang",
        "Jan Kautz",
        "Yejin Choi",
        "James Zou",
        "Carlos Guestrin",
        "Yu Sun"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-22 18:24:00+00:00",
      "link": "https://arxiv.org/pdf/2601.16175v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.16174v1",
      "title": "Beyond Predictive Uncertainty: Reliable Representation Learning with Structural Constraints",
      "abstract": "Uncertainty estimation in machine learning has traditionally focused on the prediction stage, aiming to quantify confidence in model outputs while treating learned representations as deterministic and reliable by default. In this work, we challenge this implicit assumption and argue that reliability should be regarded as a first-class property of learned representations themselves. We propose a principled framework for reliable representation learning that explicitly models representation-level uncertainty and leverages structural constraints as inductive biases to regularize the space of feasible representations. Our approach introduces uncertainty-aware regularization directly in the representation space, encouraging representations that are not only predictive but also stable, well-calibrated, and robust to noise and structural perturbations. Structural constraints, such as sparsity, relational structure, or feature-group dependencies, are incorporated to define meaningful geometry and reduce spurious variability in learned representations, without assuming fully correct or noise-free structure. Importantly, the proposed framework is independent of specific model architectures and can be integrated with a wide range of representation learning methods.",
      "authors": [
        "Yiyao Yang"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-01-22 18:19:52+00:00",
      "link": "https://arxiv.org/pdf/2601.16174v1",
      "tags": [
        "keyword:EAA"
      ]
    },
    {
      "id": "2601.16171v1",
      "title": "Non-Linearly Separable Distributed Computing: A Sparse Tensor Factorization Approach",
      "abstract": "The work considers the $N$-server distributed computing setting with $K$ users requesting functions that are arbitrary multi-variable polynomial evaluations of $L$ real (potentially non-linear) basis subfunctions. Our aim is to seek efficient task-allocation and data-communication techniques that reduce computation and communication costs. Towards this, we take a tensor-theoretic approach, in which we represent the requested non-linearly decomposable functions using a properly designed tensor $\\bar{\\mathcal{F}}$, whose sparse decomposition into a tensor $\\bar{\\mathcal{E}}$ and matrix $\\mathbf{D}$ directly defines the task assignment, connectivity, and communication patterns. We here design an achievable scheme, employing novel fixed-support SVD-based tensor factorization methods and careful multi-dimensional tiling of subtensors, yielding computation and communication protocols whose costs are derived here, and which are shown to perform substantially better than the state of art.",
      "authors": [
        "Ali Khalesi",
        "Ahmad Tanha",
        "Derya Malak",
        "Petros Elia"
      ],
      "primary_category": "cs.IT",
      "categories": [
        "cs.IT"
      ],
      "published": "2026-01-22 18:16:19+00:00",
      "link": "https://arxiv.org/pdf/2601.16171v1",
      "tags": [
        "keyword:EAA",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.16163v1",
      "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
      "abstract": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/",
      "authors": [
        "Moo Jin Kim",
        "Yihuai Gao",
        "Tsung-Yi Lin",
        "Yen-Chen Lin",
        "Yunhao Ge",
        "Grace Lam",
        "Percy Liang",
        "Shuran Song",
        "Ming-Yu Liu",
        "Chelsea Finn",
        "Jinwei Gu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "published": "2026-01-22 18:09:30+00:00",
      "link": "https://arxiv.org/pdf/2601.16163v1",
      "tags": [
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.16158v1",
      "title": "Domain-Incremental Continual Learning for Robust and Efficient Keyword Spotting in Resource Constrained Systems",
      "abstract": "Keyword Spotting (KWS) systems with small footprint models deployed on edge devices face significant accuracy and robustness challenges due to domain shifts caused by varying noise and recording conditions. To address this, we propose a comprehensive framework for continual learning designed to adapt to new domains while maintaining computational efficiency. The proposed pipeline integrates a dual-input Convolutional Neural Network, utilizing both Mel Frequency Cepstral Coefficients (MFCC) and Mel-spectrogram features, supported by a multi-stage denoising process, involving discrete wavelet transform and spectral subtraction techniques, plus model and prototype update blocks. Unlike prior methods that restrict updates to specific layers, our approach updates the complete quantized model, made possible due to compact model architecture. A subset of input samples are selected during runtime using class prototypes and confidence-driven filtering, which are then pseudo-labeled and combined with rehearsal buffer for incremental model retraining. Experimental results on noisy test dataset demonstrate the framework's effectiveness, achieving 99.63\\% accuracy on clean data and maintaining robust performance (exceeding 94\\% accuracy) across diverse noisy environments, even at -10 dB Signal-to-Noise Ratio. The proposed framework work confirms that integrating efficient denoising with prototype-based continual learning enables KWS models to operate autonomously and robustly in resource-constrained, dynamic environments.",
      "authors": [
        "Prakash Dhungana",
        "Sayed Ahmad Salehi"
      ],
      "primary_category": "cs.SD",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "published": "2026-01-22 17:59:31+00:00",
      "link": "https://arxiv.org/pdf/2601.16158v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.16156v1",
      "title": "All ascents exponential from valued constraint graphs of pathwidth three",
      "abstract": "Many combinatorial optimization problems can be formulated as finding as assignment that maximized some pseudo-Boolean function (that we call the fitness function). Strict local search starts with some assignment and follows some update rule to proceed to an adjacent assignment of strictly higher fitness. This means that strict local search algorithms follow ascents in the fitness landscape of the pseudo-Boolean function. The complexity of the pseudo-Boolean function (and the fitness landscapes that it represents) can be parameterized by properties of the valued constraint satisfaction problem (VCSP) that encodes the pseudo-Boolean function. We focus on properties of the constraint graphs of the VCSP, with the intuition that spare graphs are less complex than dense ones. Specifically, we argue that pathwidth is the natural sparsity parameter for understanding limits on the power of strict local search. We show that prior constructions of sparse VCSPs where all ascents are exponentially long had pathwidth greater than or equal to four. We improve this this with our controlled doubling construction: a valued constraint satisfaction problem of pathwidth three where all ascents are exponentially long from a designated initial assignment. From this, we conclude that all strict local search algorithms can be forced to take an exponential number of steps even on simple valued constraint graphs of pathwidth three.",
      "authors": [
        "Artem Kaznatcheev",
        "Willemijn Volgering"
      ],
      "primary_category": "cs.DM",
      "categories": [
        "cs.DM",
        "cs.DS"
      ],
      "published": "2026-01-22 17:57:54+00:00",
      "link": "https://arxiv.org/pdf/2601.16156v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.16142v1",
      "title": "Computing Fixpoints of Learned Functions: Chaotic Iteration and Simple Stochastic Games",
      "abstract": "The problem of determining the (least) fixpoint of (higher-dimensional) functions over the non-negative reals frequently occurs when dealing with systems endowed with a quantitative semantics. We focus on the situation in which the functions of interest are not known precisely but can only be approximated. As a first contribution we generalize an iteration scheme called dampened Mann iteration, recently introduced in the literature. The improved scheme relaxes previous constraints on parameter sequences, allowing learning rates to converge to zero or not converge at all. While seemingly minor, this flexibility is essential to enable the implementation of chaotic iterations, where only a subset of components is updated in each step, allowing to tackle higher-dimensional problems. Additionally, by allowing learning rates to converge to zero, we can relax conditions on the convergence speed of function approximations, making the method more adaptable to various scenarios. We also show that dampened Mann iteration applies immediately to compute the expected payoff in various probabilistic models, including simple stochastic games, not covered by previous work.",
      "authors": [
        "Paolo Baldan",
        "Sebastian Gurke",
        "Barbara König",
        "Florian Wittbold"
      ],
      "primary_category": "cs.LO",
      "categories": [
        "cs.LO",
        "cs.LG"
      ],
      "published": "2026-01-22 17:36:19+00:00",
      "link": "https://arxiv.org/pdf/2601.16142v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.16139v1",
      "title": "On the Intrinsic Dimensions of Data in Kernel Learning",
      "abstract": "The manifold hypothesis suggests that the generalization performance of machine learning methods improves significantly when the intrinsic dimension of the input distribution's support is low. In the context of KRR, we investigate two alternative notions of intrinsic dimension. The first, denoted $d_ρ$, is the upper Minkowski dimension defined with respect to the canonical metric induced by a kernel function $K$ on a domain $Ω$. The second, denoted $d_K$, is the effective dimension, derived from the decay rate of Kolmogorov $n$-widths associated with $K$ on $Ω$. Given a probability measure $μ$ on $Ω$, we analyze the relationship between these $n$-widths and eigenvalues of the integral operator $φ\\to \\int_ΩK(\\cdot,x)φ(x)dμ(x)$. We show that, for a fixed domain $Ω$, the Kolmogorov $n$-widths characterize the worst-case eigenvalue decay across all probability measures $μ$ supported on $Ω$. These eigenvalues are central to understanding the generalization behavior of constrained KRR, enabling us to derive an excess error bound of order $O(n^{-\\frac{2+d_K}{2+2d_K} + ε})$ for any $ε> 0$, when the training set size $n$ is large. We also propose an algorithm that estimates upper bounds on the $n$-widths using only a finite sample from $μ$. For distributions close to uniform, we prove that $ε$-accurate upper bounds on all $n$-widths can be computed with high probability using at most $O\\left(ε^{-d_ρ}\\log\\frac{1}ε\\right)$ samples, with fewer required for small $n$. Finally, we compute the effective dimension $d_K$ for various fractal sets and present additional numerical experiments. Our results show that, for kernels such as the Laplace kernel, the effective dimension $d_K$ can be significantly smaller than the Minkowski dimension $d_ρ$, even though $d_K = d_ρ$ provably holds on regular domains.",
      "authors": [
        "Rustem Takhanov"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-22 17:32:24+00:00",
      "link": "https://arxiv.org/pdf/2601.16139v1",
      "tags": [
        "keyword:EAA"
      ]
    },
    {
      "id": "2601.16112v1",
      "title": "Variable Splitting Binary Tree Models Based on Bayesian Context Tree Models for Time Series Segmentation",
      "abstract": "We propose a variable splitting binary tree (VSBT) model based on Bayesian context tree (BCT) models for time series segmentation. Unlike previous applications of BCT models, the tree structure in our model represents interval partitioning on the time domain. Moreover, interval partitioning is represented by recursive logistic regression models. By adjusting logistic regression coefficients, our model can represent split positions at arbitrary locations within each interval. This enables more compact tree representations. For simultaneous estimation of both split positions and tree depth, we develop an effective inference algorithm that combines local variational approximation for logistic regression with the context tree weighting (CTW) algorithm. We present numerical examples on synthetic data demonstrating the effectiveness of our model and algorithm.",
      "authors": [
        "Yuta Nakahara",
        "Shota Saito",
        "Kohei Horinouchi",
        "Koshi Shimada",
        "Naoki Ichijo",
        "Manabu Kobayashi",
        "Toshiyasu Matsushima"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-22 16:58:34+00:00",
      "link": "https://arxiv.org/pdf/2601.16112v1",
      "tags": [
        "keyword:EAA"
      ]
    },
    {
      "id": "2601.16097v1",
      "title": "Adapter Fusion for Multilingual Text2Cypher with Linear and Learned Gating",
      "abstract": "Large Language Models enable users to access database using natural language interfaces using tools like Text2SQL, Text2SPARQL, and Text2Cypher, which translate user questions into structured database queries. While these systems improve database accessibility, most research focuses on English with limited multilingual support. This work investigates a scalable multilingual Text2Cypher, aiming to support new languages without re-running full fine-tuning, avoiding manual hyper-parameter tuning, and maintaining performance close to joint multilingual fine-tuning. We train language-specific LoRA adapters for English, Spanish, and Turkish and combined them via uniform linear merging or learned fusion MLP with dynamic gating. Experimental results show that the fusion MLP recovers around 75\\% of the accuracy gains from joint multilingual fine-tuning while requiring only a smaller subset of the data, outperforming linear merging across all three languages. This approach enables incremental language expansion to new languages by requiring only one LoRA adapter and a lightweight MLP retraining. Learned adapter fusion offers a practical alternative to expensive joint fine-tuning, balancing performance, data efficiency, and scalability for multilingual Text2Cypher task.",
      "authors": [
        "Makbule Gulcin Ozsoy"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-22 16:46:57+00:00",
      "link": "https://arxiv.org/pdf/2601.16097v1",
      "tags": [
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.16091v1",
      "title": "Delayed Assignments in Online Non-Centroid Clustering with Stochastic Arrivals",
      "abstract": "Clustering is a fundamental problem, aiming to partition a set of elements, like agents or data points, into clusters such that elements in the same cluster are closer to each other than to those in other clusters. In this paper, we present a new framework for studying online non-centroid clustering with delays, where elements, that arrive one at a time as points in a finite metric space, should be assigned to clusters, but assignments need not be immediate. Specifically, upon arrival, each point's location is revealed, and an online algorithm has to irrevocably assign it to an existing cluster or create a new one containing, at this moment, only this point. However, we allow decisions to be postponed at a delay cost, instead of following the more common assumption of immediate decisions upon arrival. This poses a critical challenge: the goal is to minimize both the total distance costs between points in each cluster and the overall delay costs incurred by postponing assignments. In the classic worst-case arrival model, where points arrive in an arbitrary order, no algorithm has a competitive ratio better than sublogarithmic in the number of points. To overcome this strong impossibility, we focus on a stochastic arrival model, where points' locations are drawn independently across time from an unknown and fixed probability distribution over the finite metric space. We offer hope for beyond worst-case adversaries: we devise an algorithm that is constant competitive in the sense that, as the number of points grows, the ratio between the expected overall costs of the output clustering and an optimal offline clustering is bounded by a constant.",
      "authors": [
        "Saar Cohen"
      ],
      "primary_category": "cs.MA",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-22 16:42:05+00:00",
      "link": "https://arxiv.org/pdf/2601.16091v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.16083v1",
      "title": "Probably Approximately Correct Maximum A Posteriori Inference",
      "abstract": "Computing the conditional mode of a distribution, better known as the $\\mathit{maximum\\ a\\ posteriori}$ (MAP) assignment, is a fundamental task in probabilistic inference. However, MAP estimation is generally intractable, and remains hard even under many common structural constraints and approximation schemes. We introduce $\\mathit{probably\\ approximately\\ correct}$ (PAC) algorithms for MAP inference that provide provably optimal solutions under variable and fixed computational budgets. We characterize tractability conditions for PAC-MAP using information theoretic measures that can be estimated from finite samples. Our PAC-MAP solvers are efficiently implemented using probabilistic circuits with appropriate architectures. The randomization strategies we develop can be used either as standalone MAP inference techniques or to improve on popular heuristics, fortifying their solutions with rigorous guarantees. Experiments confirm the benefits of our method in a range of benchmarks.",
      "authors": [
        "Matthew Shorvon",
        "Frederik Mallmann-Trenn",
        "David S. Watson"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-22 16:28:01+00:00",
      "link": "https://arxiv.org/pdf/2601.16083v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.16074v1",
      "title": "Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical Systems",
      "abstract": "Industrial Cyber-Physical Systems (CPS) are sensitive infrastructure from both safety and economics perspectives, making their reliability critically important. Machine Learning (ML), specifically deep learning, is increasingly integrated in industrial CPS, but the inherent complexity of ML models results in non-transparent operation. Rigorous evaluation is needed to prevent models from exhibiting unexpected behaviour on future, unseen data. Explainable AI (XAI) can be used to uncover model reasoning, allowing a more extensive analysis of behaviour. We apply XAI to to improve predictive performance of ML models intended for industrial CPS. We analyse the effects of components from time-series data decomposition on model predictions using SHAP values. Through this method, we observe evidence on the lack of sufficient contextual information during model training. By increasing the window size of data instances, informed by the XAI findings, we are able to improve model performance.",
      "authors": [
        "Annemarie Jutte",
        "Uraz Odyurt"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-22 16:18:22+00:00",
      "link": "https://arxiv.org/pdf/2601.16074v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.16072v1",
      "title": "CLASP: An online learning algorithm for Convex Losses And Squared Penalties",
      "abstract": "We study Constrained Online Convex Optimization (COCO), where a learner chooses actions iteratively, observes both unanticipated convex loss and convex constraint, and accumulates loss while incurring penalties for constraint violations. We introduce CLASP (Convex Losses And Squared Penalties), an algorithm that minimizes cumulative loss together with squared constraint violations. Our analysis departs from prior work by fully leveraging the firm non-expansiveness of convex projectors, a proof strategy not previously applied in this setting. For convex losses, CLASP achieves regret $O\\left(T^{\\max\\{β,1-β\\}}\\right)$ and cumulative squared penalty $O\\left(T^{1-β}\\right)$ for any $β\\in (0,1)$. Most importantly, for strongly convex problems, CLASP provides the first logarithmic guarantees on both regret and cumulative squared penalty. In the strongly convex case, the regret is upper bounded by $O( \\log T )$ and the cumulative squared penalty is also upper bounded by $O( \\log T )$.",
      "authors": [
        "Ricardo N. Ferreira",
        "Cláudia Soares",
        "João Xavier"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "published": "2026-01-22 16:13:52+00:00",
      "link": "https://arxiv.org/pdf/2601.16072v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.16056v1",
      "title": "Designing faster mixed integer linear programming algorithm via learning the optimal path",
      "abstract": "Designing faster algorithms for solving Mixed-Integer Linear Programming (MILP) problems is highly desired across numerous practical domains, as a vast array of complex real-world challenges can be effectively modeled as MILP formulations. Solving these problems typically employs the branch-and-bound algorithm, the core of which can be conceived as searching for a path of nodes (or sub-problems) that contains the optimal solution to the original MILP problem. Traditional approaches to finding this path rely heavily on hand-crafted, intuition-based heuristic strategies, which often suffer from unstable and unpredictable performance across different MILP problem instances. To address this limitation, we introduce DeepBound, a deep learning-based node selection algorithm that automates the learning of such human intuition from data. The core of DeepBound lies in learning to prioritize nodes containing the optimal solution, thereby improving solving efficiency. DeepBound introduces a multi-level feature fusion network to capture the node representations. To tackle the inherent node imbalance in branch-and-bound trees, DeepBound employs a pairwise training paradigm that enhances the model's ability to discriminate between nodes. Extensive experiments on three NP-hard MILP benchmarks demonstrate that DeepBound achieves superior solving efficiency over conventional heuristic rules and existing learning-based approaches, obtaining optimal feasible solutions with significantly reduced computation time. Moreover, DeepBound demonstrates strong generalization capability on large and complex instances. The analysis of its learned features reveals that the method can automatically discover more flexible and robust feature selection, which may effectively improve and potentially replace human-designed heuristic rules.",
      "authors": [
        "Ruizhi Liu",
        "Liming Xu",
        "Xulin Huang",
        "Jingyan Sui",
        "Shizhe Ding",
        "Boyang Xia",
        "Chungong Yu",
        "Dongbo Bu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 15:41:22+00:00",
      "link": "https://arxiv.org/pdf/2601.16056v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.16038v1",
      "title": "Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval",
      "abstract": "Large Language Models (LLMs) can aid synthesis planning in chemistry, but standard prompting methods often yield hallucinated or outdated suggestions. We study LLM interactions with a reaction knowledge graph by casting reaction path retrieval as a Text2Cypher (natural language to graph query) generation problem, and define single- and multi-step retrieval tasks. We compare zero-shot prompting to one-shot variants using static, random, and embedding-based exemplar selection, and assess a checklist-driven validator/corrector loop. To evaluate our framework, we consider query validity and retrieval accuracy. We find that one-shot prompting with aligned exemplars consistently performs best. Our checklist-style self-correction loop mainly improves executability in zero-shot settings and offers limited additional retrieval gains once a good exemplar is present. We provide a reproducible Text2Cypher evaluation setup to facilitate further work on KG-grounded LLMs for synthesis planning. Code is available at https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.",
      "authors": [
        "Olga Bunkova",
        "Lorenzo Di Fruscia",
        "Sophia Rupprecht",
        "Artur M. Schweidtmann",
        "Marcel J. T. Reinders",
        "Jana M. Weber"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 15:11:02+00:00",
      "link": "https://arxiv.org/pdf/2601.16038v1",
      "tags": [
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.16028v1",
      "title": "Data-Driven Conditional Flexibility Index",
      "abstract": "With the increasing flexibilization of processes, determining robust scheduling decisions has become an important goal. Traditionally, the flexibility index has been used to identify safe operating schedules by approximating the admissible uncertainty region using simple admissible uncertainty sets, such as hypercubes. Presently, available contextual information, such as forecasts, has not been considered to define the admissible uncertainty set when determining the flexibility index. We propose the conditional flexibility index (CFI), which extends the traditional flexibility index in two ways: by learning the parametrized admissible uncertainty set from historical data and by using contextual information to make the admissible uncertainty set conditional. This is achieved using a normalizing flow that learns a bijective mapping from a Gaussian base distribution to the data distribution. The admissible latent uncertainty set is constructed as a hypersphere in the latent space and mapped to the data space. By incorporating contextual information, the CFI provides a more informative estimate of flexibility by defining admissible uncertainty sets in regions that are more likely to be relevant under given conditions. Using an illustrative example, we show that no general statement can be made about data-driven admissible uncertainty sets outperforming simple sets, or conditional sets outperforming unconditional ones. However, both data-driven and conditional admissible uncertainty sets ensure that only regions of the uncertain parameter space containing realizations are considered. We apply the CFI to a security-constrained unit commitment example and demonstrate that the CFI can improve scheduling quality by incorporating temporal information.",
      "authors": [
        "Moritz Wedemeyer",
        "Eike Cramer",
        "Alexander Mitsos",
        "Manuel Dahmen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-22 14:56:10+00:00",
      "link": "https://arxiv.org/pdf/2601.16028v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.16025v1",
      "title": "EAIFD: A Fast and Scalable Algorithm for Incremental Functional Dependency Discovery",
      "abstract": "Functional dependencies (FDs) are fundamental integrity constraints in relational databases, but discovering them under incremental updates remains challenging. While static algorithms are inefficient due to full re-execution, incremental algorithms suffer from severe performance and memory bottlenecks. To address these challenges, this paper proposes EAIFD, a novel algorithm for incremental FD discovery. EAIFD maintains the partial hypergraph of difference sets and reframes the incremental FD discovery problem into minimal hitting set enumeration on hypergraph, avoiding full re-runs. EAIFD introduces two key innovations. First, a multi-attribute hash table ($MHT$) is devised for high-frequency key-value mappings of valid FDs, whose memory consumption is proven to be independent of the dataset size. Second, two-step validation strategy is developed to efficiently validate the enumerated candidates, which leverages $MHT$ to effectively reduce the validation space and then selectively loads data blocks for batch validation of remaining candidates, effectively avoiding repeated I/O operations. Experimental results on real-world datasets demonstrate the significant advantages of EAIFD. Compared to existing algorithms, EAIFD achieves up to an order-of-magnitude speedup in runtime while reducing memory usage by over two orders-of-magnitude, establishing it as a highly efficient and scalable solution for incremental FD discovery.",
      "authors": [
        "Yajuan Xu",
        "Xixian Han",
        "Xiaolong Wan"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB"
      ],
      "published": "2026-01-22 14:52:36+00:00",
      "link": "https://arxiv.org/pdf/2601.16025v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15992v1",
      "title": "Efficient Cloud-edge Collaborative Approaches to SPARQL Queries over Large RDF graphs",
      "abstract": "With the increasing use of RDF graphs, storing and querying such data using SPARQL remains a critical problem. Current mainstream solutions rely on cloud-based data management architectures, but often suffer from performance bottle- necks in environments with limited bandwidth or high system load. To address this issue, this paper explores for the first time the integration of edge computing to move graph data storage and processing to edge environments, thereby improving query performance. This approach requires offloading query processing to edge servers, which involves addressing two challenges: data localization and network scheduling. First, the data localization challenge lies in computing the subgraphs maintained on edge servers to quickly identify the servers that can handle specific queries. To address this challenge, we introduce a new concept of pattern-induced subgraphs. Second, the network scheduling challenge involves efficiently assigning queries to edge and cloud servers to optimize overall system performance. We tackle this by constructing a overall system model that jointly captures data distribution, query characteristics, network communication, and computational resources. Accordingly, we further propose a joint formulation of query assignment and computational resource allocation, modeling it as a Mixed Integer Nonlinear Programming (MINLP) problem and solve this problem using a modified branch-and-bound algorithm. Experimental results on real datasets under a real cloud platform demonstrate that our proposed method outperforms the state-of-the-art baseline methods in terms of efficiency. The codes are available on GitHub",
      "authors": [
        "Shidan Ma",
        "Peng Peng",
        "Xu Zhou",
        "M. Tamer Özsu",
        "Lei Zou",
        "Guo Chen"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB"
      ],
      "published": "2026-01-22 14:14:37+00:00",
      "link": "https://arxiv.org/pdf/2601.15992v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15984v1",
      "title": "Partially Lazy Gradient Descent for Smoothed Online Learning",
      "abstract": "We introduce $k$-lazyGD, an online learning algorithm that bridges the gap between greedy Online Gradient Descent (OGD, for $k=1$) and lazy GD/dual-averaging (for $k=T$), creating a spectrum between reactive and stable updates. We analyze this spectrum in Smoothed Online Convex Optimization (SOCO), where the learner incurs both hitting and movement costs. Our main contribution is establishing that laziness is possible without sacrificing hitting performance: we prove that $k$-lazyGD achieves the optimal dynamic regret $\\mathcal{O}(\\sqrt{(P_T+1)T})$ for any laziness slack $k$ up to $Θ(\\sqrt{T/P_T})$, where $P_T$ is the comparator path length. This result formally connects the allowable laziness to the comparator's shifts, showing that $k$-lazyGD can retain the inherently small movements of lazy methods without compromising tracking ability. We base our analysis on the Follow the Regularized Leader (FTRL) framework, and derive a matching lower bound. Since the slack depends on $P_T$, an ensemble of learners with various slacks is used, yielding a method that is provably stable when it can be, and agile when it must be.",
      "authors": [
        "Naram Mhaisen",
        "George Iosifidis"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-22 14:05:08+00:00",
      "link": "https://arxiv.org/pdf/2601.15984v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15953v1",
      "title": "Decoupling Return-to-Go for Efficient Decision Transformer",
      "abstract": "The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT's performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.",
      "authors": [
        "Yongyi Wang",
        "Hanyu Liu",
        "Lingfeng Li",
        "Bozhou Chen",
        "Ang Li",
        "Qirui Zheng",
        "Xionghui Yang",
        "Wenxin Li"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 13:42:08+00:00",
      "link": "https://arxiv.org/pdf/2601.15953v1",
      "tags": [
        "keyword:EAA"
      ]
    },
    {
      "id": "2601.15928v1",
      "title": "A Remark on Downlink Massive Random Access",
      "abstract": "In downlink massive random access (DMRA), a base station transmits messages to a typically small subset of active users, selected randomly from a massive number of total users. Explicitly encoding the identities of active users would incur a significant overhead scaling logarithmically with the number of total users. Recently, via a random coding argument, Song, Attiah and Yu have shown that the overhead can be reduced to within some upper bound irrespective of the number of total users. In this remark, recognizing that the code design for DMRA is an instance of covering arrays in combinatorics, we show that there exists deterministic construction of variable-length codes that incur an overhead no greater than $1 + log_2 e$ bits.",
      "authors": [
        "Yuchen Liao",
        "Wenyi Zhang"
      ],
      "primary_category": "cs.IT",
      "categories": [
        "cs.IT"
      ],
      "published": "2026-01-22 13:05:12+00:00",
      "link": "https://arxiv.org/pdf/2601.15928v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15915v1",
      "title": "Progressive Power Homotopy for Non-convex Optimization",
      "abstract": "We propose a novel first-order method for non-convex optimization of the form $\\max_{\\bm{w}\\in\\mathbb{R}^d}\\mathbb{E}_{\\bm{x}\\sim\\mathcal{D}}[f_{\\bm{w}}(\\bm{x})]$, termed Progressive Power Homotopy (Prog-PowerHP). The method applies stochastic gradient ascent to a surrogate objective obtained by first performing a power transformation and then Gaussian smoothing, $F_{N,σ}(\\bmμ):=\\mathbb{E}_{\\bm{w}\\sim\\mathcal{N}(\\bmμ,σ^2I_d),\\bm{x}\\sim\\mathcal{D}}[e^{Nf_w(\\bm{x})}]$, while progressively increasing the power parameter $N$ and decreasing the smoothing scale $σ$ along the optimization trajectory. We prove that, under mild regularity conditions, Prog-PowerHP converges to a small neighborhood of the global optimum with an iteration complexity scaling nearly as $O(d^2\\varepsilon^{-2})$. Empirically, Prog-PowerHP demonstrates clear advantages in phase retrieval when the samples-to-dimension ratio approaches the information-theoretic limit, and in training two-layer neural networks in under-parameterized regimes. These results suggest that Prog-PowerHP is particularly effective for navigating cluttered non-convex landscapes where standard first-order methods struggle.",
      "authors": [
        "Chen Xu"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-22 12:44:25+00:00",
      "link": "https://arxiv.org/pdf/2601.15915v1",
      "tags": [
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15904v1",
      "title": "Dynamic Server Allocation Under Stochastic Switchover on Time-Varying Links",
      "abstract": "Dynamic resource allocation to parallel queues is a cornerstone of network scheduling, yet classical solutions often fail when accounting for the overhead of switching delays to queues with superior link conditions. In particular, system performance is further degraded when switching delays are stochastic and inhomogeneous. In this domain, the myopic, Max-Weight policy struggles, as it is agnostic to switching delays. This paper introduces ACI, a non-myopic, frame-based scheduling framework that directly amortizes these switching delays. We first use a Lyapunov drift analysis to prove that backlog-driven ACI is throughput-optimal with respect to a scaled capacity region; then validate ACI's effectiveness on multi-UAV networks with an FSO backhaul. Finally, we demonstrate how adapting its core urgency metric provides the flexibility to navigate the throughput-latency trade-off.",
      "authors": [
        "Hossein Mohammadalizadeh",
        "Holger Karl"
      ],
      "primary_category": "cs.NI",
      "categories": [
        "cs.NI"
      ],
      "published": "2026-01-22 12:31:17+00:00",
      "link": "https://arxiv.org/pdf/2601.15904v1",
      "tags": [
        "keyword:EAA",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15892v1",
      "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
      "abstract": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.",
      "authors": [
        "Chenghao Fan",
        "Wen Heng",
        "Bo Li",
        "Sichen Liu",
        "Yuxuan Song",
        "Jing Su",
        "Xiaoye Qu",
        "Kai Shen",
        "Wei Wei"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-22 12:13:17+00:00",
      "link": "https://arxiv.org/pdf/2601.15892v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15876v1",
      "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
      "abstract": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.",
      "authors": [
        "Taofeng Xue",
        "Chong Peng",
        "Mianqiu Huang",
        "Linsen Guo",
        "Tiancheng Han",
        "Haozhe Wang",
        "Jianing Wang",
        "Xiaocheng Zhang",
        "Xin Yang",
        "Dengchang Zhao",
        "Jinrui Ding",
        "Xiandi Ma",
        "Yuchen Xie",
        "Peng Pei",
        "Xunliang Cai",
        "Xipeng Qiu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 11:36:43+00:00",
      "link": "https://arxiv.org/pdf/2601.15876v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15871v1",
      "title": "Why Inference in Large Models Becomes Decomposable After Training",
      "abstract": "Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.",
      "authors": [
        "Jidong Jin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-22 11:20:57+00:00",
      "link": "https://arxiv.org/pdf/2601.15871v1",
      "tags": [
        "keyword:EAA"
      ]
    },
    {
      "id": "2601.15864v1",
      "title": "Minimum Envy Graphical House Allocation Beyond Identical Valuations",
      "abstract": "House allocation is an extremely well-studied problem in the field of fair allocation, where the goal is to assign $n$ houses to $n$ agents while satisfying certain fairness criterion, e.g., envy-freeness. To model social interactions, the Graphical House Allocation framework introduces a social graph $G$, in which each vertex corresponds to an agent, and an edge $(u, v)$ corresponds to the potential of agent $u$ to envy the agent $v$, based on their allocations and valuations. In undirected social graphs, the potential for envy is in both the directions. In the Minimum Envy Graphical House Allocation (ME-GHA) problem, given a set of $n$ agents, $n$ houses, a social graph, and agent's valuation functions, the goal is to find an allocation that minimizes the total envy summed up over all the edges of $G$. Recent work, [Hosseini et al., AAMAS 2023, AAMAS 2024] studied ME-GHA in the regime of polynomial-time algorithms, and designed exact and approximation algorithms, for certain graph classes under identical agent valuations. We initiate the study of \\gha with non-identical valuations, a setting that has so far remained unexplored. We investigate the multivariate (parameterized) complexity of \\gha by identifying structural restrictions on the social graph and valuation functions that yield tractability. We also design moderately exponential-time algorithms for several graph classes, and a polynomial-time algorithm for {binary valuations that returns an allocation with envy at most one when the social graph has maximum degree at most one.",
      "authors": [
        "Tanmay Inamdar",
        "Pallavi Jain",
        "Pranjal Pandey"
      ],
      "primary_category": "cs.GT",
      "categories": [
        "cs.GT",
        "cs.DS"
      ],
      "published": "2026-01-22 11:12:09+00:00",
      "link": "https://arxiv.org/pdf/2601.15864v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15861v1",
      "title": "Finding large sparse induced subgraphs in graphs of small (but not very small) tree-independence number",
      "abstract": "The independence number of a tree decomposition is the size of a largest independent set contained in a single bag. The tree-independence number of a graph $G$ is the minimum independence number of a tree decomposition of $G$. As shown recently by Lima et al. [ESA~2024], a large family of optimization problems asking for a maximum-weight induced subgraph of bounded treewidth, satisfying a given \\textsf{CMSO}$_2$ property, can be solved in polynomial time in graphs whose tree-independence number is bounded by some constant~$k$.   However, the complexity of the algorithm of Lima et al. grows rapidly with $k$, making it useless if the tree-independence number is superconstant. In this paper we present a refined version of the algorithm. We show that the same family of problems can be solved in time~$n^{\\mathcal{O}(k)}$, where $n$ is the number of vertices of the instance, $k$ is the tree-independence number, and the $\\mathcal{O}(\\cdot)$-notation hides factors depending on the treewidth bound of the solution and the considered \\textsf{CMSO}$_2$ property.   This running time is quasipolynomial for classes of graphs with polylogarithmic tree-independence number; several such classes were recently discovered. Furthermore, the running time is subexponential for many natural classes of geometric intersection graphs -- namely, ones that admit balanced clique-based separators of sublinear size.",
      "authors": [
        "Daniel Lokshtanov",
        "Michał Pilipczuk",
        "Paweł Rzążewski"
      ],
      "primary_category": "cs.DS",
      "categories": [
        "cs.DS"
      ],
      "published": "2026-01-22 11:08:49+00:00",
      "link": "https://arxiv.org/pdf/2601.15861v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15860v1",
      "title": "STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion",
      "abstract": "Table retrieval is the task of retrieving the most relevant tables from large-scale corpora given natural language queries. However, structural and semantic discrepancies between unstructured text and structured tables make embedding alignment particularly challenging. Recent methods such as QGpT attempt to enrich table semantics by generating synthetic queries, yet they still rely on coarse partial-table sampling and simple fusion strategies, which limit semantic diversity and hinder effective query-table alignment. We propose STAR (Semantic Table Representation), a lightweight framework that improves semantic table representation through semantic clustering and weighted fusion. STAR first applies header-aware K-means clustering to group semantically similar rows and selects representative centroid instances to construct a diverse partial table. It then generates cluster-specific synthetic queries to comprehensively cover the table's semantic space. Finally, STAR employs weighted fusion strategies to integrate table and query embeddings, enabling fine-grained semantic alignment. This design enables STAR to capture complementary information from structured and textual sources, improving the expressiveness of table representations. Experiments on five benchmarks show that STAR achieves consistently higher Recall than QGpT on all datasets, demonstrating the effectiveness of semantic clustering and adaptive weighted fusion for robust table representation. Our code is available at https://github.com/adsl135789/STAR.",
      "authors": [
        "Shui-Hsiang Hsu",
        "Tsung-Hsiang Chou",
        "Chen-Jui Yu",
        "Yao-Chung Fan"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-01-22 11:08:46+00:00",
      "link": "https://arxiv.org/pdf/2601.15860v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15853v1",
      "title": "Practical applications of Set Shaping Theory to Non-Uniform Sequences",
      "abstract": "Set Shaping Theory (SST) moves beyond the classical fixed-space model by constructing bijective mappings the original sequence set into structured regions of a larger sequence space. These shaped subsets are characterized by a reduced average information content, measured by the product of the empirical entropy and the length, yielding (N +k)H0(f(s)) < NH0(s), which represents the universal coding limit when the source distribution is unknown. The principal experimental difficulty in applying Set Shaping Theory to non-uniform sequences arises from the need to order the sequences of both the original and transformed sets according to their information content. An exact ordering of these sets entails exponential complexity, rendering a direct implementation impractical. In this article, we show that this obstacle can be overcome by performing an approximate but informative ordering that preserves the structural requirements of SST while achieving the shaping gain predicted by the theory. This result extends previous experimental findings obtained for uniformly distributed sequences and demonstrates that the shaping advantage of SST persists for non-uniform sequences. Finally, to ensure full reproducibility, the software implementing the proposed method has been made publicly available on GitHub, enabling independent verification of the results reported in this work",
      "authors": [
        "A. Schmidt",
        "A. Vdberg",
        "A. Petit"
      ],
      "primary_category": "cs.IT",
      "categories": [
        "cs.IT"
      ],
      "published": "2026-01-22 11:01:15+00:00",
      "link": "https://arxiv.org/pdf/2601.15853v1",
      "tags": [
        "keyword:EAA",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15849v1",
      "title": "CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval",
      "abstract": "General-purpose embedding models have demonstrated strong performance in text retrieval but remain suboptimal for table retrieval, where highly structured content leads to semantic compression and query-table mismatch. Recent LLM-based retrieval augmentation methods mitigate this issue by generating synthetic queries, yet they often rely on heuristic partial-table selection and seldom leverage these synthetic queries as supervision to improve the embedding model. We introduce CGPT, a training framework that enhances table retrieval through LLM-generated supervision. CGPT constructs semantically diverse partial tables by clustering table instances using K-means and sampling across clusters to broaden semantic coverage. An LLM then generates synthetic queries for these partial tables, which are used in hard-negative contrastive fine-tuning to refine the embedding model. Experiments across four public benchmarks (MimoTable, OTTQA, FetaQA, and E2E-WTQ) show that CGPT consistently outperforms retrieval baselines, including QGpT, with an average R@1 improvement of 16.54 percent. In a unified multi-domain corpus setting, CGPT further demonstrates strong cross-domain generalization and remains effective even when using smaller LLMs for synthetic query generation. These results indicate that semantically guided partial-table construction, combined with contrastive training from LLM-generated supervision, provides an effective and scalable paradigm for large-scale table retrieval. Our code is available at https://github.com/yumeow0122/CGPT.",
      "authors": [
        "Tsung-Hsiang Chou",
        "Chen-Jui Yu",
        "Shui-Hsiang Hsu",
        "Yao-Chung Fan"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-01-22 10:58:56+00:00",
      "link": "https://arxiv.org/pdf/2601.15849v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15808v1",
      "title": "Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification",
      "abstract": "Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.",
      "authors": [
        "Yuxuan Wan",
        "Tianqing Fang",
        "Zaitang Li",
        "Yintong Huo",
        "Wenxuan Wang",
        "Haitao Mi",
        "Dong Yu",
        "Michael R. Lyu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 09:47:31+00:00",
      "link": "https://arxiv.org/pdf/2601.15808v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15807v1",
      "title": "Algebraic Statistics in OSCAR",
      "abstract": "We introduce the AlgebraicStatistics section of the OSCAR computer algebra system. We give an overview of its extensible design and highlight its features including serialization of data types for sharing results and creating databases, and state-of-the-art implicitization algorithms.",
      "authors": [
        "Tobias Boege",
        "Antony Della Vecchia",
        "Marina Garrote-López",
        "Benjamin Hollering"
      ],
      "primary_category": "stat.CO",
      "categories": [
        "stat.CO",
        "cs.NE",
        "math.AC",
        "math.ST"
      ],
      "published": "2026-01-22 09:46:08+00:00",
      "link": "https://arxiv.org/pdf/2601.15807v1",
      "tags": [
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15804v1",
      "title": "Entangled Life and Code: A Computational Design Taxonomy for Synergistic Bio-Digital Systems",
      "abstract": "Bio-digital systems that merge microbial life with technology promise new modes of computation, combining biological adaptability with digital precision. Yet realizing this potential symbiotically -- where biological and digital agents co-adapt and co-process -- remains elusive, largely due to the absence of a shared vocabulary bridging biology and computing. Consequently, microbes are often constrained to uni-directional roles, functioning as sensors or actuators rather than as active, computational partners in bio-digital systems. In response, we propose a taxonomy and pathways that articulate and expand the roles of biological and digital entities for synergetic bio-digital computation. Using this taxonomy, we analysed 70 systems across HCI, design, and engineering, identifying how biological mechanisms can be mapped onto computational abstractions. We argue that such mappings enable computationally actionable directions that foster richer and reciprocal relationships in bio-digital systems, supporting regenerative ecologies across time and scale while inspiring new paradigms for computation in HCI.",
      "authors": [
        "Zoë Breed",
        "Elvin Karana",
        "Alessandro Bozzon",
        "Katherine W. Song"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC"
      ],
      "published": "2026-01-22 09:41:11+00:00",
      "link": "https://arxiv.org/pdf/2601.15804v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15778v1",
      "title": "Agentic Confidence Calibration",
      "abstract": "AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.",
      "authors": [
        "Jiaxin Zhang",
        "Caiming Xiong",
        "Chien-Sheng Wu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-22 09:08:25+00:00",
      "link": "https://arxiv.org/pdf/2601.15778v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15761v1",
      "title": "Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning",
      "abstract": "Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \\textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.",
      "authors": [
        "Xiefeng Wu",
        "Mingyu Hu",
        "Shu Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 08:51:16+00:00",
      "link": "https://arxiv.org/pdf/2601.15761v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15758v1",
      "title": "NL4ST: A Natural Language Query Tool for Spatio-Temporal Databases",
      "abstract": "The advancement of mobile computing devices and positioning technologies has led to an explosive growth of spatio-temporal data managed in databases. Representative queries over such data include range queries, nearest neighbor queries, and join queries. However, formulating those queries usually requires domain-specific expertise and familiarity with executable query languages, which would be a challenging task for non-expert users. It leads to a great demand for well-supported natural language queries (NLQs) in spatio-temporal databases. To bridge the gap between non-experts and query plans in databases, we present NL4ST, an interactive tool that allows users to query spatio-temporal databases in natural language. NL4ST features a three-layer architecture: (i) knowledge base and corpus for knowledge preparation, (ii) natural language understanding for entity linking, and (iii) generating physical plans. Our demonstration will showcase how NL4ST provides effective spatio-temporal physical plans, verified by using four real and synthetic datasets. We make NL4ST online and provide the demo video at https://youtu.be/-J1R7R5WoqQ.",
      "authors": [
        "Xieyang Wang",
        "Mengyi Liu",
        "Weijia Yi",
        "Jianqiu Xu",
        "Raymond Chi-Wing Wong"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB"
      ],
      "published": "2026-01-22 08:48:32+00:00",
      "link": "https://arxiv.org/pdf/2601.15758v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15751v1",
      "title": "Tabular Incremental Inference",
      "abstract": "Tabular data is a fundamental form of data structure. The evolution of table analysis tools reflects humanity's continuous progress in data acquisition, management, and processing. The dynamic changes in table columns arise from technological advancements, changing needs, data integration, etc. However, the standard process of training AI models on tables with fixed columns and then performing inference is not suitable for handling dynamically changed tables. Therefore, new methods are needed for efficiently handling such tables in an unsupervised manner. In this paper, we introduce a new task, Tabular Incremental Inference (TabII), which aims to enable trained models to incorporate new columns during the inference stage, enhancing the practicality of AI models in scenarios where tables are dynamically changed. Furthermore, we demonstrate that this new task can be framed as an optimization problem based on the information bottleneck theory, which emphasizes that the key to an ideal tabular incremental inference approach lies in minimizing mutual information between tabular data and representation while maximizing between representation and task labels. Under this guidance, we design a TabII method with Large Language Model placeholders and Pretrained TabAdapter to provide external knowledge and Incremental Sample Condensation blocks to condense the task-relevant information given by incremental column attributes. Experimental results across eight public datasets show that TabII effectively utilizes incremental attributes, achieving state-of-the-art performance.",
      "authors": [
        "Xinda Chen",
        "Xing Zhen",
        "Hanyu Zhang",
        "Weimin Tan",
        "Bo Yan"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 08:24:31+00:00",
      "link": "https://arxiv.org/pdf/2601.15751v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15738v1",
      "title": "LLM-Assisted Automatic Dispatching Rule Design for Dynamic Flexible Assembly Flow Shop Scheduling",
      "abstract": "Dynamic multi-product delivery environments demand rapid coordination of part completion and product-level kitting within hybrid processing and assembly systems to satisfy strict hierarchical supply constraints. The flexible assembly flow shop scheduling problem formally defines dependencies for multi-stage kitting, yet dynamic variants make designing integrated scheduling rules under multi-level time coupling highly challenging. Existing automated heuristic design methods, particularly genetic programming constrained to fixed terminal symbol sets, struggle to capture and leverage dynamic uncertainties and hierarchical dependency information under transient decision states. This study develops an LLM-assisted Dynamic Rule Design framework (LLM4DRD) that automatically evolves integrated online scheduling rules adapted to scheduling features. Firstly, multi-stage processing and assembly supply decisions are transformed into feasible directed edge orderings based on heterogeneous graph. Then, an elite knowledge guided initialization embeds advanced design expertise into initial rules to enhance initial quality. Additionally, a dual-expert mechanism is introduced in which LLM-A evolutionary code to generate candidate rules and LLM-S conducts scheduling evaluation, while dynamic feature-fitting rule evolution combined with hybrid evaluation enables continuous improvement and extracts adaptive rules with strong generalization capability. A series of experiments are conducted to validate the effectiveness of the method. The average tardiness of LLM4DRD is 3.17-12.39% higher than state-of-the-art methods in 20 practical instances used for training and testing, respectively. In 24 scenarios with different resource configurations, order loads, and disturbance levels totaling 480 instances, it achieves 11.10% higher performance than the second best competitor, exhibiting excellent robustness.",
      "authors": [
        "Junhao Qiu",
        "Haoyang Zhuang",
        "Fei Liu",
        "Jianjun Liu",
        "Qingfu Zhang"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-01-22 08:06:40+00:00",
      "link": "https://arxiv.org/pdf/2601.15738v1",
      "tags": [
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15729v1",
      "title": "DualShield: Safe Model Predictive Diffusion via Reachability Analysis for Interactive Autonomous Driving",
      "abstract": "Diffusion models have emerged as a powerful approach for multimodal motion planning in autonomous driving. However, their practical deployment is typically hindered by the inherent difficulty in enforcing vehicle dynamics and a critical reliance on accurate predictions of other agents, making them prone to safety issues under uncertain interactions. To address these limitations, we introduce DualShield, a planning and control framework that leverages Hamilton-Jacobi (HJ) reachability value functions in a dual capacity. First, the value functions act as proactive guidance, steering the diffusion denoising process towards safe and dynamically feasible regions. Second, they form a reactive safety shield using control barrier-value functions (CBVFs) to modify the executed actions and ensure safety. This dual mechanism preserves the rich exploration capabilities of diffusion models while providing principled safety assurance under uncertain and even adversarial interactions. Simulations in challenging unprotected U-turn scenarios demonstrate that DualShield significantly improves both safety and task efficiency compared to leading methods from different planning paradigms under uncertainty.",
      "authors": [
        "Rui Yang",
        "Lei Zheng",
        "Ruoyu Yao",
        "Jun Ma"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ],
      "published": "2026-01-22 07:56:36+00:00",
      "link": "https://arxiv.org/pdf/2601.15729v1",
      "tags": [
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15728v1",
      "title": "Benchmarking Text-to-Python against Text-to-SQL: The Impact of Explicit Logic and Ambiguity",
      "abstract": "While Text-to-SQL remains the dominant approach for database interaction, real-world analytics increasingly require the flexibility of general-purpose programming languages such as Python or Pandas to manage file-based data and complex analytical workflows. Despite this growing need, the reliability of Text-to-Python in core data retrieval remains underexplored relative to the mature SQL ecosystem. To address this gap, we introduce BIRD-Python, a benchmark designed for cross-paradigm evaluation. We systematically refined the original dataset to reduce annotation noise and align execution semantics, thereby establishing a consistent and standardized baseline for comparison. Our analysis reveals a fundamental paradigmatic divergence: whereas SQL leverages implicit DBMS behaviors through its declarative structure, Python requires explicit procedural logic, making it highly sensitive to underspecified user intent. To mitigate this challenge, we propose the Logic Completion Framework (LCF), which resolves ambiguity by incorporating latent domain knowledge into the generation process. Experimental results show that (1) performance differences primarily stem from missing domain context rather than inherent limitations in code generation, and (2) when these gaps are addressed, Text-to-Python achieves performance parity with Text-to-SQL. These findings establish Python as a viable foundation for analytical agents-provided that systems effectively ground ambiguous natural language inputs in executable logical specifications. Resources are available at https://anonymous.4open.science/r/Bird-Python-43B7/.",
      "authors": [
        "Hangle Hu",
        "Chenyu Hou",
        "Bin Cao",
        "Ruizhe Li"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "published": "2026-01-22 07:54:45+00:00",
      "link": "https://arxiv.org/pdf/2601.15728v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15727v1",
      "title": "Towards Automated Kernel Generation in the Era of LLMs",
      "abstract": "The performance of modern AI systems is fundamentally constrained by the quality of their underlying kernels, which translate high-level algorithmic semantics into low-level hardware operations. Achieving near-optimal kernels requires expert-level understanding of hardware architectures and programming models, making kernel engineering a critical but notoriously time-consuming and non-scalable process. Recent advances in large language models (LLMs) and LLM-based agents have opened new possibilities for automating kernel generation and optimization. LLMs are well-suited to compress expert-level kernel knowledge that is difficult to formalize, while agentic systems further enable scalable optimization by casting kernel development as an iterative, feedback-driven loop. Rapid progress has been made in this area. However, the field remains fragmented, lacking a systematic perspective for LLM-driven kernel generation. This survey addresses this gap by providing a structured overview of existing approaches, spanning LLM-based approaches and agentic optimization workflows, and systematically compiling the datasets and benchmarks that underpin learning and evaluation in this domain. Moreover, key open challenges and future research directions are further outlined, aiming to establish a comprehensive reference for the next generation of automated kernel optimization. To keep track of this field, we maintain an open-source GitHub repository at https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation.",
      "authors": [
        "Yang Yu",
        "Peiyu Zang",
        "Chi Hsu Tsai",
        "Haiming Wu",
        "Yixin Shen",
        "Jialing Zhang",
        "Haoyu Wang",
        "Zhiyou Xiao",
        "Jingze Shi",
        "Yuyu Luo",
        "Wentao Zhang",
        "Chunlei Men",
        "Guang Liu",
        "Yonghua Lin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-01-22 07:53:52+00:00",
      "link": "https://arxiv.org/pdf/2601.15727v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15726v1",
      "title": "Profit Maximization for Viral Marketing in Online Social Networks using Two Phase Diffusion Approach",
      "abstract": "Now-a-days, Online Social Networks (OSNs) are extensively used by different commercial houses for viral marketing. The key problem that arises in this context is to choose a limited number of highly influential users as the initial adopters of a brand such that the influence regarding the brand in the network gets maximized. Deviating from this standard setting, in this paper, we study the problem where every user of the network is associated with a selection cost and a benefit value. This benefit value can be earned from the user if (s)he is influenced by the brand. A fixed amount of budget is allocated for selecting the seed users. The goal of initial adopters is to choose a set of seed users within the budget such that the profit is maximized. We propose a two phase diffusion model for this problem where the goal is to split the diffusion process into two phases, and hence, split the budget into two halves. First, we spend the first half budget to select seed users for the first phase and observe the diffusion for a few rounds and then deploy the seed users for the second phase and successively complete the diffusion process. We prove several properties of the two phase influence function. Three solution approaches have been proposed for our problem with detailed analysis and illustrative examples. We conduct a number of experiments with three real-world social network datasets. From the experiments, we observe that the two phase diffusion approach leads to more amount of profit compared to the single-phase diffusion. In particular, for most instances, this improvement is greater than 18% and reaching as high as 40% by the proposed methodologies.",
      "authors": [
        "Poonam Sharma",
        "Suman Banerjee"
      ],
      "primary_category": "cs.SI",
      "categories": [
        "cs.SI"
      ],
      "published": "2026-01-22 07:51:17+00:00",
      "link": "https://arxiv.org/pdf/2601.15726v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15723v1",
      "title": "Generalized Information Inequalities via Submodularity, and Two Combinatorial Problems",
      "abstract": "It is well known that there is a strong connection between entropy inequalities and submodularity, since the entropy of a collection of random variables is a submodular function. Unifying frameworks for information inequalities arising from submodularity were developed by Madiman and Tetali (2010) and Sason (2022). Madiman and Tetali (2010) established strong and weak fractional inequalities that subsume classical results such as Han's inequality and Shearer's lemma. Sason (2022) introduced a convex-functional framework for generalizing Han's inequality, and derived unified inequalities for submodular and supermodular functions. In this work, we build on these frameworks and make three contributions. First, we establish convex-functional generalizations of the strong and weak Madiman and Tetali inequalities for submodular functions. Second, using a special case of the strong Madiman-Tetali inequality, we derive a new Loomis-Whitney-type projection inequality for finite point sets in $\\mathbb{R}^d$, which improves upon the classical Loomis-Whitney bound by incorporating slice-level structural information. Finally, we study an extremal graph theory problem that recovers and extends the previously known results of Sason (2022) and Boucheron et al., employing Shearer's lemma in contrast to the use of Han's inequality in those works.",
      "authors": [
        "Gunank Jakhar",
        "Gowtham R. Kurri",
        "Suryajith Chillara",
        "Vinod M. Prabhakaran"
      ],
      "primary_category": "cs.IT",
      "categories": [
        "cs.IT",
        "math.CO"
      ],
      "published": "2026-01-22 07:47:27+00:00",
      "link": "https://arxiv.org/pdf/2601.15723v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15722v1",
      "title": "Communication-efficient Federated Graph Classification via Generative Diffusion Modeling",
      "abstract": "Graph Neural Networks (GNNs) unlock new ways of learning from graph-structured data, proving highly effective in capturing complex relationships and patterns. Federated GNNs (FGNNs) have emerged as a prominent distributed learning paradigm for training GNNs over decentralized data. However, FGNNs face two significant challenges: high communication overhead from multiple rounds of parameter exchanges and non-IID data characteristics across clients. To address these issues, we introduce CeFGC, a novel FGNN paradigm that facilitates efficient GNN training over non-IID data by limiting communication between the server and clients to three rounds only. The core idea of CeFGC is to leverage generative diffusion models to minimize direct client-server communication. Each client trains a generative diffusion model that captures its local graph distribution and shares this model with the server, which then redistributes it back to all clients. Using these generative models, clients generate synthetic graphs combined with their local graphs to train local GNN models. Finally, clients upload their model weights to the server for aggregation into a global GNN model. We theoretically analyze the I/O complexity of communication volume to show that CeFGC reduces to a constant of three communication rounds only. Extensive experiments on several real graph datasets demonstrate the effectiveness and efficiency of CeFGC against state-of-the-art competitors, reflecting our superior performance on non-IID graphs by aligning local and global model objectives and enriching the training set with diverse graphs.",
      "authors": [
        "Xiuling Wang",
        "Xin Huang",
        "Haibo Hu",
        "Jianliang Xu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-22 07:46:47+00:00",
      "link": "https://arxiv.org/pdf/2601.15722v1",
      "tags": [
        "keyword:EAA",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15717v1",
      "title": "Investigation of the Generalisation Ability of Genetic Programming-evolved Scheduling Rules in Dynamic Flexible Job Shop Scheduling",
      "abstract": "Dynamic Flexible Job Shop Scheduling (DFJSS) is a complex combinatorial optimisation problem that requires simultaneous machine assignment and operation sequencing decisions in dynamic production environments. Genetic Programming (GP) has been widely applied to automatically evolve scheduling rules for DFJSS. However, existing studies typically train and test GP-evolved rules on DFJSS instances of the same type, which differ only by random seeds rather than by structural characteristics, leaving their cross-type generalisation ability largely unexplored. To address this gap, this paper systematically investigates the generalisation ability of GP-evolved scheduling rules under diverse DFJSS conditions. A series of experiments are conducted across multiple dimensions, including problem scale (i.e., the number of machines and jobs), key job shop parameters (e.g., utilisation level), and data distributions, to analyse how these factors influence GP performance on unseen instance types. The results show that good generalisation occurs when the training instances contain more jobs than the test instances while keeping the number of machines fixed, and when both training and test instances have similar scales or job shop parameters. Further analysis reveals that the number and distribution of decision points in DFJSS instances play a crucial role in explaining these performance differences. Similar decision point distributions lead to better generalisation, whereas significant discrepancies result in a marked degradation of performance. Overall, this study provides new insights into the generalisation ability of GP in DFJSS and highlights the necessity of evolving more generalisable GP rules capable of handling heterogeneous DFJSS instances effectively.",
      "authors": [
        "Luyao Zhu",
        "Fangfang Zhang",
        "Yi Mei",
        "Mengjie Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 07:38:27+00:00",
      "link": "https://arxiv.org/pdf/2601.15717v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15715v1",
      "title": "Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind",
      "abstract": "Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication under severe information asymmetry rather than a simple technical debate. Consequently, current approaches struggle as they largely imitate surface-level linguistics, missing the essential element of perspective-taking required for effective persuasion. In this paper, we introduce RebuttalAgent, the first framework to ground academic rebuttal in Theory of Mind (ToM), operationalized through a ToM-Strategy-Response (TSR) pipeline that models reviewer mental state, formulates persuasion strategy, and generates strategy-grounded response. To train our agent, we construct RebuttalBench, a large-scale dataset synthesized via a novel critique-and-refine approach. Our training process consists of two stages, beginning with a supervised fine-tuning phase to equip the agent with ToM-based analysis and strategic planning capabilities, followed by a reinforcement learning phase leveraging the self-reward mechanism for scalable self-improvement. For reliable and efficient automated evaluation, we further develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of multi-source rebuttal data, which achieves scoring consistency with human preferences surpassing powerful judge GPT-4.1. Extensive experiments show RebuttalAgent significantly outperforms the base model by an average of 18.3% on automated metrics, while also outperforming advanced proprietary models across both automated and human evaluations. Disclaimer: the generated rebuttal content is for reference only to inspire authors and assist in drafting. It is not intended to replace the author's own critical analysis and response.",
      "authors": [
        "Zhitao He",
        "Zongwei Lyu",
        "Yi R Fung"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-22 07:36:48+00:00",
      "link": "https://arxiv.org/pdf/2601.15715v1",
      "tags": [
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15714v1",
      "title": "Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in Trustworthy LLMs",
      "abstract": "We propose Zero-Error Horizon (ZEH) for trustworthy LLMs, which represents the maximum range that a model can solve without any errors. While ZEH itself is simple, we demonstrate that evaluating the ZEH of state-of-the-art LLMs yields abundant insights. For example, by evaluating the ZEH of GPT-5.2, we found that GPT-5.2 cannot even compute the parity of a short string like 11000, and GPT-5.2 cannot determine whether the parentheses in ((((()))))) are balanced. This is surprising given the excellent capabilities of GPT-5.2. The fact that LLMs make mistakes on such simple problems serves as an important lesson when applying LLMs to safety-critical domains. By applying ZEH to Qwen2.5 and conducting detailed analysis, we found that while ZEH correlates with accuracy, the detailed behaviors differ, and ZEH provides clues about the emergence of algorithmic capabilities. Finally, while computing ZEH incurs significant computational cost, we discuss how to mitigate this cost by achieving up to one order of magnitude speedup using tree structures and online softmax.",
      "authors": [
        "Ryoma Sato"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-22 07:36:01+00:00",
      "link": "https://arxiv.org/pdf/2601.15714v1",
      "tags": [
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15709v1",
      "title": "AgentSM: Semantic Memory for Agentic Text-to-SQL",
      "abstract": "Recent advances in LLM-based Text-to-SQL have achieved remarkable gains on public benchmarks such as BIRD and Spider. Yet, these systems struggle to scale in realistic enterprise settings with large, complex schemas, diverse SQL dialects, and expensive multi-step reasoning. Emerging agentic approaches show potential for adaptive reasoning but often suffer from inefficiency and instability-repeating interactions with databases, producing inconsistent outputs, and occasionally failing to generate valid answers. To address these challenges, we introduce Agent Semantic Memory (AgentSM), an agentic framework for Text-to-SQL that builds and leverages interpretable semantic memory. Instead of relying on raw scratchpads or vector retrieval, AgentSM captures prior execution traces-or synthesizes curated ones-as structured programs that directly guide future reasoning. This design enables systematic reuse of reasoning paths, which allows agents to scale to larger schemas, more complex questions, and longer trajectories efficiently and reliably. Compared to state-of-the-art systems, AgentSM achieves higher efficiency by reducing average token usage and trajectory length by 25% and 35%, respectively, on the Spider 2.0 benchmark. It also improves execution accuracy, reaching a state-of-the-art accuracy of 44.8% on the Spider 2.0 Lite benchmark.",
      "authors": [
        "Asim Biswal",
        "Chuan Lei",
        "Xiao Qin",
        "Aodong Li",
        "Balakrishnan Narayanaswamy",
        "Tim Kraska"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.DB",
        "cs.LG"
      ],
      "published": "2026-01-22 07:31:19+00:00",
      "link": "https://arxiv.org/pdf/2601.15709v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15703v1",
      "title": "Agentic Uncertainty Quantification",
      "abstract": "Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.",
      "authors": [
        "Jiaxin Zhang",
        "Prafulla Kumar Choubey",
        "Kung-Hsiang Huang",
        "Caiming Xiong",
        "Chien-Sheng Wu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-22 07:16:26+00:00",
      "link": "https://arxiv.org/pdf/2601.15703v1",
      "tags": [
        "keyword:EAA"
      ]
    },
    {
      "id": "2601.15697v1",
      "title": "Balancing Security and Privacy: The Pivotal Role of AI in Modern Healthcare Systems",
      "abstract": "As digital threats continue to grow, organizations must find ways to enhance security while protecting user privacy. This paper explores how artificial intelligence (AI) plays a crucial role in achieving this balance. AI technologies can improve security by detecting threats, monitoring systems, and automating responses. However, using AI also raises privacy concerns that need careful consideration.We examine real-world examples from the healthcare sector to illustrate how organizations can implement AI solutions that strengthen security without compromising patient privacy. Additionally, we discuss the importance of creating transparent AI systems and adhering to privacy regulations.Ultimately, this paper provides insights and recommendations for integrating AI into healthcare security practices, helping organizations navigate the challenges of modern management while keeping patient data safe.",
      "authors": [
        "Binu V P",
        "Deepthy K Bhaskar",
        "Minimol B"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "published": "2026-01-22 06:51:45+00:00",
      "link": "https://arxiv.org/pdf/2601.15697v1",
      "tags": [
        "keyword:EAA"
      ]
    },
    {
      "id": "2601.15690v1",
      "title": "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models",
      "abstract": "While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \\textbf{advanced reasoning} to optimize computation and trigger self-correction; in \\textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \\textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.",
      "authors": [
        "Jiaxin Zhang",
        "Wendi Cui",
        "Zhuohang Li",
        "Lifu Huang",
        "Bradley Malin",
        "Caiming Xiong",
        "Chien-Sheng Wu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "stat.AP"
      ],
      "published": "2026-01-22 06:21:31+00:00",
      "link": "https://arxiv.org/pdf/2601.15690v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15688v1",
      "title": "Performance-guided Reinforced Active Learning for Object Detection",
      "abstract": "Active learning (AL) strategies aim to train high-performance models with minimal labeling efforts, only selecting the most informative instances for annotation. Current approaches to evaluating data informativeness predominantly focus on the data's distribution or intrinsic information content and do not directly correlate with downstream task performance, such as mean average precision (mAP) in object detection. Thus, we propose Performance-guided (i.e. mAP-guided) Reinforced Active Learning for Object Detection (MGRAL), a novel approach that leverages the concept of expected model output changes as informativeness. To address the combinatorial explosion challenge of batch sample selection and the non-differentiable correlation between model performance and selected batches, MGRAL skillfully employs a reinforcement learning-based sampling agent that optimizes selection using policy gradient with mAP improvement as reward. Moreover, to reduce the computational overhead of mAP estimation with unlabeled samples, MGRAL utilizes an unsupervised way with fast look-up tables, ensuring feasible deployment. We evaluate MGRAL's active learning performance on detection tasks over PASCAL VOC and COCO benchmarks. Our approach demonstrates the highest AL curve with convincing visualizations, establishing a new paradigm in reinforcement learning-driven active object detection.",
      "authors": [
        "Zhixuan Liang",
        "Xingyu Zeng",
        "Rui Zhao",
        "Ping Luo"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-01-22 06:17:08+00:00",
      "link": "https://arxiv.org/pdf/2601.15688v1",
      "tags": [
        "keyword:EAA"
      ]
    },
    {
      "id": "2601.15686v1",
      "title": "Beyond Hard Writes and Rigid Preservation: Soft Recursive Least-Squares for Lifelong LLM Editing",
      "abstract": "Model editing updates a pre-trained LLM with new facts or rules without re-training, while preserving unrelated behavior. In real deployment, edits arrive as long streams, and existing editors often face a plasticity-stability dilemma: locate-then-edit \"hard writes\" can accumulate interference over time, while null-space-style \"hard preservation\" preserves only what is explicitly constrained, so past edits can be overwritten and unconstrained behaviors may deviate, degrading general capabilities in the many-edits regime. We propose RLSEdit, a recursive least-squares editor for long sequential editing. RLSEdit formulates editing as an online quadratic optimization with soft constraints, minimizing a cumulative key-value fitting objective with two regularizers that control for both deviation from the pre-trained weights and from a designated anchor mapping. The resulting update admits an efficient online recursion via the Woodbury identity, with per-edit cost independent of history length and scaling only with the current edit size. We further provide deviation bounds and an asymptotic characterization of the adherence-preservation trade-off in the many-edits regime. Experiments on multiple model families demonstrate stable scaling to 10K edits, outperforming strong baselines in both edit success and holistic stability -- crucially retaining early edits, and preserving general capabilities on GLUE and held-out reasoning/code benchmarks.",
      "authors": [
        "Xinyu Wang",
        "Sicheng Lyu",
        "Yu Gu",
        "Jerry Huang",
        "Peng Lu",
        "Yufei Cui",
        "Xiao-Wen Chang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-22 06:11:44+00:00",
      "link": "https://arxiv.org/pdf/2601.15686v1",
      "tags": [
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15678v1",
      "title": "Connect the Dots: Knowledge Graph-Guided Crawler Attack on Retrieval-Augmented Generation Systems",
      "abstract": "Retrieval-augmented generation (RAG) systems integrate document retrieval with large language models and have been widely adopted. However, in privacy-related scenarios, RAG introduces a new privacy risk: adversaries can issue carefully crafted queries to exfiltrate sensitive content from the underlying corpus gradually. Although recent studies have demonstrated multi-turn extraction attacks, they rely on heuristics and fail to perform long-term extraction planning. To address these limitations, we formulate the RAG extraction attack as an adaptive stochastic coverage problem (ASCP). In ASCP, each query is treated as a probabilistic action that aims to maximize conditional marginal gain (CMG), enabling principled long-term planning under uncertainty. However, integrating ASCP with practical RAG attack faces three key challenges: unobservable CMG, intractability in the action space, and feasibility constraints. To overcome these challenges, we maintain a global attacker-side state to guide the attack. Building on this idea, we introduce RAGCRAWLER, which builds a knowledge graph to represent revealed information, uses this global state to estimate CMG, and plans queries in semantic space that target unretrieved regions. In comprehensive experiments across diverse RAG architectures and datasets, our proposed method, RAGCRAWLER, consistently outperforms all baselines. It achieves up to 84.4% corpus coverage within a fixed query budget and deliver an average improvement of 20.7% over the top-performing baseline. It also maintains high semantic fidelity and strong content reconstruction accuracy with low attack cost. Crucially, RAGCRAWLER proves its robustness by maintaining effectiveness against advanced RAG systems employing query rewriting and multi-query retrieval strategies. Our work reveals significant security gaps and highlights the pressing need for stronger safeguards for RAG.",
      "authors": [
        "Mengyu Yao",
        "Ziqi Zhang",
        "Ning Luo",
        "Shaofei Li",
        "Yifeng Cai",
        "Xiangqun Chen",
        "Yao Guo",
        "Ding Li"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2026-01-22 05:59:42+00:00",
      "link": "https://arxiv.org/pdf/2601.15678v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15673v1",
      "title": "Enhancing guidance for missing data in diffusion-based sequential recommendation",
      "abstract": "Contemporary sequential recommendation methods are becoming more complex, shifting from classification to a diffusion-guided generative paradigm. However, the quality of guidance in the form of user information is often compromised by missing data in the observed sequences, leading to suboptimal generation quality. Existing methods address this by removing locally similar items, but overlook ``critical turning points'' in user interest, which are crucial for accurately predicting subsequent user intent. To address this, we propose a novel Counterfactual Attention Regulation Diffusion model (CARD), which focuses on amplifying the signal from key interest-turning-point items while concurrently identifying and suppressing noise within the user sequence. CARD consists of (1) a Dual-side Thompson Sampling method to identify sequences undergoing significant interest shift, and (2) a counterfactual attention mechanism for these sequences to quantify the importance of each item. In this manner, CARD provides the diffusion model with a high-quality guidance signal composed of dynamically re-weighted interaction vectors to enable effective generation. Experiments show our method works well on real-world data without being computationally expensive. Our code is available at https://github.com/yanqilong3321/CARD.",
      "authors": [
        "Qilong Yan",
        "Yifei Xing",
        "Dugang Liu",
        "Jingpu Duan",
        "Jian Yin"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-01-22 05:55:21+00:00",
      "link": "https://arxiv.org/pdf/2601.15673v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15671v1",
      "title": "StreetDesignAI: A Multi-Persona Evaluation System for Inclusive Infrastructure Design",
      "abstract": "Designing inclusive cycling infrastructure requires balancing competing needs of diverse user groups, yet designers often struggle to anticipate how different cyclists experience the same street. We investigate how persona-based multi-agent evaluation can support inclusive design by making experiential conflicts explicit. We present StreetDesignAI, an interactive system that enables designers to (1) ground evaluation in street context through imagery and map data, (2) receive parallel feedback from cyclist personas spanning confident to cautious users, and (3) iteratively modify designs while surfacing conflicts across perspectives. A within-subjects study with 26 transportation professionals demonstrates that structured multi-perspective feedback significantly improves designers' understanding of diverse user perspectives, ability to identify persona needs, and confidence in translating them into design decisions, with higher satisfaction and stronger intention for professional adoption. Qualitative findings reveal how conflict surfacing transforms design exploration from single-perspective optimization toward deliberate trade-off reasoning. We discuss implications for AI tools that scaffold inclusive design through disagreement as an interaction primitive.",
      "authors": [
        "Ziyi Wang",
        "Yilong Dai",
        "Duanya Lyu",
        "Mateo Nader",
        "Sihan Chen",
        "Wanghao Ye",
        "Zjian Ding",
        "Xiang Yan"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "published": "2026-01-22 05:53:05+00:00",
      "link": "https://arxiv.org/pdf/2601.15671v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15663v1",
      "title": "TempoNet: Learning Realistic Communication and Timing Patterns for Network Traffic Simulation",
      "abstract": "Realistic network traffic simulation is critical for evaluating intrusion detection systems, stress-testing network protocols, and constructing high-fidelity environments for cybersecurity training. While attack traffic can often be layered into training environments using red-teaming or replay methods, generating authentic benign background traffic remains a core challenge -- particularly in simulating the complex temporal and communication dynamics of real-world networks. This paper introduces TempoNet, a novel generative model that combines multi-task learning with multi-mark temporal point processes to jointly model inter-arrival times and all packet- and flow-header fields. TempoNet captures fine-grained timing patterns and higher-order correlations such as host-pair behavior and seasonal trends, addressing key limitations of GAN-, LLM-, and Bayesian-based methods that fail to reproduce structured temporal variation. TempoNet produces temporally consistent, high-fidelity traces, validated on real-world datasets. Furthermore, we show that intrusion detection models trained on TempoNet-generated background traffic perform comparably to those trained on real data, validating its utility for real-world security applications.",
      "authors": [
        "Kristen Moore",
        "Diksha Goel",
        "Cody James Christopher",
        "Zhen Wang",
        "Minjune Kim",
        "Ahmed Ibrahim",
        "Ahmad Mohsin",
        "Seyit Camtepe"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-22 05:23:19+00:00",
      "link": "https://arxiv.org/pdf/2601.15663v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15657v1",
      "title": "Integrating Knowledge Distillation Methods: A Sequential Multi-Stage Framework",
      "abstract": "Knowledge distillation (KD) transfers knowledge from large teacher models to compact student models, enabling efficient deployment on resource constrained devices. While diverse KD methods, including response based, feature based, and relation based approaches, capture different aspects of teacher knowledge, integrating multiple methods or knowledge sources is promising but often hampered by complex implementation, inflexible combinations, and catastrophic forgetting, which limits practical effectiveness.   This work proposes SMSKD (Sequential Multi Stage Knowledge Distillation), a flexible framework that sequentially integrates heterogeneous KD methods. At each stage, the student is trained with a specific distillation method, while a frozen reference model from the previous stage anchors learned knowledge to mitigate forgetting. In addition, we introduce an adaptive weighting mechanism based on the teacher true class probability (TCP) that dynamically adjusts the reference loss per sample to balance knowledge retention and integration.   By design, SMSKD supports arbitrary method combinations and stage counts with negligible computational overhead. Extensive experiments show that SMSKD consistently improves student accuracy across diverse teacher student architectures and method combinations, outperforming existing baselines. Ablation studies confirm that stage wise distillation and reference model supervision are primary contributors to performance gains, with TCP based adaptive weighting providing complementary benefits. Overall, SMSKD is a practical and resource efficient solution for integrating heterogeneous KD methods.",
      "authors": [
        "Yinxi Tian",
        "Changwu Huang",
        "Ke Tang",
        "Xin Yao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-22 05:13:12+00:00",
      "link": "https://arxiv.org/pdf/2601.15657v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15640v1",
      "title": "An Empirical Study on Ensemble-Based Transfer Learning Bayesian Optimisation with Mixed Variable Types",
      "abstract": "Bayesian optimisation is a sample efficient method for finding a global optimum of expensive black-box objective functions. Historic datasets from related problems can be exploited to help improve performance of Bayesian optimisation by adapting transfer learning methods to various components of the Bayesian optimisation pipeline. In this study we perform an empirical analysis of various ensemble-based transfer learning Bayesian optimisation methods and pipeline components. We expand on previous work in the literature by contributing some specific pipeline components, and three new real-time transfer learning Bayesian optimisation benchmarks. In particular we propose to use a weighting strategy for ensemble surrogate model predictions based on regularised regression with weights constrained to be positive, and a related component for handling the case when transfer learning is not improving Bayesian optimisation performance. We find that in general, two components that help improve transfer learning Bayesian optimisation performance are warm start initialisation and constraining weights used with ensemble surrogate model to be positive.",
      "authors": [
        "Natasha Trinkle",
        "Huong Ha",
        "Jeffrey Chan"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-01-22 04:41:26+00:00",
      "link": "https://arxiv.org/pdf/2601.15640v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15639v1",
      "title": "A Class of Subadditive Information Measures and their Applications",
      "abstract": "We introduce a two-parameter family of discrepancy measures, termed \\emph{$(G,f)$-divergences}, obtained by applying a non-decreasing function $G$ to an $f$-divergence $D_f$. Building on Csiszár's formulation of mutual $f$-information, we define a corresponding $(G,f)$-information measure $ I_{G,f}(X;Y)$. A central theme of the paper is subadditivity over product distributions and product channels. We develop reduction principles showing that, for broad classes of $G$, it suffices to verify divergence subadditivity on binary alphabets. Specializing to the functions $G(x)\\in\\{x,\\log(1+x),-\\log(1-x)\\}$, we derive tractable sufficient conditions on $f$ that guarantee subadditivity, covering many standard $f$-divergences. Finally, we present applications to finite-blocklength converses for channel coding, bounds in binary hypothesis testing, and an extension of the Shannon--Gallager--Berlekamp sphere-packing exponent framework to subadditive $(G,f)$-divergences.",
      "authors": [
        "Hamidreza Abin",
        "Mahdi Zinati",
        "Amin Gohari",
        "Mohammad Hossein Yassaee",
        "Mohammad Mahdi Mojahedian"
      ],
      "primary_category": "cs.IT",
      "categories": [
        "cs.IT"
      ],
      "published": "2026-01-22 04:38:05+00:00",
      "link": "https://arxiv.org/pdf/2601.15639v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15630v1",
      "title": "Agentic AI Governance and Lifecycle Management in Healthcare",
      "abstract": "Healthcare organizations are beginning to embed agentic AI into routine workflows, including clinical documentation support and early-warning monitoring. As these capabilities diffuse across departments and vendors, health systems face agent sprawl, causing duplicated agents, unclear accountability, inconsistent controls, and tool permissions that persist beyond the original use case. Existing AI governance frameworks emphasize lifecycle risk management but provide limited guidance for the day-to-day operations of agent fleets. We propose a Unified Agent Lifecycle Management (UALM) blueprint derived from a rapid, practice-oriented synthesis of governance standards, agent security literature, and healthcare compliance requirements. UALM maps recurring gaps onto five control-plane layers: (1) an identity and persona registry, (2) orchestration and cross-domain mediation, (3) PHI-bounded context and memory, (4) runtime policy enforcement with kill-switch triggers, and (5) lifecycle management and decommissioning linked to credential revocation and audit logging. A companion maturity model supports staged adoption. UALM offers healthcare CIOs, CISOs, and clinical leaders an implementable pattern for audit-ready oversight that preserves local innovation and enables safer scaling across clinical and administrative domains.",
      "authors": [
        "Chandra Prakash",
        "Mary Lind",
        "Avneesh Sisodia"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 04:01:41+00:00",
      "link": "https://arxiv.org/pdf/2601.15630v1",
      "tags": [
        "keyword:EAA"
      ]
    },
    {
      "id": "2601.15626v1",
      "title": "Bridging Qualitative Rubrics and AI: A Binary Question Framework for Criterion-Referenced Grading in Engineering",
      "abstract": "PURPOSE OR GOAL: This study investigates how GenAI can be integrated with a criterion-referenced grading framework to improve the efficiency and quality of grading for mathematical assessments in engineering. It specifically explores the challenges demonstrators face with manual, model solution-based grading and how a GenAI-supported system can be designed to reliably identify student errors, provide high-quality feedback, and support human graders. The research also examines human graders' perceptions of the effectiveness of this GenAI-assisted approach. ACTUAL OR ANTICIPATED OUTCOMES: The study found that GenAI achieved an overall grading accuracy of 92.5%, comparable to two experienced human graders. The two researchers, who also served as subject demonstrators, perceived the GenAI as a helpful second reviewer that improved accuracy by catching small errors and provided more complete feedback than they could manually. A central outcome was the significant enhancement of formative feedback. However, they noted the GenAI tool is not yet reliable enough for autonomous use, especially with unconventional solutions. CONCLUSIONS/RECOMMENDATIONS/SUMMARY: This study demonstrates that GenAI, when paired with a structured, criterion-referenced framework using binary questions, can grade engineering mathematical assessments with an accuracy comparable to human experts. Its primary contribution is a novel methodological approach that embeds the generation of high-quality, scalable formative feedback directly into the assessment workflow. Future work should investigate student perceptions of GenAI grading and feedback.",
      "authors": [
        "Lili Chen",
        "Winn Wing-Yiu Chow",
        "Stella Peng",
        "Bencheng Fan",
        "Sachitha Bandara"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY",
        "cs.AI"
      ],
      "published": "2026-01-22 03:57:47+00:00",
      "link": "https://arxiv.org/pdf/2601.15626v1",
      "tags": [
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15620v1",
      "title": "Closing the Gap on the Sample Complexity of 1-Identification",
      "abstract": "1-identification is a fundamental multi-armed bandit formulation on pure exploration. An agent aims to determine whether there exists a qualified arm whose mean reward is not less than a known threshold $μ_0$, or to output \\textsf{None} if it believes such an arm does not exist. The agent needs to guarantee its output is correct with probability at least $1-δ$, while making expected total pulling times $\\mathbb{E}τ$ as small as possible. We work on 1-identification with two main contributions. (1) We utilize an optimization formulation to derive a new lower bound of $\\mathbb{E}τ$, when there is at least one qualified arm. (2) We design a new algorithm, deriving tight upper bounds whose gap to lower bounds are up to a polynomial of logarithm factor across all problem instance. Our result complements the analysis of $\\mathbb{E}τ$ when there are multiple qualified arms, which is an open problem left by history literature.",
      "authors": [
        "Zitian Li",
        "Wang Chi Cheung"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-22 03:50:31+00:00",
      "link": "https://arxiv.org/pdf/2601.15620v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15609v1",
      "title": "When Sharpening Becomes Collapse: Sampling Bias and Semantic Coupling in RL with Verifiable Rewards",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is a central paradigm for turning large language models (LLMs) into reliable problem solvers, especially in logic-heavy domains. Despite its empirical success, it remains unclear whether RLVR elicits novel capabilities or merely sharpens the distribution over existing knowledge. We study this by formalizing over-sharpening, a phenomenon where the policy collapses onto limited modes, suppressing valid alternatives. At a high level, we discover finite-batch updates intrinsically bias learning toward sampled modes, triggering a collapse that propagates globally via semantic coupling. To mitigate this, we propose inverse-success advantage calibration to prioritize difficult queries and distribution-level calibration to diversify sampling via a memory network. Empirical evaluations validate that our strategies can effectively improve generalization.",
      "authors": [
        "Mingyuan Fan",
        "Weiguang Han",
        "Daixin Wang",
        "Cen Chen",
        "Zhiqiang Zhang",
        "Jun Zhou"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-01-22 03:15:57+00:00",
      "link": "https://arxiv.org/pdf/2601.15609v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15599v1",
      "title": "Autonomous Business System via Neuro-symbolic AI",
      "abstract": "Current business environments require organizations to continuously reconfigure cross-functional processes, yet enterprise systems are still organized around siloed departments, rigid workflows, and hard-coded automation. Meanwhile large language models (LLMs) excel at interpreting natural language and unstructured data but lack deterministic, verifiable execution of complex business logic. To address this gap, here we introduce AUTOBUS, an Autonomous Business System that integrates LLM-based AI agents, predicate-logic programming, and business-semantics-centric enterprise data into a coherent neuro-symbolic AI architecture for orchestrating end-to-end business initiatives. AUTOBUS models an initiative as a network of tasks with explicit pre/post conditions, required data, evaluation rules, and API-level actions. Enterprise data is organized as a knowledge graph whose entities, relationships, and constraints are translated into logic facts and foundational rules, providing the semantic grounding for task reasoning. Core AI agents synthesize task instructions, enterprise semantics, and available tools into task-specific logic programs, which are executed by a logic engine that enforces constraints, coordinates auxiliary tools, and orchestrate execution of actions and outcomes. Humans define and maintain the semantics, policies and task instructions, curate tools, and supervise high-impact or ambiguous decisions, ensuring accountability and adaptability. We detail the AUTOBUS architecture, the anatomy of the AI agent generated logic programs, and the role of humans and auxiliary tools in the lifecycle of a business initiative.",
      "authors": [
        "Cecil Pang",
        "Hiroki Sayama"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 02:49:06+00:00",
      "link": "https://arxiv.org/pdf/2601.15599v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15593v1",
      "title": "Parallelism and Generation Order in Masked Diffusion Language Models: Limits Today, Potential Tomorrow",
      "abstract": "Masked Diffusion Language Models (MDLMs) promise parallel token generation and arbitrary-order decoding, yet it remains unclear to what extent current models truly realize these capabilities. We characterize MDLM behavior along two dimensions -- parallelism strength and generation order -- using Average Finalization Parallelism (AFP) and Kendall's tau. We evaluate eight mainstream MDLMs (up to 100B parameters) on 58 benchmarks spanning knowledge, reasoning, and programming. The results show that MDLMs still lag behind comparably sized autoregressive models, mainly because parallel probabilistic modeling weakens inter-token dependencies. Meanwhile, MDLMs exhibit adaptive decoding behavior: their parallelism and generation order vary significantly with the task domain, the stage of reasoning, and whether the output is correct. On tasks that require \"backward information\" (e.g., Sudoku), MDLMs adopt a solution order that tends to fill easier Sudoku blanks first, highlighting their advantages. Finally, we provide theoretical motivation and design insights supporting a Generate-then-Edit paradigm, which mitigates dependency loss while retaining the efficiency of parallel decoding.",
      "authors": [
        "Yangyang Zhong",
        "Yanmei Gu",
        "Zhengqing Zang",
        "Xiaomeng Li",
        "Yuqi Ding",
        "Xibei Jia",
        "Yuting Shen",
        "Zhenzhong Lan",
        "Liwang Zhu",
        "Weiping Liu",
        "Junlin Zhou",
        "Haisheng Liu",
        "Zhong Xin Yu",
        "Pengxin Luo",
        "Donglian Qi",
        "Yunfeng Yan",
        "Junbo Zhao"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-22 02:39:36+00:00",
      "link": "https://arxiv.org/pdf/2601.15593v1",
      "tags": [
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15589v1",
      "title": "Deep Learning for Perishable Inventory Systems with Human Knowledge",
      "abstract": "Managing perishable products with limited lifetimes is a fundamental challenge in inventory management, as poor ordering decisions can quickly lead to stockouts or excessive waste. We study a perishable inventory system with random lead times in which both the demand process and the lead time distribution are unknown. We consider a practical setting where orders are placed using limited historical data together with observed covariates and current system states. To improve learning efficiency under limited data, we adopt a marginal cost accounting scheme that assigns each order a single lifetime cost and yields a unified loss function for end-to-end learning. This enables training a deep learning-based policy that maps observed covariates and system states directly to order quantities. We develop two end-to-end variants: a purely black-box approach that outputs order quantities directly (E2E-BB), and a structure-guided approach that embeds the projected inventory level (PIL) policy, capturing inventory effects through explicit computation rather than additional learning (E2E-PIL). We further show that the objective induced by E2E-PIL is homogeneous of degree one, enabling a boosting technique from operational data analytics (ODA) that yields an enhanced policy (E2E-BPIL). Experiments on synthetic and real data establish a robust performance ordering: E2E-BB is dominated by E2E-PIL, which is further improved by E2E-BPIL. Using an excess-risk decomposition, we show that embedding heuristic policy structure reduces effective model complexity and improves learning efficiency with only a modest loss of flexibility. More broadly, our results suggest that deep learning-based decision tools are more effective and robust when guided by human knowledge, highlighting the value of integrating advanced analytics with inventory theory.",
      "authors": [
        "Xuan Liao",
        "Zhenkang Peng",
        "Ying Rong"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-22 02:26:32+00:00",
      "link": "https://arxiv.org/pdf/2601.15589v1",
      "tags": [
        "keyword:EAA",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15580v1",
      "title": "Screening for Choice Sets",
      "abstract": "We study a screening problem in which an agent privately observes a set of feasible technologies and can strategically disclose only a subset to the principal. The principal then takes an action whose payoff consequences for both players are publicly known. Under the assumption that the possible technology sets are ordered by set inclusion, we show that the optimal mechanism promises the agent a utility that is weakly increasing as the reported set expands, and the choice of the principal maximizes her own utility subject to this promised utility constraint. Moreover, the optimal promised utility either coincides with the agent's utility under the complete information benchmark or remains locally constant, with the number of constant segments bounded by the number of downward-sloping segments of the complete information benchmark.",
      "authors": [
        "Tan Gan",
        "Yingkai Li"
      ],
      "primary_category": "econ.TH",
      "categories": [
        "econ.TH",
        "cs.GT"
      ],
      "published": "2026-01-22 02:00:34+00:00",
      "link": "https://arxiv.org/pdf/2601.15580v1",
      "tags": [
        "keyword:EAA",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15578v1",
      "title": "MapViT: A Two-Stage ViT-Based Framework for Real-Time Radio Quality Map Prediction in Dynamic Environments",
      "abstract": "Recent advancements in mobile and wireless networks are unlocking the full potential of robotic autonomy, enabling robots to take advantage of ultra-low latency, high data throughput, and ubiquitous connectivity. However, for robots to navigate and operate seamlessly, efficiently and reliably, they must have an accurate understanding of both their surrounding environment and the quality of radio signals. Achieving this in highly dynamic and ever-changing environments remains a challenging and largely unsolved problem. In this paper, we introduce MapViT, a two-stage Vision Transformer (ViT)-based framework inspired by the success of pre-train and fine-tune paradigm for Large Language Models (LLMs). MapViT is designed to predict both environmental changes and expected radio signal quality. We evaluate the framework using a set of representative Machine Learning (ML) models, analyzing their respective strengths and limitations across different scenarios. Experimental results demonstrate that the proposed two-stage pipeline enables real-time prediction, with the ViT-based implementation achieving a strong balance between accuracy and computational efficiency. This makes MapViT a promising solution for energy- and resource-constrained platforms such as mobile robots. Moreover, the geometry foundation model derived from the self-supervised pre-training stage improves data efficiency and transferability, enabling effective downstream predictions even with limited labeled data. Overall, this work lays the foundation for next-generation digital twin ecosystems, and it paves the way for a new class of ML foundation models driving multi-modal intelligence in future 6G-enabled systems.",
      "authors": [
        "Cyril Shih-Huan Hsu",
        "Xi Li",
        "Lanfranco Zanzi",
        "Zhiheng Yang",
        "Chrysa Papagianni",
        "Xavier Costa Pérez"
      ],
      "primary_category": "cs.NI",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "published": "2026-01-22 01:57:48+00:00",
      "link": "https://arxiv.org/pdf/2601.15578v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15571v1",
      "title": "Verified polynomial-time reductions in Lean 4: formalizing the complexity of decision-relevant information",
      "abstract": "We present a Lean 4 framework for polynomial-time reductions and complexity-theory proofs, and use it to formalize the complexity of identifying decision-relevant information. Problem: given a decision problem, which coordinates suffice to compute an optimal action? (SUFFICIENCY-CHECK; explicit encodings). Verified complexity results (Lean): coNP-complete; $(1-\\varepsilon)\\ln n$ inapproximable (from SET-COVER); $2^{Ω(n)}$ lower bounds under ETH for succinct encodings; W[2]-hard for a natural parameterization; and a dichotomy between explicit and succinct models. Formalization contributions: bundled Karp reductions with polynomial-time witnesses; composition lemmas/tactics; and templates for NP/coNP and $Σ_2^P$ membership and hardness. Scale: about 5,600 lines of Lean across 36 files, with 230+ theorems and explicit polynomial bounds.",
      "authors": [
        "Tristan Simas"
      ],
      "primary_category": "cs.CC",
      "categories": [
        "cs.CC"
      ],
      "published": "2026-01-22 01:29:10+00:00",
      "link": "https://arxiv.org/pdf/2601.15571v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15561v1",
      "title": "Enhanced Convergence in p-bit Based Simulated Annealing with Partial Deactivation for Large-Scale Combinatorial Optimization Problems",
      "abstract": "This article critically investigates the limitations of the simulated annealing algorithm using probabilistic bits (pSA) in solving large-scale combinatorial optimization problems. The study begins with an in-depth analysis of the pSA process, focusing on the issues resulting from unexpected oscillations among p-bits. These oscillations hinder the energy reduction of the Ising model and thus obstruct the successful execution of pSA in complex tasks. Through detailed simulations, we unravel the root cause of this energy stagnation, identifying the feedback mechanism inherent to the pSA operation as the primary contributor to these disruptive oscillations. To address this challenge, we propose two novel algorithms, time average pSA (TApSA) and stalled pSA (SpSA). These algorithms are designed based on partial deactivation of p-bits and are thoroughly tested using Python simulations on maximum cut benchmarks that are typical combinatorial optimization problems. On the 16 benchmarks from 800 to 5,000 nodes, the proposed methods improve the normalized cut value from 0.8% to 98.4% on average in comparison with the conventional pSA.",
      "authors": [
        "Naoya Onizawa",
        "Takahiro Hanyu"
      ],
      "primary_category": "cs.ET",
      "categories": [
        "cs.ET",
        "cs.LG"
      ],
      "published": "2026-01-22 01:01:35+00:00",
      "link": "https://arxiv.org/pdf/2601.15561v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15552v1",
      "title": "BanditLP: Large-Scale Stochastic Optimization for Personalized Recommendations",
      "abstract": "We present BanditLP, a scalable multi-stakeholder contextual bandit framework that unifies neural Thompson Sampling for learning objective-specific outcomes with a large-scale linear program for constrained action selection at serving time. The methodology is application-agnostic, compatible with arbitrary neural architectures, and deployable at web scale, with an LP solver capable of handling billions of variables. Experiments on public benchmarks and synthetic data show consistent gains over strong baselines. We apply this approach in LinkedIn's email marketing system and demonstrate business win, illustrating the value of integrated exploration and constrained optimization in production.",
      "authors": [
        "Phuc Nguyen",
        "Benjamin Zelditch",
        "Joyce Chen",
        "Rohit Patra",
        "Changshuai Wei"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2026-01-22 00:48:45+00:00",
      "link": "https://arxiv.org/pdf/2601.15552v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15551v1",
      "title": "ALIGNAgent: Adaptive Learner Intelligence for Gap Identification and Next-step guidance",
      "abstract": "Personalized learning systems have emerged as a promising approach to enhance student outcomes by tailoring educational content, pacing, and feedback to individual needs. However, most existing systems remain fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation, but rarely integrating these components into a cohesive adaptive cycle. In this paper, we propose ALIGNAgent (Adaptive Learner Intelligence for Gap Identification and Next-step guidance), a multi-agent educational framework designed to deliver personalized learning through integrated knowledge estimation, skill-gap identification, and targeted resource recommendation.ALIGNAgent begins by processing student quiz performance, gradebook data, and learner preferences to generate topic-level proficiency estimates using a Skill Gap Agent that employs concept-level diagnostic reasoning to identify specific misconceptions and knowledge deficiencies. After identifying skill gaps, the Recommender Agent retrieves preference-aware learning materials aligned with diagnosed deficiencies, implementing a continuous feedback loop where interventions occur before advancing to subsequent topics. Extensive empirical evaluation on authentic datasets from two undergraduate computer science courses demonstrates ALIGNAgent's effectiveness, with GPT-4o-based agents achieving precision of 0.87-0.90 and F1 scores of 0.84-0.87 in knowledge proficiency estimation validated against actual exam performance.",
      "authors": [
        "Bismack Tokoli",
        "Luis Jaimes",
        "Ayesha S. Dina"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "published": "2026-01-22 00:45:15+00:00",
      "link": "https://arxiv.org/pdf/2601.15551v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15547v1",
      "title": "Learning Neural Operators from Partial Observations via Latent Autoregressive Modeling",
      "abstract": "Real-world scientific applications frequently encounter incomplete observational data due to sensor limitations, geographic constraints, or measurement costs. Although neural operators significantly advanced PDE solving in terms of computational efficiency and accuracy, their underlying assumption of fully-observed spatial inputs severely restricts applicability in real-world applications. We introduce the first systematic framework for learning neural operators from partial observation. We identify and formalize two fundamental obstacles: (i) the supervision gap in unobserved regions that prevents effective learning of physical correlations, and (ii) the dynamic spatial mismatch between incomplete inputs and complete solution fields. Specifically, our proposed Latent Autoregressive Neural Operator~(\\ours) introduces two novel components designed explicitly to address the core difficulties of partial observations: (i) a mask-to-predict training strategy that creates artificial supervision by strategically masking observed regions, and (ii) a Physics-Aware Latent Propagator that reconstructs solutions through boundary-first autoregressive generation in latent space. Additionally, we develop POBench-PDE, a dedicated and comprehensive benchmark designed specifically for evaluating neural operators under partial observation conditions across three PDE-governed tasks. \\ours achieves state-of-the-art performance with 18--69$\\%$ relative L2 error reduction across all benchmarks under patch-wise missingness with less than 50$\\%$ missing rate, including real-world climate prediction. Our approach effectively addresses practical scenarios involving up to 75$\\%$ missing rate, to some extent bridging the existing gap between idealized research settings and the complexities of real-world scientific computing.",
      "authors": [
        "Jingren Hou",
        "Hong Wang",
        "Pengyu Xu",
        "Chang Gao",
        "Huafeng Liu",
        "Liping Jing"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-22 00:33:38+00:00",
      "link": "https://arxiv.org/pdf/2601.15547v1",
      "tags": [
        "keyword:EAA"
      ]
    },
    {
      "id": "2601.15544v1",
      "title": "RDumb++: Drift-Aware Continual Test-Time Adaptation",
      "abstract": "Continual Test-Time Adaptation (CTTA) seeks to update a pretrained model during deployment using only the incoming, unlabeled data stream. Although prior approaches such as Tent, EATA etc. provide meaningful improvements under short evolving shifts, they struggle when the test distribution changes rapidly or over extremely long horizons. This challenge is exemplified by the CCC benchmark, where models operate over streams of 7.5M samples with continually changing corruption types and severities. We propose RDumb++, a principled extension of RDumb that introduces two drift-detection mechanisms i.e entropy-based drift scoring and KL-divergence drift scoring, together with adaptive reset strategies. These mechanisms allow the model to detect when accumulated adaptation becomes harmful and to recover before prediction collapse occurs. Across CCC-medium with three speeds and three seeds (nine runs, each containing one million samples), RDumb++ consistently surpasses RDumb, yielding approx 3% absolute accuracy gains while maintaining stable adaptation throughout the entire stream. Ablation experiments on drift thresholds and reset strengths further show that drift-aware resetting is essential for preventing collapse and achieving reliable long-horizon CTTA.",
      "authors": [
        "Himanshu Mishra"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-22 00:20:23+00:00",
      "link": "https://arxiv.org/pdf/2601.15544v1",
      "tags": [
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15518v1",
      "title": "DS@GT at TREC TOT 2025: Bridging Vague Recollection with Fusion Retrieval and Learned Reranking",
      "abstract": "We develop a two-stage retrieval system that combines multiple complementary retrieval methods with a learned reranker and LLM-based reranking, to address the TREC Tip-of-the-Tongue (ToT) task. In the first stage, we employ hybrid retrieval that merges LLM-based retrieval, sparse (BM25), and dense (BGE-M3) retrieval methods. We also introduce topic-aware multi-index dense retrieval that partitions the Wikipedia corpus into 24 topical domains. In the second stage, we evaluate both a trained LambdaMART reranker and LLM-based reranking. To support model training, we generate 5000 synthetic ToT queries using LLMs. Our best system achieves recall of 0.66 and NDCG@1000 of 0.41 on the test set by combining hybrid retrieval with Gemini-2.5-flash reranking, demonstrating the effectiveness of fusion retrieval.",
      "authors": [
        "Wenxin Zhou",
        "Ritesh Mehta",
        "Anthony Miyaguchi"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-01-21 23:09:17+00:00",
      "link": "https://arxiv.org/pdf/2601.15518v1",
      "tags": [
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15500v1",
      "title": "Low-Dimensional Adaptation of Rectified Flow: A New Perspective through the Lens of Diffusion and Stochastic Localization",
      "abstract": "In recent years, Rectified flow (RF) has gained considerable popularity largely due to its generation efficiency and state-of-the-art performance. In this paper, we investigate the degree to which RF automatically adapts to the intrinsic low dimensionality of the support of the target distribution to accelerate sampling. We show that, using a carefully designed choice of the time-discretization scheme and with sufficiently accurate drift estimates, the RF sampler enjoys an iteration complexity of order $O(k/\\varepsilon)$ (up to log factors), where $\\varepsilon$ is the precision in total variation distance and $k$ is the intrinsic dimension of   the target distribution. In addition, we show that the denoising diffusion probabilistic model (DDPM) procedure is equivalent to a stochastic version of RF by establishing a novel connection between these processes and stochastic localization. Building on this connection, we further design a stochastic RF sampler that also adapts to the low-dimensionality of the target distribution under milder requirements on the accuracy of the drift estimates, and also with a specific time schedule. We illustrate with simulations on the synthetic data and text-to-image data experiments the improved performance of the proposed samplers implementing the newly designed time-discretization schedules.",
      "authors": [
        "Saptarshi Roy",
        "Alessandro Rinaldo",
        "Purnamrita Sarkar"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.ST"
      ],
      "published": "2026-01-21 22:09:27+00:00",
      "link": "https://arxiv.org/pdf/2601.15500v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15495v1",
      "title": "Tracking the Limits of Knowledge Propagation: How LLMs Fail at Multi-Step Reasoning with Conflicting Knowledge",
      "abstract": "A common solution for mitigating outdated or incorrect information in Large Language Models (LLMs) is to provide updated facts in-context or through knowledge editing. However, these methods introduce knowledge conflicts when the knowledge update fails to overwrite the model's parametric knowledge, which propagate to faulty reasoning. Current benchmarks for this problem, however, largely focus only on single knowledge updates and fact recall without evaluating how these updates affect downstream reasoning. In this work, we introduce TRACK (Testing Reasoning Amid Conflicting Knowledge), a new benchmark for studying how LLMs propagate new knowledge through multi-step reasoning when it conflicts with the model's initial parametric knowledge. Spanning three reasoning-intensive scenarios (WIKI, CODE, and MATH), TRACK introduces multiple, realistic conflicts to mirror real-world complexity. Our results on TRACK reveal that providing updated facts to models for reasoning can worsen performance compared to providing no updated facts to a model, and that this performance degradation exacerbates as more updated facts are provided. We show this failure stems from both inability to faithfully integrate updated facts, but also flawed reasoning even when knowledge is integrated. TRACK provides a rigorous new benchmark to measure and guide future progress on propagating conflicting knowledge in multi-step reasoning.",
      "authors": [
        "Yiyang Feng",
        "Zeming Chen",
        "Haotian Wu",
        "Jiawei Zhou",
        "Antoine Bosselut"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-21 21:56:35+00:00",
      "link": "https://arxiv.org/pdf/2601.15495v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15493v1",
      "title": "Testing Deep Learning Libraries via Neurosymbolic Constraint Learning",
      "abstract": "Deep Learning (DL) libraries (e.g., PyTorch) are popular in AI development. These libraries are complex and contain bugs. Researchers have proposed various bug-finding techniques for such libraries. Yet, there is much room for improvement. A key challenge in testing DL libraries is the lack of API specifications. Prior testing approaches often inaccurately model the input specifications of DL APIs, resulting in missed valid inputs that could reveal bugs or false alarms due to invalid inputs.   To address this challenge, we develop Centaur -- the first neurosymbolic technique to test DL library APIs using dynamically learned input constraints. Centaur leverages the key idea that formal API constraints can be learned from a small number of automatically generated seed inputs, and that the learned constraints can be solved using SMT solvers to generate valid and diverse test inputs.   We develop a novel grammar that represents first-order logic formulae over API parameters and expresses tensor-related properties (e.g., shape, data types) as well as relational properties between parameters. We use the grammar to guide a Large Language Model (LLM) to enumerate syntactically correct candidate rules, validated using seed inputs. Further, we develop a custom refinement strategy to prune the set of learned rules to eliminate spurious or redundant rules. We use the learned constraints to systematically generate valid and diverse inputs by integrating SMT-based solving with randomized sampling.   We evaluate Centaur for testing PyTorch and TensorFlow. Our results show that Centaur's constraints have a recall of 94.0% and a precision of 94.0% on average. In terms of coverage, Centaur covers 203, 150, and 9,608 more branches than TitanFuzz, ACETest and Pathfinder, respectively. Using Centaur, we also detect 26 new bugs in PyTorch and TensorFlow, 18 of which are confirmed.",
      "authors": [
        "M M Abid Naziri",
        "Shinhae Kim",
        "Feiran Qin",
        "Marcelo d'Amorim",
        "Saikat Dutta"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE"
      ],
      "published": "2026-01-21 21:54:41+00:00",
      "link": "https://arxiv.org/pdf/2601.15493v1",
      "tags": [
        "keyword:EAA"
      ]
    },
    {
      "id": "2601.15487v1",
      "title": "MiRAGE: A Multiagent Framework for Generating Multimodal Multihop Question-Answer Dataset for RAG Evaluation",
      "abstract": "The rapid evolution of Retrieval-Augmented Generation (RAG) toward multimodal, high-stakes enterprise applications has outpaced the development of domain specific evaluation benchmarks. Existing datasets often rely on general-domain corpora or purely textual retrieval, failing to capture the complexity of specialized technical documents where information is inextricably multimodal and reasoning requires synthesizing disjoint evidence. We address this gap by introducing MiRAGE, a Multiagent framework for RAG systems Evaluation, that leverages a collaborative swarm of specialized agents to generate verified, domain-specific, multimodal, and multi-hop Question-Answer datasets. MiRAGE orchestrates a swarm of specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent to guarantee factual grounding, and an agent to recognize the expert persona and the relevant domain to mimic expert cognitive workflows. Extensive empirical evaluation across four distinct domains (regulations, finance, quantitative biology, and journalism) demonstrates that MiRAGE generates datasets with significantly higher reasoning complexity (>2.3 average hops) and factual faithfulness. Our ablation studies point that MiRAGE can be powered by LLMs if textual descriptions of the images are available. Visual grounding still remains a frontier. By automating the creation of gold standard evaluation datasets that reflect the latent thematic structure of proprietary corpora, MiRAGE provides the necessary infrastructure to rigorously benchmark the next generation information retrieval systems.",
      "authors": [
        "Chandan Kumar Sahu",
        "Premith Kumar Chilukuri",
        "Matthew Hetrich"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "published": "2026-01-21 21:39:09+00:00",
      "link": "https://arxiv.org/pdf/2601.15487v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15485v1",
      "title": "The Rise of Large Language Models and the Direction and Impact of US Federal Research Funding",
      "abstract": "Federal research funding shapes the direction, diversity, and impact of the US scientific enterprise. Large language models (LLMs) are rapidly diffusing into scientific practice, holding substantial promise while raising widespread concerns. Despite growing attention to AI use in scientific writing and evaluation, little is known about how the rise of LLMs is reshaping the public funding landscape. Here, we examine LLM involvement at key stages of the federal funding pipeline by combining two complementary data sources: confidential National Science Foundation (NSF) and National Institutes of Health (NIH) proposal submissions from two large US R1 universities, including funded, unfunded, and pending proposals, and the full population of publicly released NSF and NIH awards. We find that LLM use rises sharply beginning in 2023 and exhibits a bimodal distribution, indicating a clear split between minimal and substantive use. Across both private submissions and public awards, higher LLM involvement is consistently associated with lower semantic distinctiveness, positioning projects closer to recently funded work within the same agency. The consequences of this shift are agency-dependent. LLM use is positively associated with proposal success and higher subsequent publication output at NIH, whereas no comparable associations are observed at NSF. Notably, the productivity gains at NIH are concentrated in non-hit papers rather than the most highly cited work. Together, these findings provide large-scale evidence that the rise of LLMs is reshaping how scientific ideas are positioned, selected, and translated into publicly funded research, with implications for portfolio governance, research diversity, and the long-run impact of science.",
      "authors": [
        "Yifan Qian",
        "Zhe Wen",
        "Alexander C. Furnas",
        "Yue Bai",
        "Erzhuo Shao",
        "Dashun Wang"
      ],
      "primary_category": "cs.DL",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.CY",
        "physics.soc-ph"
      ],
      "published": "2026-01-21 21:37:08+00:00",
      "link": "https://arxiv.org/pdf/2601.15485v1",
      "tags": [
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15478v1",
      "title": "Equal-Pay Contracts",
      "abstract": "We study multi-agent contract design, where a principal incentivizes a team of agents to take costly actions that jointly determine the project success via a combinatorial reward function. While prior work largely focuses on unconstrained contracts that allow heterogeneous payments across agents, many real-world environments limit payment dispersion. Motivated by this, we study equal-pay contracts, where all agents receive identical payments. Our results also extend to nearly-equal-pay contracts where any two payments are identical up to a constant factor.   We provide both algorithmic and hardness results across a broad hierarchy of reward functions, under both binary and combinatorial action models. While we focus on equal-pay contracts, our analysis also yields new insights into unconstrained contract design, and resolves two important open problems. On the positive side, we design polynomial-time O(1)-approximation algorithms for (i) submodular rewards under combinatorial actions, and (ii) XOS rewards under binary actions. These guarantees are tight: We rule out the existence of (i) a PTAS for combinatorial actions, even for gross substitutes rewards (unless P = NP), and (ii) any O(1)-approximation for XOS rewards with combinatorial actions. Crucially, our hardness results hold even for unconstrained contracts, thereby settling the corresponding open problems in this setting.   Finally, we quantify the loss induced by fairness via the price of equality, defined as the worst-case ratio between the optimal principal's utility achievable by unconstrained contracts and that achievable by equal-pay contracts. We obtain a bound of $Θ(\\log n/ \\log \\log n)$, where $n$ is the number of agents. This gap is tight in a strong sense: the upper bound applies even for XOS rewards with combinatorial actions, while the lower bound arises already for additive rewards with binary actions.",
      "authors": [
        "Michal Feldman",
        "Yoav Gal-Tzur",
        "Tomasz Ponitka",
        "Maya Schlesinger"
      ],
      "primary_category": "cs.GT",
      "categories": [
        "cs.GT"
      ],
      "published": "2026-01-21 21:29:13+00:00",
      "link": "https://arxiv.org/pdf/2601.15478v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15473v1",
      "title": "Panther: Faster and Cheaper Computations with Randomized Numerical Linear Algebra",
      "abstract": "Training modern deep learning models is increasingly constrained by GPU memory and compute limits. While Randomized Numerical Linear Algebra (RandNLA) offers proven techniques to compress these models, the lack of a unified, production-grade library prevents widely adopting these methods. We present Panther, a PyTorch-compatible library that consolidates established RandNLA algorithms into a single high-performance framework. Panther engineers efficient, drop-in replacements for standard components including sketched linear layers, 2D convolution, multi-head attention, and randomized matrix decompositions (such as pivoted CholeskyQR). By implementing a custom C++/CUDA backend (pawX), Panther provides an optimized implementation that can run on both CPUs and GPUs. We demonstrate the effectiveness of RandNLA techniques and Panther's ease of adoption. By replacing standard PyTorch linear layers with Panther layers (requiring only a few lines of code) we achieve significant memory savings (up to 75%) on BERT while maintaining comparable loss. Source code is available (MIT License) at https://github.com/FahdSeddik/panther, along with demonstration video at https://youtu.be/7M3RQb4KWxs.",
      "authors": [
        "Fahd Seddik",
        "Abdulrahman Elbedewy",
        "Gaser Sami",
        "Mohamed Abdelmoniem",
        "Yahia Zakaria"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-21 21:23:00+00:00",
      "link": "https://arxiv.org/pdf/2601.15473v1",
      "tags": [
        "keyword:EAA"
      ]
    },
    {
      "id": "2601.15470v1",
      "title": "Nested and outlier embeddings into trees",
      "abstract": "In this paper, we consider outlier embeddings into HSTs and ultrametrics. In particular, for $(X,d)$, let $k$ be the size of the smallest subset of $X$ such that all but that subset (i.e. the ``outlier set'') can be probabilistically embedded into the space of HSTs with expected distortion at most $c$. Our primary result is showing that there exists an efficient algorithm that takes in $(X,d)$ and a target distortion $c$ and samples from a probabilistic embedding with at most $O(\\frac k ε\\log^2k)$ outliers and distortion at most $(32+ε)c$, for any $ε>0$. This leads to better instance-specific approximations for certain instances of the buy-at-bulk and dial-a-ride problems, whose current best approximation algorithms go through HST embeddings.   In order to facilitate our results, we largely focus on the concept of compositions of nested embeddings introduced by [Chawla and Sheridan 2024]. A nested embedding is a composition of two embeddings of a metric space $(X,d)$ -- a low distortion embedding of a subset $S$ of nodes, and a higher distortion embedding of the entire metric. The composition is a single embedding that preserves the low distortion over $S$ and does not increase distortion over the remaining points by much. In this paper, we expand this concept from the setting of deterministic embeddings to the setting of probabilistic embeddings. We show how to find good nested compositions of embeddings into HSTs, and combine this with an approximation algorithm of [Munagala et al. 2023] to obtain our results.",
      "authors": [
        "Shuchi Chawla",
        "Kristin Sheridan"
      ],
      "primary_category": "cs.DS",
      "categories": [
        "cs.DS"
      ],
      "published": "2026-01-21 21:09:18+00:00",
      "link": "https://arxiv.org/pdf/2601.15470v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15457v1",
      "title": "Chunking, Retrieval, and Re-ranking: An Empirical Evaluation of RAG Architectures for Policy Document Question Answering",
      "abstract": "The integration of Large Language Models (LLMs) into the public health policy sector offers a transformative approach to navigating the vast repositories of regulatory guidance maintained by agencies such as the Centers for Disease Control and Prevention (CDC). However, the propensity for LLMs to generate hallucinations, defined as plausible but factually incorrect assertions, presents a critical barrier to the adoption of these technologies in high-stakes environments where information integrity is non-negotiable. This empirical evaluation explores the effectiveness of Retrieval-Augmented Generation (RAG) architectures in mitigating these risks by grounding generative outputs in authoritative document context. Specifically, this study compares a baseline Vanilla LLM against Basic RAG and Advanced RAG pipelines utilizing cross-encoder re-ranking. The experimental framework employs a Mistral-7B-Instruct-v0.2 model and an all-MiniLM-L6-v2 embedding model to process a corpus of official CDC policy analytical frameworks and guidance documents. The analysis measures the impact of two distinct chunking strategies, recursive character-based and token-based semantic splitting, on system accuracy, measured through faithfulness and relevance scores across a curated set of complex policy scenarios. Quantitative findings indicate that while Basic RAG architectures provide a substantial improvement in faithfulness (0.621) over Vanilla baselines (0.347), the Advanced RAG configuration achieves a superior faithfulness average of 0.797. These results demonstrate that two-stage retrieval mechanisms are essential for achieving the precision required for domain-specific policy question answering, though structural constraints in document segmentation remain a significant bottleneck for multi-step reasoning tasks.",
      "authors": [
        "Anuj Maharjan",
        "Umesh Yadav"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "published": "2026-01-21 20:52:48+00:00",
      "link": "https://arxiv.org/pdf/2601.15457v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15455v1",
      "title": "Remarks on Algebraic Reconstruction of Types and Effects",
      "abstract": "In their 1991 paper \"Algebraic Reconstruction of Types and Effects,\" Pierre Jouvelot and David Gifford presented a type-and-effect reconstruction algorithm based on an algebraic structure of effects. Their work is considered a milestone in the development of type-and-effect systems, and has inspired numerous subsequent works in the area of static analysis. However, unlike the later research it spawned, the original algorithm considered a language with higher-rank polymorphism, a feature which is challenging to implement correctly. In this note, we identify subtle bugs related to variable binding in their approach to this feature. We revisit their type system and reconstruction algorithm, and describe the discovered issues.",
      "authors": [
        "Patrycja Balik",
        "Szymon Jędras",
        "Piotr Polesiuk"
      ],
      "primary_category": "cs.PL",
      "categories": [
        "cs.PL"
      ],
      "published": "2026-01-21 20:50:21+00:00",
      "link": "https://arxiv.org/pdf/2601.15455v1",
      "tags": [
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15442v1",
      "title": "A tensor network formalism for neuro-symbolic AI",
      "abstract": "The unification of neural and symbolic approaches to artificial intelligence remains a central open challenge. In this work, we introduce a tensor network formalism, which captures sparsity principles originating in the different approaches in tensor decompositions. In particular, we describe a basis encoding scheme for functions and model neural decompositions as tensor decompositions. The proposed formalism can be applied to represent logical formulas and probability distributions as structured tensor decompositions. This unified treatment identifies tensor network contractions as a fundamental inference class and formulates efficiently scaling reasoning algorithms, originating from probability theory and propositional logic, as contraction message passing schemes. The framework enables the definition and training of hybrid logical and probabilistic models, which we call Hybrid Logic Network. The theoretical concepts are accompanied by the python library tnreason, which enables the implementation and practical use of the proposed architectures.",
      "authors": [
        "Alex Goessmann",
        "Janina Schütte",
        "Maximilian Fröhlich",
        "Martin Eigel"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.LO",
        "math.NA",
        "stat.ML"
      ],
      "published": "2026-01-21 20:20:31+00:00",
      "link": "https://arxiv.org/pdf/2601.15442v1",
      "tags": [
        "keyword:EAA"
      ]
    },
    {
      "id": "2601.15434v1",
      "title": "ManuRAG: Multi-modal Retrieval Augmented Generation for Manufacturing Question Answering",
      "abstract": "The evolution of digital manufacturing requires intelligent Question Answering (QA) systems that can seamlessly integrate and analyze complex multi-modal data, such as text, images, formulas, and tables. Conventional Retrieval Augmented Generation (RAG) methods often fall short in handling this complexity, resulting in subpar performance. We introduce ManuRAG, an innovative multi-modal RAG framework designed for manufacturing QA, incorporating specialized techniques to improve answer accuracy, reliability, and interpretability. To benchmark performance, we evaluate ManuRAG on three datasets comprising a total of 1,515 QA pairs, corresponding to mathematical, multiple-choice, and review-based questions in manufacturing principles and practices. Experimental results show that ManuRAG consistently outperforms existing methods across all evaluated datasets. Furthermore, ManuRAG's adaptable design makes it applicable to other domains, including law, healthcare, and finance, positioning it as a versatile tool for domain-specific QA.",
      "authors": [
        "Yunqing Li",
        "Zihan Dong",
        "Farhad Ameri",
        "Jianbang Zhang"
      ],
      "primary_category": "cs.CE",
      "categories": [
        "cs.CE"
      ],
      "published": "2026-01-21 19:59:27+00:00",
      "link": "https://arxiv.org/pdf/2601.15434v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15432v1",
      "title": "MEDFORD in a Box: Improvements and Future Directions for a Metadata Description Language",
      "abstract": "Scientific research metadata is vital to ensure the validity, reusability, and cost-effectiveness of research efforts. The MEDFORD metadata language was previously introduced to simplify the process of writing and maintaining metadata for non-programmers. However, barriers to entry and usability remain, including limited automatic validation, difficulty of data transport, and user unfamiliarity with text file editing. To address these issues, we introduce MEDFORD-in-a-Box (MIAB), a documentation ecosystem to facilitate researcher adoption and earlier metadata capture. MIAB contains many improvements, including an updated MEDFORD parser with expanded validation routines and BagIt export capability. MIAB also includes an improved VS Code extension that supports these changes through a visual IDE. By simplifying metadata generation, this new tool supports the creation of correct, consistent, and reusable metadata, ultimately improving research reproducibility.",
      "authors": [
        "Polina Shpilker",
        "Benjamin Stubbs",
        "Michael Sayers",
        "Yumin Lee",
        "Lenore Cowen",
        "Donna Slonim",
        "Shaun Wallace",
        "Alva Couch",
        "Noah M. Daniels"
      ],
      "primary_category": "cs.DL",
      "categories": [
        "cs.DL",
        "cs.IR"
      ],
      "published": "2026-01-21 19:56:57+00:00",
      "link": "https://arxiv.org/pdf/2601.15432v1",
      "tags": [
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15423v1",
      "title": "Lattice: A Confidence-Gated Hybrid System for Uncertainty-Aware Sequential Prediction with Behavioral Archetypes",
      "abstract": "We introduce Lattice, a hybrid sequential prediction system that conditionally activates learned behavioral structure using binary confidence gating. The system clusters behavior windows into behavioral archetypes and uses binary confidence gating to activate archetype-based scoring only when confidence exceeds a threshold, falling back to baseline predictions when uncertain. We validate Lattice on recommendation systems (MovieLens), scientific time-series (LIGO), and financial markets, using LSTM and transformer backbones. On MovieLens with LSTM, Lattice achieves +31.9% improvement over LSTM baseline in HR@10 (p < 3.29 x 10^-25, 30 seeds), outperforming transformer baselines by 109.4% over SASRec and 218.6% over BERT4Rec. On LIGO and financial data, the system correctly refuses archetype activation when distribution shift occurs - a successful outcome demonstrating confidence gating prevents false activation. On transformer backbones, Lattice provides 0.0% improvement (neutral, no degradation), gracefully deferring when structure is already present. This bidirectional validation - activating when patterns apply, refusing when they don't, and deferring when redundant - supports confidence gating as a promising architectural principle for managing epistemic uncertainty in safety-critical applications.",
      "authors": [
        "Lorian Bannis"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-21 19:37:57+00:00",
      "link": "https://arxiv.org/pdf/2601.15423v1",
      "tags": [
        "keyword:EAA",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15417v1",
      "title": "Ambient Dataloops: Generative Models for Dataset Refinement",
      "abstract": "We propose Ambient Dataloops, an iterative framework for refining datasets that makes it easier for diffusion models to learn the underlying data distribution. Modern datasets contain samples of highly varying quality, and training directly on such heterogeneous data often yields suboptimal models. We propose a dataset-model co-evolution process; at each iteration of our method, the dataset becomes progressively higher quality, and the model improves accordingly. To avoid destructive self-consuming loops, at each generation, we treat the synthetically improved samples as noisy, but at a slightly lower noisy level than the previous iteration, and we use Ambient Diffusion techniques for learning under corruption. Empirically, Ambient Dataloops achieve state-of-the-art performance in unconditional and text-conditional image generation and de novo protein design. We further provide a theoretical justification for the proposed framework that captures the benefits of the data looping procedure.",
      "authors": [
        "Adrián Rodríguez-Muñoz",
        "William Daspit",
        "Adam Klivans",
        "Antonio Torralba",
        "Constantinos Daskalakis",
        "Giannis Daras"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-21 19:29:04+00:00",
      "link": "https://arxiv.org/pdf/2601.15417v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15399v1",
      "title": "Attention-Informed Surrogates for Navigating Power-Performance Trade-offs in HPC",
      "abstract": "High-Performance Computing (HPC) schedulers must balance user performance with facility-wide resource constraints. The task boils down to selecting the optimal number of nodes for a given job. We present a surrogate-assisted multi-objective Bayesian optimization (MOBO) framework to automate this complex decision. Our core hypothesis is that surrogate models informed by attention-based embeddings of job telemetry can capture performance dynamics more effectively than standard regression techniques. We pair this with an intelligent sample acquisition strategy to ensure the approach is data-efficient. On two production HPC datasets, our embedding-informed method consistently identified higher-quality Pareto fronts of runtime-power trade-offs compared to baselines. Furthermore, our intelligent data sampling strategy drastically reduced training costs while improving the stability of the results. To our knowledge, this is the first work to successfully apply embedding-informed surrogates in a MOBO framework to the HPC scheduling problem, jointly optimizing for performance and power on production workloads.",
      "authors": [
        "Ashna Nawar Ahmed",
        "Banooqa Banday",
        "Terry Jones",
        "Tanzima Z. Islam"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-21 19:11:12+00:00",
      "link": "https://arxiv.org/pdf/2601.15399v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15394v1",
      "title": "Memorization Dynamics in Knowledge Distillation for Language Models",
      "abstract": "Knowledge Distillation (KD) is increasingly adopted to transfer capabilities from large language models to smaller ones, offering significant improvements in efficiency and utility while often surpassing standard fine-tuning. Beyond performance, KD is also explored as a privacy-preserving mechanism to mitigate the risk of training data leakage. While training data memorization has been extensively studied in standard pre-training and fine-tuning settings, its dynamics in a knowledge distillation setup remain poorly understood. In this work, we study memorization across the KD pipeline using three large language model (LLM) families (Pythia, OLMo-2, Qwen-3) and three datasets (FineWeb, Wikitext, Nemotron-CC-v2). We find: (1) distilled models memorize significantly less training data than standard fine-tuning (reducing memorization by more than 50%); (2) some examples are inherently easier to memorize and account for a large fraction of memorization during distillation (over ~95%); (3) student memorization is predictable prior to distillation using features based on zlib entropy, KL divergence, and perplexity; and (4) while soft and hard distillation have similar overall memorization rates, hard distillation poses a greater risk: it inherits $2.7\\times$ more teacher-specific examples than soft distillation. Overall, we demonstrate that distillation can provide both improved generalization and reduced memorization risks compared to standard fine-tuning.",
      "authors": [
        "Jaydeep Borkar",
        "Karan Chadha",
        "Niloofar Mireshghallah",
        "Yuchen Zhang",
        "Irina-Elena Veliche",
        "Archi Mitra",
        "David A. Smith",
        "Zheng Xu",
        "Diego Garcia-Olano"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-21 19:04:40+00:00",
      "link": "https://arxiv.org/pdf/2601.15394v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15390v1",
      "title": "FedUMM: A General Framework for Federated Learning with Unified Multimodal Models",
      "abstract": "Unified multimodal models (UMMs) are emerging as strong foundation models that can do both generation and understanding tasks in a single architecture. However, they are typically trained in centralized settings where all training and downstream datasets are gathered in a central server, limiting the deployment in privacy-sensitive and geographically distributed scenarios. In this paper, we present FedUMM, a general federated learning framework for UMMs under non-IID multimodal data with low communication cost. Built on NVIDIA FLARE, FedUMM instantiates federation for a BLIP3o backbone via parameter-efficient fine-tuning: clients train lightweight LoRA adapters while freezing the foundation models, and the server aggregates only adapter updates. We evaluate on VQA v2 and the GenEval compositional generation benchmarks under Dirichlet-controlled heterogeneity with up to 16 clients. Results show slight degradation as client count and heterogeneity increase, while remaining competitive with centralized training. We further analyze computation--communication trade-offs and demonstrate that adapter-only federation reduces per-round communication by over an order of magnitude compared to full fine-tuning, enabling practical federated UMM training. This work provides empirical experience for future research on privacy-preserving federated unified multimodal models.",
      "authors": [
        "Zhaolong Su",
        "Leheng Zhao",
        "Xiaoying Wu",
        "Ziyue Xu",
        "Jindong Wang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-21 19:02:52+00:00",
      "link": "https://arxiv.org/pdf/2601.15390v1",
      "tags": [
        "keyword:EAA",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15370v1",
      "title": "Improving MoE Compute Efficiency by Composing Weight and Data Sparsity",
      "abstract": "Mixture-of-Experts layers achieve compute efficiency through weight sparsity: each token activates only a subset of experts. Data sparsity, where each expert processes only a subset of tokens, offers a complementary axis. Expert-choice routing implements data sparsity directly but violates causality in autoregressive models, creating train-inference mismatch. We recover data sparsity within causal token-choice MoE by leveraging zero-compute (null) experts within the routing pool. When a token routes to null experts, those slots consume no compute. The standard load balancing objective trains the model to uniformly use all experts (real and null) therefore creating data sparsity in expectation without the causality violations. We evaluate on vision-language model training, where data heterogeneity is pronounced: vision encoders produce many low-information tokens while text tokens are denser. At matched expected FLOPs, composing weight and data sparsity yields a more compute-efficient frontier than weight sparsity alone, with gains in training loss and downstream performance. The model learns implicit modality-aware allocation, routing vision tokens to null experts more aggressively than text, without explicit modality routing.",
      "authors": [
        "Maciej Kilian",
        "Oleg Mkrtchyan",
        "Luke Zettlemoyer",
        "Akshat Shrivastava",
        "Armen Aghajanyan"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-21 18:53:58+00:00",
      "link": "https://arxiv.org/pdf/2601.15370v1",
      "tags": [
        "keyword:EAA"
      ]
    },
    {
      "id": "2601.15258v1",
      "title": "Distributed Agent-Constrained Truthful Facility Location",
      "abstract": "We study a distributed facility location problem in which a set of agents, each with a private position on the real line, is partitioned into a collection of fixed, disjoint groups. The goal is to open $k$ facilities at locations chosen from the set of positions reported by the agents. This decision is made by mechanisms that operate in two phases. In Phase 1, each group selects the position of one of its agents to serve as the group's representative location. In Phase 2, $k$ representatives are chosen as facility locations. Once the facility locations are determined, each agent incurs an individual cost, defined either as the sum of its distances to all facilities (sum-variant) or as the distance to its farthest facility (max-variant). We focus on the class of strategyproof mechanisms, which preclude the agents from benefiting through strategic misreporting, and establish tight bounds on the approximation ratio with respect to the social cost (the total individual agent cost) in both variants.",
      "authors": [
        "Argyrios Deligkas",
        "Panagiotis Kanellopoulos",
        "Alexandros A. Voudouris"
      ],
      "primary_category": "cs.GT",
      "categories": [
        "cs.GT"
      ],
      "published": "2026-01-21 18:40:17+00:00",
      "link": "https://arxiv.org/pdf/2601.15258v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15249v2",
      "title": "Recommending Best Paper Awards for ML/AI Conferences via the Isotonic Mechanism",
      "abstract": "Machine learning and artificial intelligence conferences such as NeurIPS and ICML now regularly receive tens of thousands of submissions, posing significant challenges to maintaining the quality and consistency of the peer review process. This challenge is particularly acute for best paper awards, which are an important part of the peer review process, yet whose selection has increasingly become a subject of debate in recent years. In this paper, we introduce an author-assisted mechanism to facilitate the selection of best paper awards. Our method employs the Isotonic Mechanism for eliciting authors' assessments of their own submissions in the form of a ranking, which is subsequently utilized to adjust the raw review scores for optimal estimation of the submissions' ground-truth quality. We demonstrate that authors are incentivized to report truthfully when their utility is a convex additive function of the adjusted scores, and we validate this convexity assumption for best paper awards using publicly accessible review data of ICLR from 2019 to 2023 and NeurIPS from 2021 to 2023. Crucially, in the special case where an author has a single quota -- that is, may nominate only one paper -- we prove that truthfulness holds even when the utility function is merely nondecreasing and additive. This finding represents a substantial relaxation of the assumptions required in prior work. For practical implementation, we extend our mechanism to accommodate the common scenario of overlapping authorship. Finally, simulation results demonstrate that our mechanism significantly improves the quality of papers selected for awards.",
      "authors": [
        "Garrett G. Wen",
        "Buxin Su",
        "Natalie Collina",
        "Zhun Deng",
        "Weijie Su"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GT",
        "stat.ME"
      ],
      "published": "2026-01-21 18:30:42+00:00",
      "link": "https://arxiv.org/pdf/2601.15249v2",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15247v1",
      "title": "Taxonomy-Aligned Risk Extraction from 10-K Filings with Autonomous Improvement Using LLMs",
      "abstract": "We present a methodology for extracting structured risk factors from corporate 10-K filings while maintaining adherence to a predefined hierarchical taxonomy. Our three-stage pipeline combines LLM extraction with supporting quotes, embedding-based semantic mapping to taxonomy categories, and LLM-as-a-judge validation that filters spurious assignments. To evaluate our approach, we extract 10,688 risk factors from S&P 500 companies and examine risk profile similarity across industry clusters. Beyond extraction, we introduce autonomous taxonomy maintenance where an AI agent analyzes evaluation feedback to identify problematic categories, diagnose failure patterns, and propose refinements, achieving 104.7% improvement in embedding separation in a case study. External validation confirms the taxonomy captures economically meaningful structure: same-industry companies exhibit 63% higher risk profile similarity than cross-industry pairs (Cohen's d=1.06, AUC 0.82, p<0.001). The methodology generalizes to any domain requiring taxonomy-aligned extraction from unstructured text, with autonomous improvement enabling continuous quality maintenance and enhancement as systems process more documents.",
      "authors": [
        "Rian Dolphin",
        "Joe Dursun",
        "Jarrett Blankenship",
        "Katie Adams",
        "Quinton Pike"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-21 18:28:31+00:00",
      "link": "https://arxiv.org/pdf/2601.15247v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15241v1",
      "title": "Feasibility Preservation under Monotone Retrieval Truncation",
      "abstract": "Retrieval-based systems approximate access to a corpus by exposing only a truncated subset of available evidence. Even when relevant information exists in the corpus, truncation can prevent compatible evidence from co-occurring, leading to failures that are not captured by relevance-based evaluation. This paper studies retrieval from a structural perspective, modeling query answering as a feasibility problem under truncation.   We formalize retrieval as a sequence of candidate evidence sets and characterize conditions under which feasibility in the limit implies feasibility at finite retrieval depth. We show that monotone truncation suffices to guarantee finite witnessability for individual queries. For classes of queries, we identify finite generation of witness certificates as the additional condition required to obtain a uniform retrieval bound, and we show that this condition is necessary. We further exhibit sharp counterexamples demonstrating failure under non-monotone truncation, non-finitely-generated query classes, and purely slotwise coverage.   Together, these results isolate feasibility preservation as a correctness criterion for retrieval independent of relevance scoring or optimization, and clarify structural limitations inherent to truncation-based retrieval.",
      "authors": [
        "Sean Plummer"
      ],
      "primary_category": "cs.LO",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "published": "2026-01-21 18:25:16+00:00",
      "link": "https://arxiv.org/pdf/2601.15241v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15236v1",
      "title": "Metadata Conditioned Large Language Models for Localization",
      "abstract": "Large language models are typically trained by treating text as a single global distribution, often resulting in geographically homogenized behavior. We study metadata conditioning as a lightweight approach for localization, pre-training 31 models (at 0.5B and 1B parameter scales) from scratch on large-scale English news data annotated with verified URLs, country tags, and continent tags, covering 4 continents and 17 countries. Across four controlled experiments, we show that metadata conditioning consistently improves in-region performance without sacrificing cross-region generalization, enables global models to recover localization comparable to region-specific models, and improves learning efficiency. Our ablation studies demonstrate that URL-level metadata alone captures much of the geographic signal, while balanced regional data coverage remains essential, as metadata cannot fully compensate for missing regions. Finally, we introduce a downstream benchmark of 800 localized news MCQs and show that after instruction tuning, metadata conditioned global models achieve accuracy comparable to LLaMA-3.2-1B-Instruct, despite being trained on substantially less data. Together, these results establish metadata conditioning as a practical and compute-efficient approach for localization of language models.",
      "authors": [
        "Anjishnu Mukherjee",
        "Ziwei Zhu",
        "Antonios Anastasopoulos"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-21 18:20:59+00:00",
      "link": "https://arxiv.org/pdf/2601.15236v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15212v1",
      "title": "ZENITH: Automated Gradient Norm Informed Stochastic Optimization",
      "abstract": "Training deep computer vision models requires manual oversight or hyperparameter tuning of the learning rate (LR) schedule. While existing adaptive optimizers schedule the LR automatically, they suffer from computational and memory overhead, incompatibility with regularization, and suboptimal LR choices. In this work, we introduce the ZENITH (Zero-overhead Evolution using Norm-Informed Training History) optimizer, which adapts the LR using the temporal evolution of the gradient norm. Image classification experiments spanning 6 CNN architectures and 6 benchmarks demonstrate that ZENITH achieves higher test accuracy in lower wall-clock time than baselines. It also yielded superior mAP in object detection, keypoint detection, and instance segmentation on MS COCO using the R-CNN family of models. Furthermore, its compatibility with regularization enables even better generalization.",
      "authors": [
        "Dhrubo Saha"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "published": "2026-01-21 17:36:12+00:00",
      "link": "https://arxiv.org/pdf/2601.15212v1",
      "tags": [
        "keyword:EAA"
      ]
    },
    {
      "id": "2601.15195v1",
      "title": "Where Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub",
      "abstract": "AI coding agents are now submitting pull requests (PRs) to software projects, acting not just as assistants but as autonomous contributors. As these agentic contributions are rapidly increasing across real repositories, little is known about how they behave in practice and why many of them fail to be merged. In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub. (RQ1) We first quantitatively characterize merged and not-merged PRs along four broad dimensions: 1) merge outcomes across task types, 2) code changes, 3) CI build results, and 4) review dynamics. We observe that tasks related to documentation, CI, and build update achieve the highest merge success, whereas performance and bug-fix tasks perform the worst. Not-merged PRs tend to involve larger code changes, touch more files, and often do not pass the project's CI/CD pipeline validation. (RQ2) To further investigate why some agentic PRs are not merged, we qualitatively analyze 600 PRs to derive a hierarchical taxonomy of rejection patterns. This analysis complements the quantitative findings in RQ1 by uncovering rejection reasons not captured by quantitative metrics, including lack of meaningful reviewer engagement, duplicate PRs, unwanted feature implementations, and agent misalignment. Together, our findings highlight key socio-technical and human-AI collaboration factors that are critical to improving the success of future agentic workflows.",
      "authors": [
        "Ramtin Ehsani",
        "Sakshi Pathak",
        "Shriya Rawal",
        "Abdullah Al Mujahid",
        "Mia Mohammad Imran",
        "Preetha Chatterjee"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "published": "2026-01-21 17:12:46+00:00",
      "link": "https://arxiv.org/pdf/2601.15195v1",
      "tags": [
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15188v1",
      "title": "Benchmarking Large Language Models for ABAP Code Generation: An Empirical Study on Iterative Improvement by Compiler Feedback",
      "abstract": "This work investigates the performance of Large Language Models (LLMs) in generating ABAP code. Despite successful applications of generative AI in many programming languages, there are hardly any systematic analyses of ABAP code generation to date. The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code, how effectively they use compiler feedback for iterative improvement, and which task types pose special challenges. For this purpose, a benchmark with 180 tasks is conducted, consisting of adapted HumanEval tasks and practical SAP scenarios. The results show significant performance differences between the models: more powerful LLMs achieve success rates of around 75% after several iterations and benefit greatly from compiler feedback, while smaller models perform significantly weaker. Overall, the study highlights the high potential of powerful LLMs for ABAP development processes, especially in iterative error correction.",
      "authors": [
        "Stephan Wallraven",
        "Tim Köhne",
        "Hartmut Westenberger",
        "Andreas Moser"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "published": "2026-01-21 17:06:41+00:00",
      "link": "https://arxiv.org/pdf/2601.15188v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15182v1",
      "title": "Supporting Humans in Evaluating AI Summaries of Legal Depositions",
      "abstract": "While large language models (LLMs) are increasingly used to summarize long documents, this trend poses significant challenges in the legal domain, where the factual accuracy of deposition summaries is crucial. Nugget-based methods have been shown to be extremely helpful for the automated evaluation of summarization approaches. In this work, we translate these methods to the user side and explore how nuggets could directly assist end users. Although prior systems have demonstrated the promise of nugget-based evaluation, its potential to support end users remains underexplored. Focusing on the legal domain, we present a prototype that leverages a factual nugget-based approach to support legal professionals in two concrete scenarios: (1) determining which of two summaries is better, and (2) manually improving an automatically generated summary.",
      "authors": [
        "Naghmeh Farzi",
        "Laura Dietz",
        "Dave D. Lewis"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "published": "2026-01-21 17:00:40+00:00",
      "link": "https://arxiv.org/pdf/2601.15182v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15178v1",
      "title": "Complexity analysis and practical resolution of the data classification problem with private characteristics",
      "abstract": "In this work we analyze the problem of, given the probability distribution of a population, questioning an unknown individual that is representative of the distribution so that our uncertainty about certain characteristics is significantly reduced -but the uncertainty about others, deemed private or sensitive, is not. Thus, the goal of the problem is extracting information being relevant to a legitimate purpose while preserving the privacy of individuals, which is crucial to enable non-intrusive selection processes in several areas. For instance, it is essential in the design of non-discriminatory personnel selection, promotion, and layoff processes in companies and institutions; in the retrieval of customer information being relevant to the service provided by a company (and no more); in certifications not revealing sensitive industrial information being irrelevant for the certification itself; etc. Interactive questioning processes are constructed for this purpose, which requires generalizing the notion of decision trees to account the amount of desired and undesired information retrieved for each branch of the plan. Our findings about this problem are both theoretical and practical: on the one hand, we prove its NP-completeness by a reduction from the Set Cover problem; and on the other hand, given this intractability, we provide heuristic solutions to find reasonable solutions in affordable time. In particular, a greedy algorithm and two genetic algorithms are presented. Our experiments indicate that the best results are obtained using a genetic algorithm reinforced with a greedy strategy.",
      "authors": [
        "David Pantoja",
        "Ismael Rodriguez",
        "Fernando Rubio",
        "Clara Segura"
      ],
      "primary_category": "cs.CC",
      "categories": [
        "cs.CC"
      ],
      "published": "2026-01-21 16:55:32+00:00",
      "link": "https://arxiv.org/pdf/2601.15178v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15172v1",
      "title": "Is Peer Review Really in Decline? Analyzing Review Quality across Venues and Time",
      "abstract": "Peer review is at the heart of modern science. As submission numbers rise and research communities grow, the decline in review quality is a popular narrative and a common concern. Yet, is it true? Review quality is difficult to measure, and the ongoing evolution of reviewing practices makes it hard to compare reviews across venues and time. To address this, we introduce a new framework for evidence-based comparative study of review quality and apply it to major AI and machine learning conferences: ICLR, NeurIPS and *ACL. We document the diversity of review formats and introduce a new approach to review standardization. We propose a multi-dimensional schema for quantifying review quality as utility to editors and authors, coupled with both LLM-based and lightweight measurements. We study the relationships between measurements of review quality, and its evolution over time. Contradicting the popular narrative, our cross-temporal analysis reveals no consistent decline in median review quality across venues and years. We propose alternative explanations, and outline recommendations to facilitate future empirical studies of review quality.",
      "authors": [
        "Ilia Kuznetsov",
        "Rohan Nayak",
        "Alla Rozovskaya",
        "Iryna Gurevych"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-21 16:48:29+00:00",
      "link": "https://arxiv.org/pdf/2601.15172v1",
      "tags": [
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15170v1",
      "title": "Large-Scale Multidimensional Knowledge Profiling of Scientific Literature",
      "abstract": "The rapid expansion of research across machine learning, vision, and language has produced a volume of publications that is increasingly difficult to synthesize. Traditional bibliometric tools rely mainly on metadata and offer limited visibility into the semantic content of papers, making it hard to track how research themes evolve over time or how different areas influence one another. To obtain a clearer picture of recent developments, we compile a unified corpus of more than 100,000 papers from 22 major conferences between 2020 and 2025 and construct a multidimensional profiling pipeline to organize and analyze their textual content. By combining topic clustering, LLM-assisted parsing, and structured retrieval, we derive a comprehensive representation of research activity that supports the study of topic lifecycles, methodological transitions, dataset and model usage patterns, and institutional research directions. Our analysis highlights several notable shifts, including the growth of safety, multimodal reasoning, and agent-oriented studies, as well as the gradual stabilization of areas such as neural machine translation and graph-based methods. These findings provide an evidence-based view of how AI research is evolving and offer a resource for understanding broader trends and identifying emerging directions. Code and dataset: https://github.com/xzc-zju/Profiling_Scientific_Literature",
      "authors": [
        "Zhucun Xue",
        "Jiangning Zhang",
        "Juntao Jiang",
        "Jinzhuo Liu",
        "Haoyang He",
        "Teng Hu",
        "Xiaobin Hu",
        "Guangming Yao",
        "Yi Yuan",
        "Yong Liu"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-01-21 16:47:05+00:00",
      "link": "https://arxiv.org/pdf/2601.15170v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15167v1",
      "title": "DeGAS: Gradient-Based Optimization of Probabilistic Programs without Sampling",
      "abstract": "We present DeGAS, a differentiable Gaussian approximate semantics for loopless probabilistic programs that enables sample-free, gradient-based optimization in models with both continuous and discrete components. DeGAS evaluates programs under a Gaussian-mixture semantics and replaces measure-zero predicates and discrete branches with a vanishing smoothing, yielding closed-form expressions for posterior and path probabilities. We prove differentiability of these quantities with respect to program parameters, enabling end-to-end optimization via standard automatic differentiation, without Monte Carlo estimators. On thirteen benchmark programs, DeGAS achieves accuracy and runtime competitive with variational inference and MCMC. Importantly, it reliably tackles optimization problems where sampling-based baselines fail to converge due to conditioning involving continuous variables.",
      "authors": [
        "Francesca Randone",
        "Romina Doz",
        "Mirco Tribastone",
        "Luca Bortolussi"
      ],
      "primary_category": "cs.PL",
      "categories": [
        "cs.PL"
      ],
      "published": "2026-01-21 16:45:10+00:00",
      "link": "https://arxiv.org/pdf/2601.15167v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15165v1",
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "abstract": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap",
      "authors": [
        "Zanlin Ni",
        "Shenzhi Wang",
        "Yang Yue",
        "Tianyu Yu",
        "Weilin Zhao",
        "Yeguo Hua",
        "Tianyi Chen",
        "Jun Song",
        "Cheng Yu",
        "Bo Zheng",
        "Gao Huang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-21 16:41:58+00:00",
      "link": "https://arxiv.org/pdf/2601.15165v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15160v1",
      "title": "Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning",
      "abstract": "Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a \"compositional bridge\", enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.",
      "authors": [
        "Yuval Kansal",
        "Niraj K. Jha"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-21 16:38:59+00:00",
      "link": "https://arxiv.org/pdf/2601.15160v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15158v1",
      "title": "Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data",
      "abstract": "Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive gradient descent to discover such systematic reasoning remains poorly understood. We address this by analyzing the gradient flow dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought (CoT) but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, gradient flow drives the model to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties required for this emergence, identifying the critical role of \"simple examples\": instances requiring fewer reasoning steps. When the training distribution places sufficient mass on these simpler instances, the model learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, gradient-based learning becomes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.",
      "authors": [
        "Yuval Ran-Milo",
        "Yotam Alexander",
        "Shahar Mendel",
        "Nadav Cohen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-21 16:36:19+00:00",
      "link": "https://arxiv.org/pdf/2601.15158v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15153v1",
      "title": "How to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain Knowledge? A Software Engineering Framework",
      "abstract": "Critical domain knowledge typically resides with few experts, creating organizational bottlenecks in scalability and decision-making. Non-experts struggle to create effective visualizations, leading to suboptimal insights and diverting expert time. This paper investigates how to capture and embed human domain knowledge into AI agent systems through an industrial case study. We propose a software engineering framework to capture human domain knowledge for engineering AI agents in simulation data visualization by augmenting a Large Language Model (LLM) with a request classifier, Retrieval-Augmented Generation (RAG) system for code generation, codified expert rules, and visualization design principles unified in an agent demonstrating autonomous, reactive, proactive, and social behavior. Evaluation across five scenarios spanning multiple engineering domains with 12 evaluators demonstrates 206% improvement in output quality, with our agent achieving expert-level ratings in all cases versus baseline's poor performance, while maintaining superior code quality with lower variance. Our contributions are: an automated agent-based system for visualization generation and a validated framework for systematically capturing human domain knowledge and codifying tacit expert knowledge into AI agents, demonstrating that non-experts can achieve expert-level outcomes in specialized domains.",
      "authors": [
        "Choro Ulan uulu",
        "Mikhail Kulyabin",
        "Iris Fuhrmann",
        "Jan Joosten",
        "Nuno Miguel Martins Pacheco",
        "Filippos Petridis",
        "Rebecca Johnson",
        "Jan Bosch",
        "Helena Holmström Olsson"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-21 16:23:22+00:00",
      "link": "https://arxiv.org/pdf/2601.15153v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15148v1",
      "title": "Interval Scheduling Games",
      "abstract": "We consider a game-theoretic variant of an interval scheduling problem. Every job is associated with a length, a weight, and a color. Each player controls all the jobs of a specific color, and needs to decide on a processing interval for each of its jobs. Jobs of the same color can be processed simultaneously by the machine. A job is covered if the machine is configured to its color during its whole processing interval. The goal of the machine is to maximize the sum of weights of all covered jobs, and the goal of each player is to place its jobs such that the sum of weights of covered jobs from its color is maximized. The study of this game is motivated by several applications like antenna scheduling for wireless networks.   We first show that given a strategy profile of the players, the machine scheduling problem can be solved in polynomial time. We then study the game from the players' point of view. We analyze the existence of Nash equilibria, its computation, and inefficiency. We distinguish between instances of the classical interval scheduling problem, in which every player controls a single job, and instances in which color sets may include multiple jobs.",
      "authors": [
        "Vipin Ravindran Vijayalakshmi",
        "Marc Schroder",
        "Tami Tamir"
      ],
      "primary_category": "cs.GT",
      "categories": [
        "cs.GT"
      ],
      "published": "2026-01-21 16:20:44+00:00",
      "link": "https://arxiv.org/pdf/2601.15148v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15141v1",
      "title": "CLEANER: Self-Purified Trajectories Boost Agentic Reinforcement Learning",
      "abstract": "Agentic Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to utilize tools like Python interpreters for complex problem-solving. However, for parameter-constrained models (e.g., 4B--7B), the exploration phase is often plagued by frequent execution failures, creating noisy trajectories that hinder policy optimization. Under standard outcome-based reward settings, this noise leads to a critical credit assignment issue, where erroneous actions are inadvertently reinforced alongside successful outcomes. Existing mitigations face a dilemma: dense rewards often trigger reward hacking, while supersampling incurs prohibitive computational costs. To address these challenges, we propose CLEANER. Distinct from external filtering methods, CLEANER exploits the model's intrinsic self-correction capabilities to eliminate error-contaminated context directly during data collection. At its core, the Similarity-Aware Adaptive Rollback (SAAR) mechanism autonomously constructs clean, purified trajectories by retrospectively replacing failures with successful self-corrections. Based on semantic similarity, SAAR adaptively regulates replacement granularity from shallow execution repairs to deep reasoning substitutions. By training on these self-purified paths, the model internalizes correct reasoning patterns rather than error-recovery loops. Empirical results on AIME24/25, GPQA, and LiveCodeBench show average accuracy gains of 6%, 3%, and 5% over baselines. Notably, CLEANER matches state-of-the-art performance using only one-third of the training steps, highlighting trajectory purification as a scalable solution for efficient agentic RL. Our models and code are available at GitHub",
      "authors": [
        "Tianshi Xu",
        "Yuteng Chen",
        "Meng Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-21 16:14:30+00:00",
      "link": "https://arxiv.org/pdf/2601.15141v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15136v1",
      "title": "Conversational AI for Social Good (CAI4SG): An Overview of Emerging Trends, Applications, and Challenges",
      "abstract": "The integration of Conversational Agents (CAs) into daily life offers opportunities to tackle global challenges, leading to the emergence of Conversational AI for Social Good (CAI4SG). This paper examines the advancements of CAI4SG using a role-based framework that categorizes systems according to their AI autonomy and emotional engagement. This framework emphasizes the importance of considering the role of CAs in social good contexts, such as serving as empathetic supporters in mental health or functioning as assistants for accessibility. Additionally, exploring the deployment of CAs in various roles raises unique challenges, including algorithmic bias, data privacy, and potential socio-technical harms. These issues can differ based on the CA's role and level of engagement. This paper provides an overview of the current landscape, offering a role-based understanding that can guide future research and design aimed at the equitable, ethical, and effective development of CAI4SG.",
      "authors": [
        "Yi-Chieh Lee",
        "Junti Zhang",
        "Tianqi Song",
        "Yugin Tan"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC"
      ],
      "published": "2026-01-21 16:10:32+00:00",
      "link": "https://arxiv.org/pdf/2601.15136v1",
      "tags": [
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15131v1",
      "title": "Vehicle Routing with Finite Time Horizon using Deep Reinforcement Learning with Improved Network Embedding",
      "abstract": "In this paper, we study the vehicle routing problem with a finite time horizon. In this routing problem, the objective is to maximize the number of customer requests served within a finite time horizon. We present a novel routing network embedding module which creates local node embedding vectors and a context-aware global graph representation. The proposed Markov decision process for the vehicle routing problem incorporates the node features, the network adjacency matrix and the edge features as components of the state space. We incorporate the remaining finite time horizon into the network embedding module to provide a proper routing context to the embedding module. We integrate our embedding module with a policy gradient-based deep Reinforcement Learning framework to solve the vehicle routing problem with finite time horizon. We trained and validated our proposed routing method on real-world routing networks, as well as synthetically generated Euclidean networks. Our experimental results show that our method achieves a higher customer service rate than the existing routing methods. Additionally, the solution time of our method is significantly lower than that of the existing methods.",
      "authors": [
        "Ayan Maity",
        "Sudeshna Sarkar"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-21 16:05:04+00:00",
      "link": "https://arxiv.org/pdf/2601.15131v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15130v1",
      "title": "The Plausibility Trap: Using Probabilistic Engines for Deterministic Tasks",
      "abstract": "The ubiquity of Large Language Models (LLMs) is driving a paradigm shift where user convenience supersedes computational efficiency. This article defines the \"Plausibility Trap\": a phenomenon where individuals with access to Artificial Intelligence (AI) models deploy expensive probabilistic engines for simple deterministic tasks-such as Optical Character Recognition (OCR) or basic verification-resulting in significant resource waste. Through micro-benchmarks and case studies on OCR and fact-checking, we quantify the \"efficiency tax\"-demonstrating a ~6.5x latency penalty-and the risks of algorithmic sycophancy. To counter this, we introduce Tool Selection Engineering and the Deterministic-Probabilistic Decision Matrix, a framework to help developers determine when to use Generative AI and, crucially, when to avoid it. We argue for a curriculum shift, emphasizing that true digital literacy relies not only in knowing how to use Generative AI, but also on knowing when not to use it.",
      "authors": [
        "Ivan Carrera",
        "Daniel Maldonado-Ruiz"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-21 16:05:01+00:00",
      "link": "https://arxiv.org/pdf/2601.15130v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15127v1",
      "title": "DeepFedNAS: A Unified Framework for Principled, Hardware-Aware, and Predictor-Free Federated Neural Architecture Search",
      "abstract": "Federated Neural Architecture Search (FedNAS) aims to automate model design for privacy-preserving Federated Learning (FL) but currently faces two critical bottlenecks: unguided supernet training that yields suboptimal models, and costly multi-hour pipelines for post-training subnet discovery. We introduce DeepFedNAS, a novel, two-phase framework underpinned by a principled, multi-objective fitness function that synthesizes mathematical network design with architectural heuristics. Enabled by a re-engineered supernet, DeepFedNAS introduces Federated Pareto Optimal Supernet Training, which leverages a pre-computed Pareto-optimal cache of high-fitness architectures as an intelligent curriculum to optimize shared supernet weights. Subsequently, its Predictor-Free Search Method eliminates the need for costly accuracy surrogates by utilizing this fitness function as a direct, zero-cost proxy for accuracy, enabling on-demand subnet discovery in mere seconds. DeepFedNAS achieves state-of-the-art accuracy (e.g., up to 1.21% absolute improvement on CIFAR-100), superior parameter and communication efficiency, and a substantial ~61x speedup in total post-training search pipeline time. By reducing the pipeline from over 20 hours to approximately 20 minutes (including initial cache generation) and enabling 20-second individual subnet searches, DeepFedNAS makes hardware-aware FL deployments instantaneous and practical. The complete source code and experimental scripts are available at: https://github.com/bostankhan6/DeepFedNAS",
      "authors": [
        "Bostan Khan",
        "Masoud Daneshtalab"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV",
        "cs.DC"
      ],
      "published": "2026-01-21 16:03:25+00:00",
      "link": "https://arxiv.org/pdf/2601.15127v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15124v1",
      "title": "Overcoming In-Memory Bottlenecks in Graph Foundation Models via Retrieval-Augmented Generation",
      "abstract": "Graph Foundation Models (GFMs) have emerged as a frontier in graph learning, which are expected to deliver transferable representations across diverse tasks. However, GFMs remain constrained by in-memory bottlenecks: they attempt to encode knowledge into model parameters, which limits semantic capacity, introduces heavy lossy compression with conflicts, and entangles graph representation with the knowledge in ways that hinder efficient adaptation, undermining scalability and interpretability. In this work,we propose RAG-GFM, a Retrieval-Augmented Generation aided Graph Foundation Model that offloads knowledge from parameters and complements parameterized learning. To externalize graph knowledge, we build a dual-modal unified retrieval module, where a semantic store from prefix-structured text and a structural store from centrality-based motif. To preserve heterogeneous information, we design a dual-view alignment objective that contrasts both modalities to capture both content and relational patterns. To enable efficient downstream adaptation, we perform in-context augmentation to enrich supporting instances with retrieved texts and motifs as contextual evidence. Extensive experiments on five benchmark graph datasets demonstrate that RAG-GFM consistently outperforms 13 state-of-the-art baselines in both cross-domain node and graph classification, achieving superior effectiveness and efficiency.",
      "authors": [
        "Haonan Yuan",
        "Qingyun Sun",
        "Jiacheng Tao",
        "Xingcheng Fu",
        "Jianxin Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-21 16:02:43+00:00",
      "link": "https://arxiv.org/pdf/2601.15124v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15120v2",
      "title": "Emerging from Ground: Addressing Intent Deviation in Tool-Using Agents via Deriving Real Calls into Virtual Trajectories",
      "abstract": "LLMs have advanced tool-using agents for real-world applications, yet they often lead to unexpected behaviors or results. Beyond obvious failures, the subtle issue of \"intent deviation\" severely hinders reliable evaluation and performance improvement. Existing post-training methods generally leverage either real system samples or virtual data simulated by LLMs. However, the former is costly due to reliance on hand-crafted user requests, while the latter suffers from distribution shift from the real tools in the wild. Additionally, both methods lack negative samples tailored to intent deviation scenarios, hindering effective guidance on preference learning. We introduce RISE, a \"Real-to-Virtual\" method designed to mitigate intent deviation. Anchoring on verified tool primitives, RISE synthesizes virtual trajectories and generates diverse negative samples through mutation on critical parameters. With synthetic data, RISE fine-tunes backbone LLMs via the two-stage training for intent alignment. Evaluation results demonstrate that data synthesized by RISE achieve promising results in eight metrics covering user requires, execution trajectories and agent responses. Integrating with training, RISE achieves an average 35.28% improvement in Acctask (task completion) and 23.27% in Accintent (intent alignment), outperforming SOTA baselines by 1.20--42.09% and 1.17--54.93% respectively.",
      "authors": [
        "Qian Xiong",
        "Yuekai Huang",
        "Bo Yang",
        "Yujia Zheng",
        "Tianhao Li",
        "Ziyou Jiang",
        "Zhiyuan Chang",
        "Zhaoyang Li",
        "Huanxiang Feng",
        "Mingyang Li"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-21 15:58:54+00:00",
      "link": "https://arxiv.org/pdf/2601.15120v2",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15100v1",
      "title": "Facilitating Proactive and Reactive Guidance for Decision Making on the Web: A Design Probe with WebSeek",
      "abstract": "Web AI agents such as ChatGPT Agent and GenSpark are increasingly used for routine web-based tasks, yet they still rely on text-based input prompts, lack proactive detection of user intent, and offer no support for interactive data analysis and decision making. We present WebSeek, a mixed-initiative browser extension that enables users to discover and extract information from webpages to then flexibly build, transform, and refine tangible data artifacts-such as tables, lists, and visualizations-all within an interactive canvas. Within this environment, users can perform analysis-including data transformations such as joining tables or creating visualizations-while an in-built AI both proactively offers context-aware guidance and automation, and reactively responds to explicit user requests. An exploratory user study (N=15) with WebSeek as a probe reveals participants' diverse analysis strategies, underscoring their desire for transparency and control during human-AI collaboration.",
      "authors": [
        "Yanwei Huang",
        "Arpit Narechania"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC"
      ],
      "published": "2026-01-21 15:38:57+00:00",
      "link": "https://arxiv.org/pdf/2601.15100v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15094v1",
      "title": "Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks",
      "abstract": "Large Language Models (LLMs) have proven highly effective in automating software engineering tasks, bridging natural language and code semantics to achieve notable results in code generation and summarization. However, their scale incurs substantial computational costs, making full fine-tuning impractical. Parameter-Efficient Fine-Tuning (PEFT) methods like QLoRA enable efficient specialization with lower resource demands. Recent studies show QLoRA-optimized Large Code Models (LCMs) perform strongly across diverse tasks, yet it remains unclear whether this effectiveness persists when a single model is QLoRA fine-tuned for multiple code-related tasks. The interaction between Multi-task fine-tuning and QLoRA optimization, and how transfer learning affects correctness and quality of generated artifacts, remains largely unexplored. We investigate Multi-task QLoRA fine-tuning across three representative tasks: code generation, translation, and summarization. We evaluate functional correctness through execution-based and similarity-based metrics, complemented by comprehensive code quality analysis--an aspect largely overlooked in prior work. Our findings show that Multi-task QLoRA effectively leverages transfer learning, achieving competitive or superior performance relative to both Single-task QLoRA and Multi-task full fine-tuning. Larger models demonstrate more consistent balance between correctness and quality, whereas smaller models preserve functionality but exhibit a higher incidence of quality-related issues.",
      "authors": [
        "Md Zahidul Haque",
        "Saima Afrin",
        "Antonio Mastropaolo"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE"
      ],
      "published": "2026-01-21 15:33:16+00:00",
      "link": "https://arxiv.org/pdf/2601.15094v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15079v1",
      "title": "LoRAP: Low-Rank Aggregation Prompting for Quantized Graph Neural Networks Training",
      "abstract": "Graph Neural Networks (GNNs) are neural networks that aim to process graph data, capturing the relationships and interactions between nodes using the message-passing mechanism. GNN quantization has emerged as a promising approach for reducing model size and accelerating inference in resource-constrained environments. Compared to quantization in LLMs, quantizing graph features is more emphasized in GNNs. Inspired by the above, we propose to leverage prompt learning, which manipulates the input data, to improve the performance of quantization-aware training (QAT) for GNNs. To mitigate the issue that prompting the node features alone can only make part of the quantized aggregation result optimal, we introduce Low-Rank Aggregation Prompting (LoRAP), which injects lightweight, input-dependent prompts into each aggregated feature to optimize the results of quantized aggregations. Extensive evaluations on 4 leading QAT frameworks over 9 graph datasets demonstrate that LoRAP consistently enhances the performance of low-bit quantized GNNs while introducing a minimal computational overhead.",
      "authors": [
        "Chenyu Liu",
        "Haige Li",
        "Luca Rossi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.SI"
      ],
      "published": "2026-01-21 15:23:18+00:00",
      "link": "https://arxiv.org/pdf/2601.15079v1",
      "tags": [
        "keyword:EAA"
      ]
    },
    {
      "id": "2601.15077v1",
      "title": "Multi-Agent Constraint Factorization Reveals Latent Invariant Solution Structure",
      "abstract": "Multi-agent systems (MAS) composed of large language models often exhibit improved problem-solving performance despite operating on identical information. In this work, we provide a formal explanation for this phenomenon grounded in operator theory and constrained optimization. We model each agent as enforcing a distinct family of validity constraints on a shared solution state, and show that a MAS implements a factorized composition of constraint-enforcement operators. Under mild conditions, these dynamics converge to invariant solution sets defined by the intersection of agent constraint sets. Such invariant structures are generally not dynamically accessible to a single agent applying all constraints simultaneously, even when expressive capacity and information are identical. We extend this result from exact constraint enforcement to soft constraints via proximal operators, and apply the formalism to contemporary text-based dialog systems.",
      "authors": [
        "Christopher Scofield"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "published": "2026-01-21 15:23:04+00:00",
      "link": "https://arxiv.org/pdf/2601.15077v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.16063v1",
      "title": "Continuum limit of hypergraph $p$-Laplacian equations on point clouds",
      "abstract": "This paper studies a class of $p$-Laplacian equations on point clouds that arise from hypergraph learning in a semi-supervised setting. Under the assumption that the point clouds consist of independent random samples drawn from a bounded domain $Ω\\subset\\mathbb{R}^d$, we investigate the asymptotic behavior of the solutions as the number of data points tends to infinity, with the number of labeled points remains fixed. We show, for any $p>d$ in the viscosity solution framework, that the continuum limit is a weighted $p$-Laplacian equation subject to mixed Dirichlet and Neumann boundary conditions. The result provides a new discretization of the $p$-Laplacian on point clouds.",
      "authors": [
        "Kehan Shi"
      ],
      "primary_category": "math.AP",
      "categories": [
        "math.AP"
      ],
      "published": "2026-01-22 15:59:41+00:00",
      "link": "https://arxiv.org/pdf/2601.16063v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.16043v1",
      "title": "A Second-Order Dynamical System for Solving Generalized Inverse Mixed Variational Inequality problems",
      "abstract": "In this paper, we study a class of generalized inverse mixed variational inequality problems (GIMVIPs). We propose a novel projection-based second-order time-varying dynamical system for solving GIMVIPs. Under the assumptions that the underlying operators are strongly monotone and Lipschitz continuous, we establish the existence and uniqueness of solution trajectories and prove their global exponential convergence to the unique solution of the GIMVIP. Furthermore, a discrete-time realization of the continuous dynamical system is developed, resulting in an inertial projection algorithm. We show that the proposed algorithm achieves linear convergence under suitable choices of parameters. Finally, numerical experiments are presented to illustrate the effectiveness and convergence behavior of the proposed method in solving GIMVIPs.",
      "authors": [
        "Nam Van Tran"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-01-22 15:18:46+00:00",
      "link": "https://arxiv.org/pdf/2601.16043v1",
      "tags": [
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15950v1",
      "title": "Extreme Score Distributions in Countable-Outcome Round-Robin Tournaments of Equally Strong Players",
      "abstract": "We consider a general class of round-robin tournament models of equally strong players. In these models, each of the $n$ players competes against every other player exactly once. For each match between two players, the outcome is a value from a countable subset of the unit interval, and the scores of the two players in a match sum to one. The final score of each player is defined as the sum of the scores obtained in matches against all other players. We study the distribution of extreme scores, including the maximum, second maximum, and lower-order extremes. Since the exact distribution is computationally intractable even for small values of $n$, we derive asymptotic results as the number of players $n$ tends to infinity, including limiting distributions, and rates of convergence.",
      "authors": [
        "Yaakov Malinovsky"
      ],
      "primary_category": "math.PR",
      "categories": [
        "math.PR",
        "math.ST"
      ],
      "published": "2026-01-22 13:38:53+00:00",
      "link": "https://arxiv.org/pdf/2601.15950v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15742v1",
      "title": "A sequential linear complementarity problem method for generalized Nash equilibrium problems",
      "abstract": "We propose a sequential linear complementarity problem (SLCP) method for solving generalized Nash equilibrium problems (GNEPs). By introducing a novel merit function that utilizes the specific structure of GNEPs, we establish global convergence of the method. The conditions guaranteeing global convergence are analogous to those for the classical sequential quadratic programming method with exact Lagrange Hessians, making this a natural and reasonable generalization. Moreover, we provide a detailed analysis of the solvability of the mixed linear complementarity subproblems, which are formulated as affine GNEPs. Sufficient characterizations for the local superlinear convergence are also derived, highlighting the efficiency of the proposed method. Finally, numerical experiments demonstrate the practical performance and effectiveness of the SLCP method in comparison with existing approaches.",
      "authors": [
        "Ruoyu Diao",
        "Yu-Hong Dai",
        "Liwei Zhang"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-01-22 08:12:51+00:00",
      "link": "https://arxiv.org/pdf/2601.15742v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15684v1",
      "title": "Parallelizable Riemannian Alternating Direction Method of Multipliers for Non-convex Pose Graph Optimization",
      "abstract": "Pose graph optimization (PGO) is fundamental to robot perception and navigation systems, serving as the mathematical backbone for solving simultaneous localization and mapping (SLAM). Existing solvers suffer from polynomial growth in computational complexity with graph size, hindering real-time deployment in large-scale scenarios. In this paper, by duplicating variables and introducing equality constraints, we reformulate the problem and propose a Parallelizable Riemannian Alternating Direction Method of Multipliers (PRADMM) to solve it efficiently. Compared with the state-of-the-art methods that usually exhibit polynomial time complexity growth with graph size, PRADMM enables efficient parallel computation across vertices regardless of graph size. Crucially, all subproblems admit closed-form solutions, ensuring PRADMM maintains exceptionally stable performance. Furthermore, by carefully exploiting the structures of the coefficient matrices in the constraints, we establish the global convergence of PRADMM under mild conditions, enabling larger relaxation step sizes within the interval $(0,2)$. Extensive empirical validation on two synthetic datasets and multiple real-world 3D SLAM benchmarks confirms the superior computational performance of PRADMM.",
      "authors": [
        "Xin Chen",
        "Chunfeng Cui",
        "Deren Han",
        "Liqun Qi"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-01-22 06:06:35+00:00",
      "link": "https://arxiv.org/pdf/2601.15684v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15658v1",
      "title": "Construction and Box-counting Dimension of the Edelstein Hidden Variable Fractal Interpolation Function",
      "abstract": "This paper presents the construction of a hidden variable fractal interpolation function using Edelstein contractions in an iterated function system based on a finite collection of data points. The approach incorporates an iterated function system where variable functions act as vertical scaling factors leading to a generalised vector-valued fractal interpolation function. Furthermore, the paper rigorously examines the smoothness of the constructed function and establishes an upper bound for the box-counting dimension of its graph.",
      "authors": [
        "Aiswarya T",
        "Srijanani Anurag Prasad"
      ],
      "primary_category": "math.DS",
      "categories": [
        "math.DS"
      ],
      "published": "2026-01-22 05:15:15+00:00",
      "link": "https://arxiv.org/pdf/2601.15658v1",
      "tags": [
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15636v1",
      "title": "Collaboration versus Specialization in Service Systems with Impatient Customers",
      "abstract": "We study tandem queueing systems in which servers work more efficiently in teams than on their own and customers are impatient in that they may leave the system while waiting for service. Our goal is to determine the server assignment policy that maximizes the long-run average throughput. We show that when each server is equally skilled at all tasks, the optimal policy has all the servers working together at all times. We also provide a complete characterization of the optimal policy for Markovian systems with two stations and two servers when each server's efficiency may be task dependent. We show that the throughput is maximized under the policy which assigns one server to each station (based on their relative skill at that station) unless station 2 has no work (in which case both servers work at station 1) or the number of customers in the buffer reaches a threshold whose value we characterize (in which case both servers work at station 2). We study how the optimal policy varies with the level of server synergy (including no synergy) and also compare the optimal policy for systems with different customer abandonment rates (including no abandonments). Finally, we investigate the case where the synergy among collaborating servers can be task-dependent and provide numerical results.",
      "authors": [
        "Bihan Chatterjee",
        "Sigrún Andradóttir",
        "Hayriye Ayhan"
      ],
      "primary_category": "math.PR",
      "categories": [
        "math.PR"
      ],
      "published": "2026-01-22 04:20:29+00:00",
      "link": "https://arxiv.org/pdf/2601.15636v1",
      "tags": [
        "keyword:EAA",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15627v1",
      "title": "Limit behavior of linearly edge-reinforced random walks on the half-line",
      "abstract": "Motivated by the article [M. Takei, Electron. J. Probab. 26 (2021), article no. 104], we study the limit behavior of linearly edge-reinforced random walks on the half-line $\\mathbb{Z}_+$ with reinforcement parameter $δ>0$, and each edge $\\{x,x+1\\}$ has the initial weight $x^α\\ln^βx$ for $x > 1$ and $1$ for $x = 0, 1$. The aim of this paper is to study the almost sure limit behavior of the walk in the recurrent regime, and extend the results of Takei mentioned above.",
      "authors": [
        "Zechun Hu",
        "Renming Song",
        "Li Wang"
      ],
      "primary_category": "math.PR",
      "categories": [
        "math.PR"
      ],
      "published": "2026-01-22 03:58:14+00:00",
      "link": "https://arxiv.org/pdf/2601.15627v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15601v1",
      "title": "Overpartitions with repeated smallest non-overlined part",
      "abstract": "Inspired by Andrews' and Bachraoui's work on partitions with repeated smallest part, we extend the concept to overpartitions.",
      "authors": [
        "Amita Malik",
        "Rishabh Sarma"
      ],
      "primary_category": "math.CO",
      "categories": [
        "math.CO",
        "math.NT"
      ],
      "published": "2026-01-22 02:56:36+00:00",
      "link": "https://arxiv.org/pdf/2601.15601v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15531v1",
      "title": "Variable Stepsize Distributed Forward-Backward Splitting Methods as Relocated Fixed-Point Iterations",
      "abstract": "We present a family of distributed forward-backward methods with variable stepsizes to find a solution of structured monotone inclusion problems. The framework is constructed by means of relocated fixed-point iterations, extending the approach introduced in arXiv:2507.07428 to conically averaged operators, thus including iteration operators for methods of forward-backward type devised by graphs. The family of methods we construct preserve the per-iteration computational cost and the convergence properties of their constant stepsize counterparts. Specifically, we show that the resulting methods generate a sequence that converges to a fixed-point of the underlying iteration operator, whose shadow sequences converge to a solution of the problem. Numerical experiments illustrate the behaviour of our framework in structured sparse optimisation problems.",
      "authors": [
        "Felipe Atenas",
        "Minh N. Dao",
        "Matthew K. Tam"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-01-21 23:33:25+00:00",
      "link": "https://arxiv.org/pdf/2601.15531v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15411v1",
      "title": "Asymptotic behaviour of coupled random dynamical systems with multiscale aspects",
      "abstract": "We examine a class of stochastic differential inclusions involving multiscale effects designed to solve a class of generalized variational inequalities. This class of problems contains constrained convex non-smooth optimization problems, constrained saddle-point problems and various equilibrium problems in economics and engineering. In order to respect constraints we adopt a penalty approach, introducing an explicit time-dependency into the evolution system. The resulting dynamics are described in terms of a non-autonomous stochastic evolution equation governed by maximally monotone operators in the drift and perturbed by a Brownian motion. We study the asymptotic behavior, as well as finite time convergence rates in terms of gap functions. The condition we use to prove convergence involves a Legendre transform of the function describing the set C, a condition first used by Attouch and Czarnecki (J. Differ. Equations, Vol. 248, Issue 6, 2010) in the context of deterministic evolution equations. We also establish a large deviations principle showing that individual trajectories exhibit exponential concentration around the solution set. Finally we show how our continuous-time approach relates to penalty-regulated algorithms of forward-backward type after performing a suitable Euler-Maruyama discretisation.",
      "authors": [
        "D. Russell Luke",
        "Johannes-Carl Schnebel",
        "Mathias Staudigl",
        "Juan Peypouquet",
        "Siqi Qu"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "math.DS"
      ],
      "published": "2026-01-21 19:20:34+00:00",
      "link": "https://arxiv.org/pdf/2601.15411v1",
      "tags": [
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15398v1",
      "title": "Understanding FISTA's weak convergence: A step-by-step introduction to the 2025 milestone",
      "abstract": "Beck and Teboulle's FISTA for finding the minimizer of the sum of two convex functions is one of the most important algorithms of the past decades. While function value convergence of the iterates was known, the actual convergence of the iterates remained elusive until October 2025 when Jang and Ryu, as well as Boţ, Fadili, and Nguyen proved weak convergence.   In this paper, we provide a gentle self-contained introduction to the proof of their remarkable result.",
      "authors": [
        "Heinz H. Bauschke",
        "Walaa M. Moursi"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-01-21 19:09:19+00:00",
      "link": "https://arxiv.org/pdf/2601.15398v1",
      "tags": [
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15252v1",
      "title": "Automating Idealness Proofs for Binary Programs with Application to Rectangle Packing",
      "abstract": "An integer program is called ideal if its continuous relaxation coincides with its convex hull allowing the problem to be solved as a continuous program and offering substantial computational advantages. Proving idealness analytically can be extraordinarily tedious -- even for small formulations -- such proofs often span many pages of intricate case analysis which motivates the development of automated verification methods. We develop a general-purpose framework for certifying idealness in Mixed Binary Linear Programs (MBLPs), formulating the verification problem as a linear program when the data is fixed and as a nonconvex quadratic program when the data is parametric. We apply this framework to study several formulations of the rectangle packing problem that are conjectured to be pairwise-ideal, obtaining computational proofs where analytic proofs were previously unknown or impractical. As our second contribution, we introduce and model a novel generalization of the rectangle packing problem that enforces edge clearances between selected rectangles. We present both existing and novel MBLP formulations which arise from different encodings of the underlying disjunctive constraints. We perform some computational experiments on these formulations under a strip-packing objective to determine the importance of pairwise-idealness in practice.",
      "authors": [
        "Jamie Fravel",
        "Robert Hildebrand"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-01-21 18:35:20+00:00",
      "link": "https://arxiv.org/pdf/2601.15252v1",
      "tags": [
        "keyword:EAA",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15218v1",
      "title": "Some reverse inequality in optimal mass transportation",
      "abstract": "Controlling the $\\mathcal W_\\infty$ Wasserstein distance by the $\\mathcal W_p$ Wasserstein distance is interesting both for theorical and numerical applications. A first paper on this problem was written several years ago [3]. Some year later [14] framed it in the same inequality for more general costs which increase with the distance. In this paper, we prove this type of inequality for optimal transport problems with pointwise cost which is a decreasing function of the distance. We show, in particular, that there is a general framework that encompasses all the cases above.",
      "authors": [
        "Luigi De Pascale",
        "Igor Pinheiro"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "math.AP"
      ],
      "published": "2026-01-21 17:49:36+00:00",
      "link": "https://arxiv.org/pdf/2601.15218v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15208v1",
      "title": "Penalty-Based Smoothing of Convex Nonsmooth Supremum Functions with Accelerated Inertial Dynamics",
      "abstract": "We propose a penalty-based smoothing framework for convex nonsmooth functions with a supremum structure. The regularization yields a differentiable surrogate with controlled approximation error, a single-valued dual maximizer, and explicit gradient formulas. We then study an accelerated inertial dynamic with vanishing damping driven by a time-dependent regularized function whose parameter decreases to zero. Under mild integrability and boundedness conditions on the regularization schedule, we establish an accelerated $\\mathcal{O}(t^{-2})$ decay estimate for the regularized residual and, in the regime $α>3$, a sharper $o(t^{-2})$ decay together with weak convergence of trajectories to a minimizer of the original nonsmooth problem via an Opial-type argument. Applications to multiobjective optimization (through Chebyshev/max scalarization) and to distributionally robust optimization (via entropic regularization over ambiguity sets) illustrate the scope of the framework.",
      "authors": [
        "Samir Adly",
        "Juan José Maulén",
        "Emilio Vilches"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-01-21 17:31:07+00:00",
      "link": "https://arxiv.org/pdf/2601.15208v1",
      "tags": [
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15092v1",
      "title": "Federated Incremental Subgradient Method for Convex Bilevel Optimization Problems",
      "abstract": "In this letter, we consider a bilevel optimization problem in which the outer-level objective function is strongly convex, whereas the inner-level problem consists of a finite sum of convex functions. Bilevel optimization problems arise in situations where the inner-level problem does not have a unique solution. This has led to the idea of introducing an outer-level objective function to select a solution with the specific desired properties. We propose an iterative method that combines an incremental algorithm with a broadcast algorithm, both based on the principles of federated learning. Under appropriate assumptions, we establish the convergence results of the proposed algorithm. To demonstrate its performance, we present two numerical examples related to binary classification and a location problem.",
      "authors": [
        "Sudkobfa Boontawee",
        "Mootta Prangprakhon",
        "Nimit Nimana"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-01-21 15:32:18+00:00",
      "link": "https://arxiv.org/pdf/2601.15092v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.16089v1",
      "title": "A forward-only scheme for online learning of proposal distributions in particle filters",
      "abstract": "We introduce a new online approach for constructing proposal distributions in particle filters using a forward scheme. Our method progressively incorporates future observations to refine proposals. This is in contrast to backward-scheme algorithms that require access to the entire dataset, such as the iterated auxiliary particle filters (Guarniero et al., 2017, arXiv:1511.06286) and controlled sequential Monte Carlo (Heng et al., 2020, arXiv:1708.08396) which leverage all future observations through backward recursion. In comparison, our forward scheme achieves a gradual improvement of proposals that converges toward the proposal targeted by these backward methods. We show that backward approaches can be numerically unstable even in simple settings. Our forward method, however, offers significantly greater robustness with only a minor trade-off in performance, measured by the variance of the marginal likelihood estimator. Numerical experiments on both simulated and real data illustrate the enhanced stability of our forward approach.",
      "authors": [
        "Sylvain Procope-Mamert",
        "Nicolas Chopin",
        "Maud Delattre",
        "Guillaume Kon Kam King"
      ],
      "primary_category": "stat.CO",
      "categories": [
        "stat.CO"
      ],
      "published": "2026-01-22 16:40:27+00:00",
      "link": "https://arxiv.org/pdf/2601.16089v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.16022v1",
      "title": "A Fast Monte Carlo Newton-Raphson Algorithm to Estimate Generalized Linear Mixed Models with Dense Covariance",
      "abstract": "Estimation of Generalised linear mixed models (GLMM) including spatial Gaussian process models is often considered computationally impractical for even moderately sized datasets. In this article, we propose a fast Monte Carlo maximum likelihood (MCML) algorithm for the estimation of GLMMs. The algorithm is a stochastic Newton-Raphson method, which approximates the expected Hessian and gradient of the log-likelihood by drawing samples of the random effects. We propose a new stopping criterion for efficient termination and preventing long runs of sampling in the stationary post-convergence phase of the algorithm and discuss Monte Carlo sample size choice. We run a series of simulation comparisons of spatial statistical models alongside the popular integrated nested Laplacian approximation method and demonstrate potential for similar or improved estimator performance and reduced running times. We also consider scaling of the algorithms to large datasets and demonstrate a greater than 100-fold reduction in running times using modern GPU hardware to illustrate the feasibility of full maximum likelihood methods with big spatial datasets.",
      "authors": [
        "Samuel I. Watson",
        "Yixin Wang",
        "Emanuele Giorgi"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME"
      ],
      "published": "2026-01-22 14:46:35+00:00",
      "link": "https://arxiv.org/pdf/2601.16022v1",
      "tags": [
        "keyword:EAA",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15942v1",
      "title": "A Hierarchical Bayesian Framework for Model-based Prognostics",
      "abstract": "In prognostics and health management (PHM) of engineered systems, maintenance decisions are ideally informed by predictions of a system's remaining useful life (RUL) based on operational data. Model-based prognostics algorithms rely on a parametric model of the system degradation process. The model parameters are learned from real-time operational data collected on the system. However, there can be valuable information in data from similar systems or components, which is not typically utilized in PHM. In this contribution, we propose a hierarchical Bayesian modeling (HBM) framework for PHM that integrates both operational data and run-to-failure data from similar systems or components. The HBM framework utilizes hyperparameter distributions learned from data of similar systems or components as priors. It enables efficient updates of predictions as more information becomes available, allowing for increasingly accurate assessments of the degradation process and its associated variability. The effectiveness of the proposed framework is demonstrated through two experimental applications involving real-world data from crack growth and lithium battery degradation. Results show significant improvements in RUL prediction accuracy and demonstrate how the framework facilitates uncertainty management through predictive distributions.",
      "authors": [
        "Xinyu Jia",
        "Iason Papaioannou",
        "Daniel Straub"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME"
      ],
      "published": "2026-01-22 13:24:33+00:00",
      "link": "https://arxiv.org/pdf/2601.15942v1",
      "tags": [
        "keyword:EOH"
      ]
    },
    {
      "id": "2601.15999v1",
      "title": "Graph Topology Identification Based on Covariance Matching",
      "abstract": "Graph topology identification (GTI) is a central challenge in networked systems, where the underlying structure is often hidden, yet nodal data are available. Conventional solutions to address these challenges rely on probabilistic models or complex optimization formulations, commonly suffering from non-convexity or requiring restrictive assumptions on acyclicity or positivity. In this paper, we propose a novel covariance matching (CovMatch) framework that directly aligns the empirical covariance of the observed data with the theoretical covariance implied by an underlying graph. We show that as long as the data-generating process permits an explicit covariance expression, CovMatch offers a unified route to topology inference.   We showcase our methodology on linear structural equation models (SEMs), showing that CovMatch naturally handles both undirected and general sparse directed graphs - whether acyclic or positively weighted - without explicit knowledge of these structural constraints. Through appropriate reparameterizations, CovMatch simplifies the graph learning problem to either a conic mixed integer program for undirected graphs or an orthogonal matrix optimization for directed graphs. Numerical results confirm that, even for relatively large graphs, our approach efficiently recovers the true topology and outperforms standard baselines in accuracy. These findings highlight CovMatch as a powerful alternative to log-determinant or Bayesian methods for GTI, paving the way for broader research on learning complex network topologies with minimal assumptions.",
      "authors": [
        "Yongsheng Han",
        "Raj Thilak Rajan",
        "Geert Leus"
      ],
      "primary_category": "eess.SP",
      "categories": [
        "eess.SP"
      ],
      "published": "2026-01-22 14:19:01+00:00",
      "link": "https://arxiv.org/pdf/2601.15999v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15471v1",
      "title": "Achievable Rate Optimization for Large Flexible Intelligent Metasurface Assisted Downlink MISO under Statistical CSI",
      "abstract": "The integration of electromagnetic metasurfaces into wireless communications enables intelligent control of the propagation environment. Recently, flexible intelligent metasurfaces (FIMs) have evolved beyond conventional reconfigurable intelligent surfaces (RISs), enabling three-dimensional surface deformation for adaptive wave manipulation. However, most existing FIM-aided system designs assume perfect instantaneous channel state information (CSI), which is impractical in large-scale networks due to the high training overhead and complicated channel estimation. To overcome this limitation, we propose a robust statistical-CSI-based optimization framework for downlink multiple-input single-output (MISO) systems with FIM-assisted transmitters. A block coordinate ascent (BCA)-based iterative algorithm is developed to jointly optimize power allocation and FIM morphing, maximizing the average achievable sum rate. Simulation results show that the proposed statistical-CSI-driven FIM design significantly outperforms conventional rigid antenna arrays (RAAs), validating its effectiveness and practicality.",
      "authors": [
        "Ling He",
        "Vaibhav Kumar",
        "Anastasios Papazafeiropoulos",
        "Miaowen Wen",
        "Le-Nam Tran",
        "Marwa Chafii"
      ],
      "primary_category": "eess.SP",
      "categories": [
        "eess.SP"
      ],
      "published": "2026-01-21 21:14:47+00:00",
      "link": "https://arxiv.org/pdf/2601.15471v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15126v1",
      "title": "Sparse Sensor Arrays for Active Sensing: Models, Configurations and Applications",
      "abstract": "This chapter focuses on active sensing using sparse arrays. In active sensing applications, such as radar, sonar, wireless communications, and medical ultrasound, a collection of sensors probes the environment by emitting self-generated energy. A key benefit of such active multi-sensor arrays is their ability to focus and steer energy in desired directions by beamforming on transmit. Sparse sensor arrays offer several advantages over conventional uniform arrays, including improved resolution using fewer physical sensors and the capability to identify more scatterers than sensors. This is facilitated by the effective transmit-receive virtual array known as the sum co-array, which can have many more virtual sensors than the number of physical transmit or receive sensors. Herein, we focus on the design of low-redundancy sparse array configurations and on employing transmit-receive (Tx-Rx) beamforming using sparse arrays. We discuss the optimal, but computationally intractable Minimum-redundancy array, and a scalable symmetric array framework, which extends many well-known passive sparse array geometries to the active case. We also examine mitigating side lobes arising from spatial undersampling by a synthetic beamforming method known as image addition. We briefly present approaches for finding the physical beamforming weights synthesizing a desired Tx-Rx beampattern, and consider related spatio-temporal trade-offs. We conclude by discussing selected applications of sparse arrays in active sensing.",
      "authors": [
        "Robin Rajamäki",
        "Visa Koivunen"
      ],
      "primary_category": "eess.SP",
      "categories": [
        "eess.SP"
      ],
      "published": "2026-01-21 16:02:57+00:00",
      "link": "https://arxiv.org/pdf/2601.15126v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.16086v1",
      "title": "Random Walks Across Dimensions: Exploring Simplicial Complexes",
      "abstract": "We introduce a novel operator to describe a random walk process on a simplicial complex. Walkers are allowed to wonder across simplices of various dimensions, bridging nodes to edges, and edges to triangles, via a nested organization that hierarchically extends to higher structures of arbitrary large, but finite, dimension. The asymptotic distribution of the walkers provides a natural ranking to gauge the relative importance of higher order simplices. Optimal search strategies in presence of stochastic teleportation are addressed and the peculiar interplay of noise with higher order structures unraveled.",
      "authors": [
        "Diego Febbe",
        "Duccio Fanelli",
        "Timoteo Carletti"
      ],
      "primary_category": "cond-mat.stat-mech",
      "categories": [
        "cond-mat.stat-mech",
        "nlin.AO",
        "physics.soc-ph"
      ],
      "published": "2026-01-22 16:33:24+00:00",
      "link": "https://arxiv.org/pdf/2601.16086v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15944v1",
      "title": "Partitioning networks into clusters of synchronized nodes via the message-passing algorithm: an unbiased scalable approach",
      "abstract": "Partitioning large networks into stable clusters of synchronized nodes is a challenging task. Recent approaches based on spectral analysis can provide exact results on specific dynamics but remain unfeasible for very large networks. Moreover, within a stochastic framework, it is unclear which dynamics should be chosen to study synchronization. Here we propose an unbiased and scalable method based on the message-passing algorithm. By exploiting the collective behavior emerging across critical points of an effective Ising-like model, we identify dynamically coherent clusters of synchronized nodes and illustrate the approach on some large real-world networks. We find that, unlike continuous-time dynamics, abrupt desyncrhronization occurs even in simple graphs, without the need to invoke higher order interactions. However, when noise is included, the transition to synchronization becomes smoother and proceeds through the formation of plateaus, albeit at the cost of requiring larger coupling strengths.",
      "authors": [
        "Massimo Ostilli"
      ],
      "primary_category": "physics.soc-ph",
      "categories": [
        "physics.soc-ph",
        "cond-mat.dis-nn",
        "physics.data-an"
      ],
      "published": "2026-01-22 13:26:31+00:00",
      "link": "https://arxiv.org/pdf/2601.15944v1",
      "tags": [
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.15440v1",
      "title": "Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization",
      "abstract": "We present dla-ideal-solver, a high-performance framework for simulating two-dimensional Diffusion-Limited Aggregation (DLA) using Numba-accelerated Python. By leveraging just-in-time (JIT) compilation, we achieve computational throughput comparable to legacy static implementations while retaining high-level flexibility. We investigate the Laplacian growth instability across varying injection geometries and walker concentrations. Our analysis confirms the robustness of the standard fractal dimension $D_f \\approx 1.71$ for dilute regimes, consistent with the Witten-Sander universality class. However, we report a distinct crossover to Eden-like compact growth ($D_f \\approx 1.87$) in high-density environments, attributed to the saturation of the screening length. Beyond standard mass-radius scaling, we employ generalized Rényi dimensions and lacunarity metrics to quantify the monofractal character and spatial heterogeneity of the aggregates. This work establishes a reproducible, open-source testbed for exploring phase transitions in non-equilibrium statistical mechanics.",
      "authors": [
        "Sandy H. S. Herho",
        "Faiz R. Fajary",
        "Iwan P. Anwar",
        "Faruq Khadami",
        "Nurjanna J. Trilaksono",
        "Rusmawan Suwarman",
        "Dasapta E. Irawan"
      ],
      "primary_category": "nlin.PS",
      "categories": [
        "nlin.PS",
        "physics.comp-ph"
      ],
      "published": "2026-01-21 20:10:12+00:00",
      "link": "https://arxiv.org/pdf/2601.15440v1",
      "tags": [
        "keyword:EOH",
        "keyword:LNS"
      ]
    },
    {
      "id": "2601.16144v1",
      "title": "Fair sampling with temperature-targeted QAOA based on quantum-classical correspondence theory",
      "abstract": "In combinatorial optimization problems with degenerate ground states, fair sampling of degenerate solutions is essential. However, the quantum approximate optimization algorithm (QAOA) with a standard transverse-field mixer induces biases among degenerate states as circuit depth increases. Based on quantum-classical correspondence theory, we propose SBO-QAOA, which employs a temperature-dependent Hamiltonian encoding a Gibbs distribution as its ground state. Numerical simulations show that, unlike standard QAOA, SBO-QAOA yields ground-state probabilities converging to finite-temperature values with uniform distribution among degenerate states. These fairness and temperature-targeting properties are preserved even with only four variational parameters under a linear schedule.",
      "authors": [
        "Tetsuro Abe",
        "Shu Tanaka"
      ],
      "primary_category": "quant-ph",
      "categories": [
        "quant-ph",
        "cond-mat.stat-mech"
      ],
      "published": "2026-01-22 17:36:32+00:00",
      "link": "https://arxiv.org/pdf/2601.16144v1",
      "tags": [
        "keyword:LNS"
      ]
    }
  ],
  "queries": [
    {
      "type": "keyword",
      "tag": "LNS",
      "paper_tag": "keyword:LNS",
      "query_text": "Find research papers describing the development and application of adaptive large neighborhood search metaheuristics and hybrid optimization techniques for solving complex combinatorial problems in logistics, scheduling, and resource allocation.",
      "sim_scores": {
        "2601.16194v1": {
          "score": 0.7898844480514526,
          "rank": 1
        },
        "2601.15717v1": {
          "score": 0.7744136452674866,
          "rank": 2
        },
        "2601.16156v1": {
          "score": 0.7743082046508789,
          "rank": 3
        },
        "2601.16091v1": {
          "score": 0.7739427089691162,
          "rank": 4
        },
        "2601.16086v1": {
          "score": 0.773263692855835,
          "rank": 5
        },
        "2601.15092v1": {
          "score": 0.7711628675460815,
          "rank": 6
        },
        "2601.15561v1": {
          "score": 0.7692410349845886,
          "rank": 7
        },
        "2601.15399v1": {
          "score": 0.767268717288971,
          "rank": 8
        },
        "2601.15992v1": {
          "score": 0.763762354850769,
          "rank": 9
        },
        "2601.16056v1": {
          "score": 0.7635733485221863,
          "rank": 10
        },
        "2601.15636v1": {
          "score": 0.760543167591095,
          "rank": 11
        },
        "2601.15552v1": {
          "score": 0.7602435350418091,
          "rank": 12
        },
        "2601.15258v1": {
          "score": 0.7601927518844604,
          "rank": 13
        },
        "2601.15904v1": {
          "score": 0.7590107321739197,
          "rank": 14
        },
        "2601.15531v1": {
          "score": 0.7583059072494507,
          "rank": 15
        },
        "2601.15148v1": {
          "score": 0.7578887343406677,
          "rank": 16
        },
        "2601.15678v1": {
          "score": 0.7573986053466797,
          "rank": 17
        },
        "2601.15738v1": {
          "score": 0.7556352019309998,
          "rank": 18
        },
        "2601.15131v1": {
          "score": 0.7552804350852966,
          "rank": 19
        },
        "2601.16083v1": {
          "score": 0.7499310970306396,
          "rank": 20
        },
        "2601.15864v1": {
          "score": 0.7499169707298279,
          "rank": 21
        },
        "2601.16142v1": {
          "score": 0.7494471073150635,
          "rank": 22
        },
        "2601.16089v1": {
          "score": 0.7490947246551514,
          "rank": 23
        },
        "2601.15849v1": {
          "score": 0.7456859350204468,
          "rank": 24
        },
        "2601.15860v1": {
          "score": 0.7455884218215942,
          "rank": 25
        },
        "2601.15658v1": {
          "score": 0.7453638315200806,
          "rank": 26
        },
        "2601.15944v1": {
          "score": 0.7447208762168884,
          "rank": 27
        },
        "2601.16028v1": {
          "score": 0.7440015077590942,
          "rank": 28
        },
        "2601.15178v1": {
          "score": 0.7436421513557434,
          "rank": 29
        },
        "2601.16025v1": {
          "score": 0.7436184883117676,
          "rank": 30
        },
        "2601.15589v1": {
          "score": 0.7430223822593689,
          "rank": 31
        },
        "2601.15853v1": {
          "score": 0.7426643967628479,
          "rank": 32
        },
        "2601.15167v1": {
          "score": 0.7422353029251099,
          "rank": 33
        },
        "2601.15487v1": {
          "score": 0.7421770691871643,
          "rank": 34
        },
        "2601.15077v1": {
          "score": 0.7421514987945557,
          "rank": 35
        },
        "2601.15950v1": {
          "score": 0.7417633533477783,
          "rank": 36
        },
        "2601.15170v1": {
          "score": 0.7414652109146118,
          "rank": 37
        },
        "2601.15915v1": {
          "score": 0.7399008274078369,
          "rank": 38
        },
        "2601.15500v1": {
          "score": 0.7396557331085205,
          "rank": 39
        },
        "2601.16175v1": {
          "score": 0.7395567893981934,
          "rank": 40
        },
        "2601.15518v1": {
          "score": 0.7392833232879639,
          "rank": 41
        },
        "2601.15434v1": {
          "score": 0.7386120557785034,
          "rank": 42
        },
        "2601.15241v1": {
          "score": 0.7385889291763306,
          "rank": 43
        },
        "2601.15124v1": {
          "score": 0.7381534576416016,
          "rank": 44
        },
        "2601.16063v1": {
          "score": 0.737519383430481,
          "rank": 45
        },
        "2601.15165v1": {
          "score": 0.7374639511108398,
          "rank": 46
        },
        "2601.15580v1": {
          "score": 0.7366569638252258,
          "rank": 47
        },
        "2601.15639v1": {
          "score": 0.7362840175628662,
          "rank": 48
        },
        "2601.15673v1": {
          "score": 0.736260712146759,
          "rank": 49
        },
        "2601.15723v1": {
          "score": 0.7362123727798462,
          "rank": 50
        },
        "2601.15928v1": {
          "score": 0.7361491918563843,
          "rank": 51
        },
        "2601.15478v1": {
          "score": 0.7356933355331421,
          "rank": 52
        },
        "2601.15252v1": {
          "score": 0.7355892658233643,
          "rank": 53
        },
        "2601.15709v1": {
          "score": 0.7352970838546753,
          "rank": 54
        },
        "2601.15627v1": {
          "score": 0.735253632068634,
          "rank": 55
        },
        "2601.15571v1": {
          "score": 0.7347849607467651,
          "rank": 56
        },
        "2601.15684v1": {
          "score": 0.7347127199172974,
          "rank": 57
        },
        "2601.15440v1": {
          "score": 0.7340121269226074,
          "rank": 58
        },
        "2601.15690v1": {
          "score": 0.7338851094245911,
          "rank": 59
        },
        "2601.15742v1": {
          "score": 0.7333378791809082,
          "rank": 60
        },
        "2601.16022v1": {
          "score": 0.7331168055534363,
          "rank": 61
        },
        "2601.16187v1": {
          "score": 0.7329399585723877,
          "rank": 62
        },
        "2601.15861v1": {
          "score": 0.7328965663909912,
          "rank": 63
        },
        "2601.15094v1": {
          "score": 0.7326667308807373,
          "rank": 64
        },
        "2601.15218v1": {
          "score": 0.7325955033302307,
          "rank": 65
        },
        "2601.16072v1": {
          "score": 0.7317246794700623,
          "rank": 66
        },
        "2601.16144v1": {
          "score": 0.7315177321434021,
          "rank": 67
        },
        "2601.15249v2": {
          "score": 0.7314975261688232,
          "rank": 68
        },
        "2601.15984v1": {
          "score": 0.7314274907112122,
          "rank": 69
        },
        "2601.15640v1": {
          "score": 0.7313299179077148,
          "rank": 70
        },
        "2601.15876v1": {
          "score": 0.7311632633209229,
          "rank": 71
        },
        "2601.15758v1": {
          "score": 0.7310222387313843,
          "rank": 72
        },
        "2601.15398v1": {
          "score": 0.7300193905830383,
          "rank": 73
        },
        "2601.16171v1": {
          "score": 0.7299895286560059,
          "rank": 74
        },
        "2601.15714v1": {
          "score": 0.7297322750091553,
          "rank": 75
        },
        "2601.15423v1": {
          "score": 0.7296211123466492,
          "rank": 76
        },
        "2601.15722v1": {
          "score": 0.7295513153076172,
          "rank": 77
        },
        "2601.15457v1": {
          "score": 0.7294728755950928,
          "rank": 78
        },
        "2601.16158v1": {
          "score": 0.7293697595596313,
          "rank": 79
        },
        "2601.15120v2": {
          "score": 0.7291949987411499,
          "rank": 80
        },
        "2601.15127v1": {
          "score": 0.7291756868362427,
          "rank": 81
        },
        "2601.15751v1": {
          "score": 0.729162335395813,
          "rank": 82
        },
        "2601.15411v1": {
          "score": 0.7289674282073975,
          "rank": 83
        },
        "2601.15593v1": {
          "score": 0.7282745838165283,
          "rank": 84
        },
        "2601.15726v1": {
          "score": 0.7281801700592041,
          "rank": 85
        },
        "2601.15663v1": {
          "score": 0.7281410098075867,
          "rank": 86
        },
        "2601.15671v1": {
          "score": 0.7280939817428589,
          "rank": 87
        },
        "2601.15999v1": {
          "score": 0.7278083562850952,
          "rank": 88
        },
        "2601.15601v1": {
          "score": 0.7275774478912354,
          "rank": 89
        },
        "2601.15470v1": {
          "score": 0.7274926900863647,
          "rank": 90
        },
        "2601.15620v1": {
          "score": 0.727295994758606,
          "rank": 91
        },
        "2601.15153v1": {
          "score": 0.7272484302520752,
          "rank": 92
        },
        "2601.15126v1": {
          "score": 0.7267935872077942,
          "rank": 93
        },
        "2601.15471v1": {
          "score": 0.7267208099365234,
          "rank": 94
        },
        "2601.15236v1": {
          "score": 0.7265219688415527,
          "rank": 95
        },
        "2601.15432v1": {
          "score": 0.7259973287582397,
          "rank": 96
        },
        "2601.15727v1": {
          "score": 0.7258898019790649,
          "rank": 97
        },
        "2601.15657v1": {
          "score": 0.7256139516830444,
          "rank": 98
        },
        "2601.15495v1": {
          "score": 0.7255027890205383,
          "rank": 99
        },
        "2601.15390v1": {
          "score": 0.7253095507621765,
          "rank": 100
        }
      }
    },
    {
      "type": "keyword",
      "tag": "EOH",
      "paper_tag": "keyword:EOH",
      "query_text": "Find research papers describing the recent advancements in the automated evolution of heuristic algorithms and metaheuristic frameworks for solving complex optimization problems across diverse computational domains.",
      "sim_scores": {
        "2601.15717v1": {
          "score": 0.7693492770195007,
          "rank": 1
        },
        "2601.15170v1": {
          "score": 0.7671289443969727,
          "rank": 2
        },
        "2601.15249v2": {
          "score": 0.7588822841644287,
          "rank": 3
        },
        "2601.15434v1": {
          "score": 0.7530499696731567,
          "rank": 4
        },
        "2601.15153v1": {
          "score": 0.7527402639389038,
          "rank": 5
        },
        "2601.16156v1": {
          "score": 0.7506579160690308,
          "rank": 6
        },
        "2601.15849v1": {
          "score": 0.750285267829895,
          "rank": 7
        },
        "2601.16175v1": {
          "score": 0.7493210434913635,
          "rank": 8
        },
        "2601.15876v1": {
          "score": 0.7490423917770386,
          "rank": 9
        },
        "2601.15690v1": {
          "score": 0.748303234577179,
          "rank": 10
        },
        "2601.15561v1": {
          "score": 0.7476970553398132,
          "rank": 11
        },
        "2601.15092v1": {
          "score": 0.7475501298904419,
          "rank": 12
        },
        "2601.15399v1": {
          "score": 0.747428834438324,
          "rank": 13
        },
        "2601.16056v1": {
          "score": 0.7473110556602478,
          "rank": 14
        },
        "2601.15094v1": {
          "score": 0.746330976486206,
          "rank": 15
        },
        "2601.16142v1": {
          "score": 0.745797336101532,
          "rank": 16
        },
        "2601.15077v1": {
          "score": 0.7456866502761841,
          "rank": 17
        },
        "2601.15551v1": {
          "score": 0.7456843852996826,
          "rank": 18
        },
        "2601.15552v1": {
          "score": 0.7450465559959412,
          "rank": 19
        },
        "2601.15531v1": {
          "score": 0.7448009252548218,
          "rank": 20
        },
        "2601.15487v1": {
          "score": 0.7441117167472839,
          "rank": 21
        },
        "2601.15678v1": {
          "score": 0.7437964677810669,
          "rank": 22
        },
        "2601.15172v1": {
          "score": 0.7426465153694153,
          "rank": 23
        },
        "2601.15804v1": {
          "score": 0.7425878047943115,
          "rank": 24
        },
        "2601.15167v1": {
          "score": 0.7418897151947021,
          "rank": 25
        },
        "2601.15860v1": {
          "score": 0.7417107820510864,
          "rank": 26
        },
        "2601.15657v1": {
          "score": 0.7411419153213501,
          "rank": 27
        },
        "2601.15751v1": {
          "score": 0.7403252720832825,
          "rank": 28
        },
        "2601.15432v1": {
          "score": 0.7383729219436646,
          "rank": 29
        },
        "2601.15100v1": {
          "score": 0.7380330562591553,
          "rank": 30
        },
        "2601.15992v1": {
          "score": 0.7377358675003052,
          "rank": 31
        },
        "2601.15738v1": {
          "score": 0.7368971109390259,
          "rank": 32
        },
        "2601.16206v1": {
          "score": 0.736828088760376,
          "rank": 33
        },
        "2601.15640v1": {
          "score": 0.7367052435874939,
          "rank": 34
        },
        "2601.15241v1": {
          "score": 0.7366138100624084,
          "rank": 35
        },
        "2601.15727v1": {
          "score": 0.736294150352478,
          "rank": 36
        },
        "2601.15626v1": {
          "score": 0.7361279726028442,
          "rank": 37
        },
        "2601.15124v1": {
          "score": 0.7360577583312988,
          "rank": 38
        },
        "2601.16086v1": {
          "score": 0.7344853281974792,
          "rank": 39
        },
        "2601.16025v1": {
          "score": 0.733622133731842,
          "rank": 40
        },
        "2601.15141v1": {
          "score": 0.7335590124130249,
          "rank": 41
        },
        "2601.16089v1": {
          "score": 0.7331070303916931,
          "rank": 42
        },
        "2601.15411v1": {
          "score": 0.7330735921859741,
          "rank": 43
        },
        "2601.15120v2": {
          "score": 0.7329473495483398,
          "rank": 44
        },
        "2601.15544v1": {
          "score": 0.7326945066452026,
          "rank": 45
        },
        "2601.15709v1": {
          "score": 0.7325581908226013,
          "rank": 46
        },
        "2601.15178v1": {
          "score": 0.7322711944580078,
          "rank": 47
        },
        "2601.15182v1": {
          "score": 0.7318564653396606,
          "rank": 48
        },
        "2601.15457v1": {
          "score": 0.7316691279411316,
          "rank": 49
        },
        "2601.15808v1": {
          "score": 0.7315243482589722,
          "rank": 50
        },
        "2601.15950v1": {
          "score": 0.7314009666442871,
          "rank": 51
        },
        "2601.15761v1": {
          "score": 0.7308923602104187,
          "rank": 52
        },
        "2601.15571v1": {
          "score": 0.7308871150016785,
          "rank": 53
        },
        "2601.15130v1": {
          "score": 0.7308120727539062,
          "rank": 54
        },
        "2601.15455v1": {
          "score": 0.7306446433067322,
          "rank": 55
        },
        "2601.16091v1": {
          "score": 0.7297669649124146,
          "rank": 56
        },
        "2601.15158v1": {
          "score": 0.7297170162200928,
          "rank": 57
        },
        "2601.15188v1": {
          "score": 0.7296597361564636,
          "rank": 58
        },
        "2601.15160v1": {
          "score": 0.7295616865158081,
          "rank": 59
        },
        "2601.15518v1": {
          "score": 0.7285255789756775,
          "rank": 60
        },
        "2601.15599v1": {
          "score": 0.7282646894454956,
          "rank": 61
        },
        "2601.15892v1": {
          "score": 0.7279599905014038,
          "rank": 62
        },
        "2601.16074v1": {
          "score": 0.7279543876647949,
          "rank": 63
        },
        "2601.15915v1": {
          "score": 0.7278431057929993,
          "rank": 64
        },
        "2601.15984v1": {
          "score": 0.7276504635810852,
          "rank": 65
        },
        "2601.16072v1": {
          "score": 0.7275733947753906,
          "rank": 66
        },
        "2601.15165v1": {
          "score": 0.7270360589027405,
          "rank": 67
        },
        "2601.15728v1": {
          "score": 0.7267456650733948,
          "rank": 68
        },
        "2601.15148v1": {
          "score": 0.7259951233863831,
          "rank": 69
        },
        "2601.15942v1": {
          "score": 0.7259411215782166,
          "rank": 70
        },
        "2601.15136v1": {
          "score": 0.7255117297172546,
          "rank": 71
        },
        "2601.15729v1": {
          "score": 0.724849283695221,
          "rank": 72
        },
        "2601.15247v1": {
          "score": 0.7247985601425171,
          "rank": 73
        },
        "2601.15398v1": {
          "score": 0.7247872352600098,
          "rank": 74
        },
        "2601.15440v1": {
          "score": 0.7246870994567871,
          "rank": 75
        },
        "2601.15758v1": {
          "score": 0.7246531844139099,
          "rank": 76
        },
        "2601.15258v1": {
          "score": 0.7241050004959106,
          "rank": 77
        },
        "2601.15208v1": {
          "score": 0.723528265953064,
          "rank": 78
        },
        "2601.15195v1": {
          "score": 0.7234222888946533,
          "rank": 79
        },
        "2601.16083v1": {
          "score": 0.7234098315238953,
          "rank": 80
        },
        "2601.15485v1": {
          "score": 0.7232401967048645,
          "rank": 81
        },
        "2601.16028v1": {
          "score": 0.7229257225990295,
          "rank": 82
        },
        "2601.15578v1": {
          "score": 0.7223429679870605,
          "rank": 83
        },
        "2601.15609v1": {
          "score": 0.722297191619873,
          "rank": 84
        },
        "2601.15686v1": {
          "score": 0.7220931053161621,
          "rank": 85
        },
        "2601.16194v1": {
          "score": 0.7220447063446045,
          "rank": 86
        },
        "2601.16097v1": {
          "score": 0.722015917301178,
          "rank": 87
        },
        "2601.15127v1": {
          "score": 0.7214378118515015,
          "rank": 88
        },
        "2601.16163v1": {
          "score": 0.7212556004524231,
          "rank": 89
        },
        "2601.15500v1": {
          "score": 0.7211661338806152,
          "rank": 90
        },
        "2601.15593v1": {
          "score": 0.7210917472839355,
          "rank": 91
        },
        "2601.15715v1": {
          "score": 0.720917820930481,
          "rank": 92
        },
        "2601.15714v1": {
          "score": 0.7207461595535278,
          "rank": 93
        },
        "2601.15658v1": {
          "score": 0.7200271487236023,
          "rank": 94
        },
        "2601.15394v1": {
          "score": 0.7200030088424683,
          "rank": 95
        },
        "2601.15417v1": {
          "score": 0.7199314832687378,
          "rank": 96
        },
        "2601.15807v1": {
          "score": 0.7198852896690369,
          "rank": 97
        },
        "2601.16043v1": {
          "score": 0.7198667526245117,
          "rank": 98
        },
        "2601.15778v1": {
          "score": 0.7198301553726196,
          "rank": 99
        },
        "2601.16038v1": {
          "score": 0.7192171812057495,
          "rank": 100
        }
      }
    },
    {
      "type": "keyword",
      "tag": "EAA",
      "paper_tag": "keyword:EAA",
      "query_text": "Find research papers describing the development and implementation of computationally efficient automatic algorithms designed to optimize complex decision-making processes and resource allocation within high-dimensional datasets and real-time systems.",
      "sim_scores": {
        "2601.15170v1": {
          "score": 0.7587915658950806,
          "rank": 1
        },
        "2601.15399v1": {
          "score": 0.7504590749740601,
          "rank": 2
        },
        "2601.16056v1": {
          "score": 0.7455752491950989,
          "rank": 3
        },
        "2601.15751v1": {
          "score": 0.7452080249786377,
          "rank": 4
        },
        "2601.15249v2": {
          "score": 0.7441288232803345,
          "rank": 5
        },
        "2601.16091v1": {
          "score": 0.7401797771453857,
          "rank": 6
        },
        "2601.16074v1": {
          "score": 0.7401363849639893,
          "rank": 7
        },
        "2601.15552v1": {
          "score": 0.7393978834152222,
          "rank": 8
        },
        "2601.15153v1": {
          "score": 0.738986611366272,
          "rank": 9
        },
        "2601.15561v1": {
          "score": 0.7386471033096313,
          "rank": 10
        },
        "2601.15571v1": {
          "score": 0.7377573251724243,
          "rank": 11
        },
        "2601.15992v1": {
          "score": 0.7364749908447266,
          "rank": 12
        },
        "2601.15727v1": {
          "score": 0.7364270687103271,
          "rank": 13
        },
        "2601.15690v1": {
          "score": 0.7359590530395508,
          "rank": 14
        },
        "2601.15717v1": {
          "score": 0.7357550263404846,
          "rank": 15
        },
        "2601.15589v1": {
          "score": 0.7350995540618896,
          "rank": 16
        },
        "2601.16083v1": {
          "score": 0.7345702648162842,
          "rank": 17
        },
        "2601.15599v1": {
          "score": 0.7343209385871887,
          "rank": 18
        },
        "2601.16142v1": {
          "score": 0.7332409620285034,
          "rank": 19
        },
        "2601.15182v1": {
          "score": 0.732260525226593,
          "rank": 20
        },
        "2601.16025v1": {
          "score": 0.7312653064727783,
          "rank": 21
        },
        "2601.15876v1": {
          "score": 0.73068767786026,
          "rank": 22
        },
        "2601.15167v1": {
          "score": 0.7305415868759155,
          "rank": 23
        },
        "2601.16175v1": {
          "score": 0.7305074334144592,
          "rank": 24
        },
        "2601.15092v1": {
          "score": 0.7301043272018433,
          "rank": 25
        },
        "2601.15950v1": {
          "score": 0.7289105653762817,
          "rank": 26
        },
        "2601.15849v1": {
          "score": 0.7286748886108398,
          "rank": 27
        },
        "2601.16206v1": {
          "score": 0.7286075949668884,
          "rank": 28
        },
        "2601.16089v1": {
          "score": 0.7285289764404297,
          "rank": 29
        },
        "2601.15657v1": {
          "score": 0.7277032136917114,
          "rank": 30
        },
        "2601.15160v1": {
          "score": 0.7276781797409058,
          "rank": 31
        },
        "2601.15860v1": {
          "score": 0.7275199890136719,
          "rank": 32
        },
        "2601.16086v1": {
          "score": 0.7273980975151062,
          "rank": 33
        },
        "2601.16022v1": {
          "score": 0.7263421416282654,
          "rank": 34
        },
        "2601.15178v1": {
          "score": 0.7263332605361938,
          "rank": 35
        },
        "2601.16171v1": {
          "score": 0.725807785987854,
          "rank": 36
        },
        "2601.15130v1": {
          "score": 0.7252675294876099,
          "rank": 37
        },
        "2601.15094v1": {
          "score": 0.7250809669494629,
          "rank": 38
        },
        "2601.15442v1": {
          "score": 0.7245070934295654,
          "rank": 39
        },
        "2601.15709v1": {
          "score": 0.7239049673080444,
          "rank": 40
        },
        "2601.15722v1": {
          "score": 0.7234024405479431,
          "rank": 41
        },
        "2601.15370v1": {
          "score": 0.7233797311782837,
          "rank": 42
        },
        "2601.15077v1": {
          "score": 0.7232480049133301,
          "rank": 43
        },
        "2601.15580v1": {
          "score": 0.7232252359390259,
          "rank": 44
        },
        "2601.15423v1": {
          "score": 0.7229934930801392,
          "rank": 45
        },
        "2601.16156v1": {
          "score": 0.722859799861908,
          "rank": 46
        },
        "2601.16028v1": {
          "score": 0.7224810123443604,
          "rank": 47
        },
        "2601.15808v1": {
          "score": 0.7223762273788452,
          "rank": 48
        },
        "2601.15100v1": {
          "score": 0.7221254110336304,
          "rank": 49
        },
        "2601.15728v1": {
          "score": 0.7220116853713989,
          "rank": 50
        },
        "2601.15258v1": {
          "score": 0.7213265895843506,
          "rank": 51
        },
        "2601.15148v1": {
          "score": 0.7210885882377625,
          "rank": 52
        },
        "2601.15158v1": {
          "score": 0.7210615873336792,
          "rank": 53
        },
        "2601.15487v1": {
          "score": 0.7209968566894531,
          "rank": 54
        },
        "2601.15127v1": {
          "score": 0.7208530902862549,
          "rank": 55
        },
        "2601.15551v1": {
          "score": 0.7207244634628296,
          "rank": 56
        },
        "2601.15493v1": {
          "score": 0.7206112146377563,
          "rank": 57
        },
        "2601.15871v1": {
          "score": 0.7205512523651123,
          "rank": 58
        },
        "2601.16072v1": {
          "score": 0.7203136682510376,
          "rank": 59
        },
        "2601.15703v1": {
          "score": 0.719890296459198,
          "rank": 60
        },
        "2601.15697v1": {
          "score": 0.7198485732078552,
          "rank": 61
        },
        "2601.15473v1": {
          "score": 0.7196285724639893,
          "rank": 62
        },
        "2601.15434v1": {
          "score": 0.7191486358642578,
          "rank": 63
        },
        "2601.15804v1": {
          "score": 0.7190942764282227,
          "rank": 64
        },
        "2601.16194v1": {
          "score": 0.7190942764282227,
          "rank": 65
        },
        "2601.15904v1": {
          "score": 0.7190812230110168,
          "rank": 66
        },
        "2601.15640v1": {
          "score": 0.7189551591873169,
          "rank": 67
        },
        "2601.15778v1": {
          "score": 0.7189103364944458,
          "rank": 68
        },
        "2601.15761v1": {
          "score": 0.7186444401741028,
          "rank": 69
        },
        "2601.15609v1": {
          "score": 0.7182106971740723,
          "rank": 70
        },
        "2601.15079v1": {
          "score": 0.7181875705718994,
          "rank": 71
        },
        "2601.15984v1": {
          "score": 0.7181761264801025,
          "rank": 72
        },
        "2601.15457v1": {
          "score": 0.718127965927124,
          "rank": 73
        },
        "2601.15188v1": {
          "score": 0.717685341835022,
          "rank": 74
        },
        "2601.15758v1": {
          "score": 0.7176798582077026,
          "rank": 75
        },
        "2601.15678v1": {
          "score": 0.7176674604415894,
          "rank": 76
        },
        "2601.15892v1": {
          "score": 0.7171148061752319,
          "rank": 77
        },
        "2601.15241v1": {
          "score": 0.7170968055725098,
          "rank": 78
        },
        "2601.15165v1": {
          "score": 0.7170848846435547,
          "rank": 79
        },
        "2601.15853v1": {
          "score": 0.7169212698936462,
          "rank": 80
        },
        "2601.16139v1": {
          "score": 0.7166453003883362,
          "rank": 81
        },
        "2601.15531v1": {
          "score": 0.71658855676651,
          "rank": 82
        },
        "2601.15417v1": {
          "score": 0.7159126996994019,
          "rank": 83
        },
        "2601.15247v1": {
          "score": 0.7158570885658264,
          "rank": 84
        },
        "2601.15394v1": {
          "score": 0.7158501148223877,
          "rank": 85
        },
        "2601.16112v1": {
          "score": 0.7156078815460205,
          "rank": 86
        },
        "2601.15212v1": {
          "score": 0.7153400182723999,
          "rank": 87
        },
        "2601.15141v1": {
          "score": 0.7152230739593506,
          "rank": 88
        },
        "2601.15953v1": {
          "score": 0.7150229215621948,
          "rank": 89
        },
        "2601.15630v1": {
          "score": 0.714762806892395,
          "rank": 90
        },
        "2601.15120v2": {
          "score": 0.7145700454711914,
          "rank": 91
        },
        "2601.15500v1": {
          "score": 0.7143951058387756,
          "rank": 92
        },
        "2601.16174v1": {
          "score": 0.7142454981803894,
          "rank": 93
        },
        "2601.15390v1": {
          "score": 0.7141889333724976,
          "rank": 94
        },
        "2601.15124v1": {
          "score": 0.7140787839889526,
          "rank": 95
        },
        "2601.15252v1": {
          "score": 0.7139384746551514,
          "rank": 96
        },
        "2601.15636v1": {
          "score": 0.7133946418762207,
          "rank": 97
        },
        "2601.15688v1": {
          "score": 0.7133132219314575,
          "rank": 98
        },
        "2601.15547v1": {
          "score": 0.7132478952407837,
          "rank": 99
        },
        "2601.15578v1": {
          "score": 0.7131547927856445,
          "rank": 100
        }
      }
    }
  ]
}