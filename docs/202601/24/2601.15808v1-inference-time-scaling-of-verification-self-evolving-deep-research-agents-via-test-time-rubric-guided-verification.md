# Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification
# 验证的推理时扩展：通过测试时准则引导验证实现自进化深度研究智能体

**Authors**: Yuxuan Wan, Tianqing Fang, Zaitang Li, Yintong Huo, Wenxuan Wang, Haitao Mi, Dong Yu, Michael R. Lyu \
**Date**: 2026-01-22 \
**PDF**: https://arxiv.org/pdf/2601.15808v1 \
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-green">EOH</span> <span class="tag-label tag-green">EAA</span> \
**Score**: 7.0 \
**Evidence**: 通过迭代反馈和细化实现自我进化的智能体 \
**TLDR**: 一种通过迭代验证和准则引导反馈实现自我进化的智能体框架。

---

## Abstract
Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.

## 摘要
深度研究智能体 (DRA) 的最新进展正在改变自动化的知识发现和问题解决。虽然现有的大多数工作集中在通过后训练来增强策略能力，但我们提出了一种替代范式：在精心设计的准则引导下，通过迭代验证策略模型的输出来实现智能体能力的自进化。这种方法带来了验证的推理时扩展，即智能体通过评估其生成的答案来产生迭代反馈和改进，从而实现自我提升。我们基于自动构建的 DRA 失败分类法推导出这些准则，该分类法将智能体的失败系统地分为五个大类和十三个子类。我们提出了 DeepVerifier，这是一种基于准则的结果奖励验证器，它利用了验证的不对称性，在元评估 F1 分数上比原始的“智能体即评委” (agent-as-judge) 和大语言模型 (LLM) 评委基准高出 12%-48%。为了实现实际的自进化，DeepVerifier 在测试时推理期间作为一个即插即用模块集成。验证器生成基于准则的详细反馈，并将其反馈给智能体进行迭代引导，在无需额外训练的情况下优化响应。在强大的闭源 LLM 支持下，这种测试时扩展在 GAIA 和 XBench-DeepResearch 的挑战性子集上实现了 8%-11% 的准确率提升。最后，为了支持开源发展，我们发布了 DeepVerifier-4K，这是一个包含 4,646 个高质量智能体步骤的精选监督微调数据集，专注于 DRA 验证。这些示例强调反思和自我批评，使开源模型能够开发出强大的验证能力。

---

## 速览摘要（自动生成）

**问题**：现有的深度研究智能体主要依赖训练后增强，缺乏在推理阶段通过自我验证来识别和修复复杂错误的能力。
**方法**：提出 DeepVerifier 框架，利用基于失败分类法制定的细粒度准则，在推理时对智能体输出进行迭代反馈与引导式自我演化。
**结论**：该方法显著提升了验证准确性，在 GAIA 等基准测试中实现 8%-11% 的性能增长，并开源了高质量的验证微调数据集。
