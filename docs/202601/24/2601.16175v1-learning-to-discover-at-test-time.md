# Learning to Discover at Test Time
# 学习在测试时进行发现

**Authors**: Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun \
**Date**: 2026-01-22 \
**PDF**: https://arxiv.org/pdf/2601.16175v1 \
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-green">EOH</span> <span class="tag-label tag-green">EAA</span> \
**Score**: 8.0 \
**Evidence**: 通过测试时搜索和强化学习自动发现最先进的解决方案 \
**TLDR**: 提出 TTT-Discover，利用测试时强化学习和搜索来发现科学问题的最优解。

---

## Abstract
How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.

## 摘要
如何利用人工智能为科学问题发现新的最先进（SOTA）解决方案？先前的测试时缩放（test-time scaling）工作（如 AlphaEvolve）通过提示冻结的大语言模型（LLM）来进行搜索。我们在测试时执行强化学习，使 LLM 能够继续训练，但现在是利用针对特定测试问题的经验。这种形式的持续学习非常特殊，因为其目标是产生一个极佳的解决方案，而非平均意义上的多个良好方案，且旨在解决当前特定问题而非泛化到其他问题。因此，我们的学习目标和搜索子程序旨在优先考虑最有希望的解决方案。我们将此方法称为“测试时训练以发现”（Test-Time Training to Discover，简称 TTT-Discover）。遵循先前的工作，我们专注于具有连续奖励的问题。我们报告了在数学、GPU 内核工程、算法设计和生物学领域尝试过的每一个问题的结果。TTT-Discover 在几乎所有问题上都创下了新的最先进纪录：(i) 埃尔德什（Erdős）最小重叠问题和自相关不等式；(ii) GPUMode 内核竞赛（比先前技术快达 2 倍）；(iii) 过去的 AtCoder 算法竞赛；以及 (iv) 单细胞分析中的去噪问题。我们的解决方案经过了专家或组织者的评审。与之前需要封闭前沿模型的最优结果不同，我们的所有结果均使用开源模型 OpenAI gpt-oss-120b 实现，并可通过我们公开的代码进行复现。我们的测试时训练运行是使用 Thinking Machines 的 API Tinker 执行的，每个问题的成本仅为几百美元。

---

## 论文详细总结（自动生成）

这是一份关于论文《Learning to Discover at Test Time》的结构化深入分析报告：

### 1. 论文的核心问题与整体含义
*   **研究动机**：科学发现（Scientific Discovery）要求 AI 提出超越现有知识边界的方案。现有的“测试时缩放”（Test-time Scaling）方法（如 AlphaEvolve）主要依赖于在冻结的大语言模型（LLM）上进行进化搜索。
*   **核心问题**：作者认为，人类在解决难题时会从失败中学习并内化新思想，而冻结的模型无法做到这一点。论文探讨了如何让 LLM 在面对单个特定测试问题时，通过强化学习（RL）进行实时训练（Test-Time Training），从而发现新的世界纪录级（SOTA）解决方案。
*   **整体含义**：这是一种特殊的持续学习形式，其目标不是泛化，而是“毕其功于一役”——针对当前问题产生一个极优解。

### 2. 论文提出的方法论：TTT-Discover
核心思想是将测试过程转化为针对单一问题的强化学习过程。
*   **核心流程**：算法在每一步从缓冲区（Buffer）中选择一个初始状态（先前的尝试），由 LLM 生成动作（代码/思考），环境评估奖励（如运行速度、数学界限），然后利用这些经验通过 LoRA 更新模型权重。
*   **关键技术细节**：
    *   **熵目标函数 (Entropic Objective)**：不同于标准 RL 最大化平均奖励，TTT-Discover 优化的是一种指数加权的熵目标 $J_\beta$。它通过自适应调节 $\beta$ 参数，强烈倾向于奖励最高的动作（Max Reward），而非平均表现。
    *   **PUCT 状态重用 (PUCT Prioritization)**：受 AlphaZero 启发，利用预测价值 $Q$（该状态下能达到的最大奖励）和探索奖励来选择初始状态，避免陷入局部最优。
    *   **自适应 $\beta$**：通过约束诱导策略与原策略之间的 KL 散度，动态调整奖励权重，确保训练稳定性。

### 3. 实验设计
*   **应用场景与数据集**：
    *   **数学**：Erdős 最小重叠问题、自相关不等式（AC1/AC2）、圆堆积（Circle Packing）。
    *   **GPU 内核工程**：GPUMode 竞赛中的 TriMul（三角矩阵乘法）和 DeepSeek MLA Decode 优化。
    *   **算法设计**：AtCoder 启发式竞赛（AHC039, AHC058）。
    *   **生物学**：单细胞 RNA 测序数据去噪（OpenProblems 评测）。
*   **Benchmark 与对比方法**：
    *   **基准**：人类专家最高纪录、先前 AI SOTA（AlphaEvolve, ThetaEvolve, ALE-Agent）。
    *   **对比方法**：Best-of-N（纯采样）、OpenEvolve（进化搜索）、Naive RL（标准 RL）。

### 4. 资源与算力
*   **模型**：主要使用开源模型 `gpt-oss-120b`，部分对比实验使用 `Qwen3-8B`。
*   **算力平台**：使用 Thinking Machines 的 **Tinker API** 进行推理和训练。
*   **成本与时长**：每个问题的训练成本约为 **500 美元**。
*   **训练参数**：每个问题运行 50 个训练步骤，每步生成 512 个 rollout（总计 25,600 个样本）。使用 LoRA（Rank=32）进行微调。
*   **硬件环境**：评估过程在 NVIDIA H100、H200、A100 及 AMD MI300X 等多种硬件上进行，以验证内核性能。

### 5. 实验数量与充分性
*   **实验覆盖面**：涵盖了数学、工程、算法、生物四个完全不同的领域，证明了方法的通用性。
*   **消融实验**：作者详细对比了“有无 TTT”、“有无熵目标”、“有无 PUCT 重用”以及“标准 RL”的效果，证明了其定制化组件的必要性。
*   **客观性**：所有结果均通过公开竞赛平台验证或由相关领域专家（如 MIT、斯坦福、GPUMode 组织者）进行评审，确保了结果的真实性和权威性。

### 6. 论文的主要结论与发现
*   **全面超越 SOTA**：TTT-Discover 在几乎所有尝试的任务中都打破了纪录。
    *   在数学问题上发现了比 AlphaEvolve 更优的界限。
    *   在 GPU 内核优化上，比人类专家最优解快了 **15%-50%**。
    *   在 AtCoder 竞赛中，如果实时参赛可获得第一名。
*   **学习优于搜索**：实验证明，在相同的采样预算下，通过 RL 更新模型权重（学习）的效果显著优于单纯的提示词工程或进化搜索。
*   **开源模型的潜力**：证明了使用中等规模的开源模型（120B）配合测试时训练，可以击败依赖闭源前沿模型（如 Gemini 2.0 Pro）的系统。

### 7. 优点
*   **目标对齐**：精准捕捉了“科学发现”任务的本质——追求极致的最大值而非平均值。
*   **高效性**：相比于动辄数万美元的预训练，几百美元的测试时训练成本在科学研究中极具性价比。
*   **跨领域通用**：无需针对特定领域设计复杂的变异算子（如进化算法所需），仅需定义奖励函数即可。

### 8. 不足与局限
*   **奖励函数依赖**：目前仅适用于具有**连续且可验证奖励**的问题。对于奖励稀疏（只有对/错）或无法自动验证的领域（如人文社科）尚不适用。
*   **上下文限制**：受限于 LLM 的上下文窗口（32k），在处理极长代码或复杂推理链时可能存在瓶颈。
*   **计算延迟**：虽然成本可控，但测试时训练需要实时进行梯度更新，这比直接推理要慢得多，不适合对响应时间要求极高的实时任务。

（完）
