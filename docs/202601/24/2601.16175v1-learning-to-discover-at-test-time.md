---
title: Learning to Discover at Test Time
title_zh: 在测试时学习以实现科学发现
authors: "Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun"
date: 2026-01-22
pdf: "https://arxiv.org/pdf/2601.16175v1"
tags: ["keyword:EOH", "keyword:EAA"]
score: 8.0
evidence: 通过测试时搜索和强化学习自动发现最先进的解决方案
tldr: 本研究提出TTT-Discover方法，旨在利用AI解决科学发现中的前沿问题。不同于以往仅通过提示词搜索的测试时缩放方法，该方法在测试阶段引入强化学习，使LLM能针对特定问题进行持续训练。通过优先考虑最有潜力的解决方案，TTT-Discover在数学不等式、GPU内核工程、算法设计及生物学单细胞分析等多个领域均取得了新的SOTA成果，证明了开源模型在低成本测试时训练下也能超越闭源模型。
motivation: 旨在克服现有测试时搜索方法仅依赖冻结模型提示、无法针对特定科学问题进行深度参数优化和经验积累的局限性。
method: 提出测试时训练发现（TTT-Discover）方法，通过在测试阶段对LLM进行强化学习，使其针对特定问题进行持续训练以搜索最优解。
result: 在Erdős数学问题、GPU内核优化（提速达2倍）、AtCoder算法竞赛及单细胞分析等多个领域均刷新了世界纪录。
conclusion: 研究证明了通过低成本的测试时训练，使用开源模型也能在复杂的科学发现任务中达到并超越闭源前沿模型的性能。
---

## 摘要
如何利用人工智能为科学问题发现新的最先进（SOTA）成果？先前的测试时缩放（test-time scaling）工作（如 AlphaEvolve）通过提示冻结的大语言模型（LLM）来进行搜索。我们在测试时执行强化学习，使 LLM 能够继续训练，但现在是利用针对特定测试问题的经验。这种形式的持续学习非常特殊，因为其目标是产生一个极佳的解决方案，而非平均意义上的多个良好方案，且旨在解决当前特定问题，而非泛化到其他问题。因此，我们的学习目标和搜索子程序旨在优先考虑最有希望的解决方案。我们将此方法称为“测试时训练以发现”（TTT-Discover）。遵循先前的工作，我们专注于具有连续奖励的问题。我们报告了在数学、GPU 内核工程、算法设计和生物学领域尝试过的每一个问题的结果。TTT-Discover 在几乎所有问题上都创下了新的最先进记录：(i) 埃尔德什（Erdős）最小重叠问题和自相关不等式；(ii) GPUMode 内核竞赛（比先前技术快达 2 倍）；(iii) 过去的 AtCoder 算法竞赛；以及 (iv) 单细胞分析中的去噪问题。我们的解决方案经过了专家或组织者的审查。与之前需要封闭前沿模型的最优结果不同，我们的所有结果均使用开源模型 OpenAI gpt-oss-120b 实现，并可通过公开代码复现。我们的测试时训练运行是使用 Thinking Machines 的 API Tinker 执行的，每个问题的成本仅为几百美元。

## Abstract
How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.

---

## 论文详细总结（自动生成）

# 论文总结：《Learning to Discover at Test Time》

## 1. 核心问题与整体含义
*   **研究背景**：科学发现（Scientific Discovery）要求 AI 能够产生超出其预训练分布、甚至超出人类现有知识边界的方案。
*   **研究动机**：现有的“测试时缩放”（Test-time scaling）方法（如 AlphaEvolve）主要依赖于在冻结权重的 LLM 上进行演化搜索（提示词工程）。这种方式下，模型无法通过经验更新自身参数，难以“内化”在解决复杂问题过程中产生的新思想。
*   **核心问题**：如何通过在测试阶段引入强化学习（RL），使模型能够针对特定问题进行参数更新（Test-Time Training, TTT），从而发现新的最先进（SOTA）解决方案？

## 2. 方法论：TTT-Discover
*   **核心思想**：将解决单个科学问题的过程建模为一个强化学习环境，通过持续训练（Continual Learning）优化模型权重，使其从失败和部分成功中学习。
*   **关键技术细节**：
    *   **熵目标函数（Entropic Objective $J_\beta$）**：不同于标准 RL 最大化平均奖励，该目标函数通过指数加权（Softmax 形式）使梯度更新强烈偏向于产生“历史最高奖励”的动作。其梯度公式为：$\nabla_\theta J_\beta \approx \mathbb{E}[w_\beta(a) \nabla_\theta \log \pi_\theta(a|s)]$，其中权重 $w_\beta$ 赋予高奖励动作极大的权重。
    *   **自适应 $\beta$ 调节**：为了防止训练不稳定，通过约束 KL 散度动态调整温度参数 $\beta$，确保模型更新不会偏离初始策略太远。
    *   **PUCT 状态重用（PUCT Prioritization）**：借鉴 AlphaZero 的搜索树策略，但在评估状态价值 $Q(s)$ 时，使用该路径下发现的“最大奖励”而非“平均奖励”，以优先探索最有潜力的分支。
    *   **算法流程**：初始化缓冲区 -> 使用 PUCT 采样初始状态 -> 模型生成代码/方案 -> 环境评估奖励 -> 更新缓冲区 -> 使用熵目标函数进行 LoRA 微调更新权重。

## 3. 实验设计
*   **实验场景与数据集**：
    1.  **数学**：Erdős 最小重叠问题（组合数学）、自相关不等式（加法组合）、圆堆积（几何）。
    2.  **GPU 内核工程**：GPUMode 竞赛中的 TriMul（三角矩阵乘法）和 MLA Decode（DeepSeek 架构组件）优化。
    3.  **算法设计**：AtCoder Heuristic Contest (AHC039/058) 启发式算法竞赛。
    4.  **生物学**：单细胞 RNA 测序数据去噪（OpenProblems Denoising）。
*   **Benchmark（基准）**：人类专家最高纪录、AlphaEvolve (V1/V2)、ThetaEvolve、ALE-Agent 等。
*   **对比方法**：Best-of-N（纯采样）、OpenEvolve（开源演化算法）、以及不同消融版本的 TTT。

## 4. 资源与算力
*   **模型使用**：主要使用开源模型 **OpenAI `gpt-oss-120b`**，部分对比实验使用了 `Qwen3-8B`。
*   **算力平台**：通过 Thinking Machines 的 **Tinker API** 运行。
*   **训练规模**：每个问题进行 **50 个训练步**，每步生成 **512 个样本**（Rollouts），总计约 25,600 个样本。
*   **成本与时长**：每个问题的解决成本仅为 **几百美元**（约 $500），证明了该方法的高效性。

## 5. 实验数量与充分性
*   **实验规模**：涵盖了 4 个跨度极大的科学与工程领域，针对约 8 个具体的复杂问题进行了深度测试。
*   **消融实验**：论文详细对比了“无 TTT（仅搜索）”、“无熵目标（标准 RL）”、“无状态重用”等变体，验证了各组件对发现 SOTA 的贡献。
*   **客观性**：所有数学和内核结果均经过了专家审查或官方评测系统验证，并提供了可复现的代码和构造结果。实验设计在相同采样预算下对比了搜索与学习，具有较高的公平性。

## 6. 主要结论与发现
*   **全面刷新纪录**：TTT-Discover 在 Erdős 最小重叠问题、AC1 不等式、GPU 内核优化、AtCoder 竞赛中均创造了新的世界纪录。
*   **开源超越闭源**：证明了通过测试时训练，使用 **开源模型** 也能超越依赖 Gemini 2.0 Pro 等顶级闭源模型的演化搜索方法。
*   **内核性能飞跃**：在 GPU 内核优化中，AI 发现的方案比人类专家最优解快了 **15% 到 2 倍**（如 A100 上的 TriMul）。
*   **学习优于搜索**：实验证明，随着计算量的增加，通过 RL 更新模型参数（学习）比单纯增加采样次数（搜索）能更有效地突破性能瓶颈。

## 7. 优点
*   **范式突破**：将测试时缩放从单纯的“概率搜索”转变为“参数进化”，使模型能针对特定问题进行深度定制。
*   **目标函数精准**：熵目标函数完美契合了“科学发现只需要一个最优解”的特性。
*   **普适性强**：在数学、代码、算法、生物等多个异构领域均表现出极强的发现能力。

## 8. 不足与局限
*   **奖励函数依赖**：该方法高度依赖于能够自动评估的连续奖励函数，对于无法量化或难以自动验证的领域（如社会科学、纯理论推导）难以应用。
*   **上下文窗口限制**：受限于 32k 的上下文窗口，在处理极长代码或复杂演化历史时可能面临信息截断。
*   **生物学深度不足**：虽然在单细胞去噪基准上得分最高，但其发现的算法是否具有真实的生物学解释性仍需进一步验证。
*   **硬件泛化风险**：GPU 内核优化往往针对特定硬件（如 H100），在不同架构间的迁移效果可能打折扣。

（完）
